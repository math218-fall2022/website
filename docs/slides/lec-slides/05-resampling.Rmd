---
title: "Math 218: Statistical Learning"
author: "Resampling"
date: "10/05/2022"
output: 
   xaringan::moon_reader:
    css: "math218-slides.css"
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
      slideNumberFormat: "%current%"  
editor_options: 
  chunk_output_type: console
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(NHANES)
library(ISLR)
knitr::opts_chunk$set(echo = F, warning = F, message = F)

```

class: center, middle

# Housekeeping

---

## Resampling

- Economically using a collected dataset by repeatedly drawing samples from the same training dataset and fitting a model of interest on each sample

  - Obtain additional information about the fitted model

- We will focus on two methods: **cross-validation** and the **bootstrap**


---

## Training vs Test errors

- Recall the distinction between the training and test datasets

  - Training data: used to fit model
  - Test data: used to test/evaluate the model 
  
- These two datasets result in two types of error:

  - **Training error**: average error resulting from using the model to predict the responses for the training data
  - **Test error**: average error from using the model to predict the responses on new, "unseen" observations
  
  
- Training error is often very different from test error

---

## Validation-set approach

- Ideally, the test data would be a designated dataset that:
  - Represents the actual dataset
  - Is large enough to generate meaningful predictions
  
--

- *Validation-set* approach: randomly divide (e.g. 50/50) the available data into two parts: a **training set** and a **validation**/**hold-out** set

  - Model is fit on training set
  - Fitted model predicts responses for the observations in the validation set
  
--

- The resulting validation-set error provides an estimate of the test error. This is typically assessed using MSE in the
case of a quantitative response and misclassification rate in
the case of a qualitative (discrete) response



---

## Validation-set approach: drawbacks

- Estimate for test error will depend on the observations included in the training and validation sets
  - Validation estimate of test error can be highly variable

--

- Only a subset of the available data (observations used in training set) are used to fit the model
  - i.e. fewer observations used to fit model might lead to *overestimating* test error rate
  
---

## Leave-One-Out Cross-Validation

- **Leave-one-out cross-validation** (LOOCV) attempts to address the drawbacks from validation set approach

--

- Still splits all observations into two sets: training and validation

  - Key difference: just *one* observation is used for the validation set is used for validation set, leaving $n-1$ observations for training set
  
--

- For example: choose observation $(x_{1}, y_{1})$ to be validation set, and fit model on training set $\{(x_{2}, y_{2}), (x_{3}, y_{3}), \ldots, (x_{n}, y_{n}) \}$

  - $\text{MSE}_{1} = (y_{1} -\hat{y}_{1})^2$ an approximately unbiased estimate for test error
  
---


## Leave-One-Out Cross-Validation

- Repeat procedure by selecting the second observation to be validation set, then third, etc. 

- Will end up with $n$ squared errors: $\text{MSE}_{1}, \text{MSE}_{2}, \ldots, \text{MSE}_{n}$

--

- LOOCV estimate for test MSE is the average:

$$\text{CV}_{(n)} = \frac{1}{n}\sum_{i=1}^{n} \text{MSE}_{i}$$

--

- Why is LOOCV preferred over validation set approach?

  - Each training set has $n-1$ observations $\rightarrow$ tend to not overestimate test error as much
  - There is no randomness in training/validation set splits
  

  
---

## Discuss

- Suppose I am fitting a simple linear regression model $Y = \beta_{0} + \beta_{1}X + \epsilon$. 

- I want to obtain an estimate of the test error using LOOCV

- Discuss exactly how you would implement this in code. Specific things to mention:

  - What "actions"/functions you would use, and in what order
  
  - What values you would compute
  
  - What values you would store
---

## LOOCV: Auto data

- Predict $\color{blue}{\text{mpg}}$ using a linear model with polynomial functions of $\color{blue}{\text{horsepower}}$

```{r loocv_auto, cache = T, fig.align="center", fig.width=8, fig.height=5}
n <- nrow(Auto)
pp <- 10
loocv_mse_df <- rep(NA, pp)
for(p in 1:pp){
  mse <- rep(NA,n)
  for(i in 1:n){
    mod <- lm(mpg ~ poly(horsepower,p), data = Auto[-i,]) 
    mse[i] <- (Auto[i,]$mpg - predict(mod, Auto[i,]))^2
  }
  loocv_mse_df[p] <- mean(mse)
}
data.frame(MSE = loocv_mse_df, degree = 1:pp) %>%
  ggplot(., aes(x = degree, y = MSE))+
  geom_point()+
  geom_line()+
  scale_x_continuous(breaks = 1:pp) + 
  ggtitle("LOOCV")+
  theme(text =element_text(size = 15))
```
---

## LOOCV: drawbacks

- LOOCV can be *expensive* to implement -- must fit the model $n$ times

  - Although if using least squares linear or polynomial regression, we have a nice shortcut!

--

$$\text{CV}_{(n)} = \frac{1}{n}\sum_{i=1}^{n} \left(\frac{y_{i} - \hat{y}_{i}}{1-h_{i}}\right)^2$$ where $\hat{y}_{i}$ is the fitted value from the least squares model fit to all $n$ observations, and $h_{i}$ is leverage

--

- Estimates for each validation set $i$  are highly correlated, so the average can have high variance


---

## k-fold Cross-Validation


- In **k-fold CV**, the observations are randomly divided into $K$ groups (or folds) of approximately equal size. 

- For each $k$ in $1, 2, \ldots, K$:
  - Leave out $k$-th group as validation set, and fit model on remaining $K-1$ parts (combined)
  - Obtain predictions for $k$-th part, and a corresponding $\text{MSE}_{k}$
  
---

## k-fold Cross-Validation

- Letting the $k$-th fold have $n_{k}$ observations:

  - $\text{MSE}_{k} = \frac{1}{n_{k}}\sum_{i \in \mathcal{C}_{k}} (y_{i} - \hat{y}^{(k)}_{i})^2$, where $\mathcal{C}_{k}$ is set of observations in $k$-th fold and $\hat{y}^{(k)}_{i}$ is fit for observation $i$ obtained from data with part $k$ removed
  
  - If $n$ is a multiple of $K$, then $n_{k} = n/K$
  
--

- The $k$-fold CV estimate of the test error is the average:

$$\text{CV}_{(K)} = \frac{1}{K} \sum_{k=1}^{K} \text{MSE}_{k}$$
---

## k-fold CV

- LOOCV is a special case of $k$-fold CV. 

  - Which value of $K$ yields LOOCV?

--

- $k$-fold CV estimate is still biased upward; bias minimized when $K = n$

  - $K = 5$ or $K=10$ often used as a compromise for bias-variance tradeoff

--

- LOOCV and $k$-fold CV are useful and commonly used because of their generality

---

## k-fold CV: Auto data

```{r}
K <- 5
J <- 8
```

- Still fitting linear regression of $\color{blue}{\text{mpg}}$ on $\color{blue}{\text{horsepower}}$

- Run `r K`-fold CV `r J` separate times on same set of data, each with different partition into folds

- LOOCV plot from before

```{r cv_5, fig.align="center", fig.width=8, fig.height=5}
cv_ls <- list()
for(j in 1:J){
  ids <- split(sample(1:n), ceiling(seq_along(1:n)/(n/K))) 
  mse_df <- matrix(NA, nrow = K, ncol = pp)
  for(k in 1:K){
    val_set_ids <- ids[[k]]
    for(p in 1:pp){
      mod <- lm(mpg ~ poly(horsepower, p), data = Auto[-val_set_ids,])
      preds <- predict(mod, Auto[val_set_ids,])
      mse_df[k,p] <- mean((preds - Auto[val_set_ids,]$mpg)^2)
    }
  }
  
  cv_ls[[j]] <- data.frame(MSE = colMeans(mse_df), polynomial = 1:pp, j = j) 
    
}

loocv_df <- data.frame(MSE = loocv_mse_df, polynomial = 1:pp, j = NA, mod = "LOOCV")
do.call(rbind,cv_ls) %>%
  mutate(j = as.factor(j),
         mod = paste0(K, "-fold CV"))%>%
  add_row(loocv_df)%>%
  ggplot(.,aes(x = polynomial, y = MSE )) +
  geom_point(aes(col = j))+
  geom_line(aes(col = j))+
  guides(color = "none")+
  scale_x_continuous(breaks = seq(1,pp,2)) +
    facet_wrap(~mod)+
  theme(text =element_text(size = 15))
```

---


## Cross-Validation for Classification

- CV can be useful for qualitative $Y$. Same procedure as for quantiative, but uses classification error rate instead of MSE

- LOOCV error rate:

$$\text{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^{n} \text{Err}_{i} = \frac{1}{n} \sum_{i=1}^{n} \mathbf{1}(y_{i} \neq \hat{y}_{i})$$
--

- Easily translated to $k$-fold CV 


---

## Caution!

- We may need to think critically about when to apply cross-validation

- Consider a scenario with binary data $(n = 50)$, and we have $p = 100$ possible predictors at hand

--

- Develop a classifier as follows:

  1. Find $p^{*} = 20$ predictors that have the largest correlation with the binary labels
  2. Apply binary classifier (e.g. logistic regression) using only those $p^*$ predictors
  
--

- Can we apply cross-validation at Step 2, forgetting Step 1? 

---

## Caution!

- No! CV at Step 2 ignores the fact that in Step 1, the procedure **already saw the labels of the training** and made use of them. Forgetting this would result in **leakage**.

  - Instead, we should include Step 1 as part of the validation process. 
  
--

- Wrong vs right procedure?

---

## Simulated data

- I can simulate data as follows:

  - Generate $p = 100$ predictors $X$ for $n = 50$ observations
  - Generate binary responses $Y_{i}$ at random
  
--

- Because the responses $Y$ are generated independent of $X$, true test error is 50%. 

- However, 10-fold CV estimate of test error when ignoring Step 1 is 0!

---

## Simulated data

```{r eval= T, echo = T}
set.seed(28)
n <- 50
p <- 1000
p_cv <- 20
y <- rbinom(n,1,0.5)
K <- 10
x <- matrix(rnorm(n*p, runif(p, -10, 10), 1),
            nrow = n, ncol = p, byrow = T)
```

```{r echo = F}
df<-data.frame(x)%>%
  mutate(y=y)
cv_ls <- list()
ids <- split(sample(1:n), ceiling(seq_along(1:n)/(n/K))) 
err_df <- rep(NA,K)
rhos <- apply(df, 2, function(x){cor.test(df$y, x)$estimate})[1:p]
cols <- which(abs(rhos) %in% (sort(abs(rhos), decreasing = T)[1:p_cv]))

for(k in 1:K){
  val_set_ids <- ids[[k]]
  mod <- glm(y ~ 1 + ., data=df[-val_set_ids,c(cols, p+1)],family = "binomial")
  preds <-(predict(mod, df[val_set_ids,], type="response")> 0.5)*1
  err_df[k] <- mean(y[val_set_ids]!=preds)
  
}
err_vec_wrong <- err_df

cv_ls <- list()
err_df <- rep(NA,K)

for(k in 1:K){
  val_set_ids <- ids[[k]]
  temp_df <- df[-val_set_ids,]
  rhos <- apply(temp_df, 2, function(x){cor.test(temp_df$y, x)$estimate})[1:p]
  cols <- which(abs(rhos) %in% (sort(abs(rhos), decreasing = T)[1:p_cv]))
  cols <- sample(1:p, p_cv)
  mod <- glm(y ~ -1 + ., data=temp_df[,c(cols, p+1)],family = "binomial")
  preds <-(predict(mod, temp_df, type="response")> 0.5)*1
  err_df[k] <- mean(y[val_set_ids]!=preds)
  
}
err_vec_correct <-   err_df
```

--

```{r echo = T}
mean(err_vec_wrong)
mean(err_vec_correct)
```
---

## Housekeeping

- No lab this week!

- Homework 4 due Thursday, October 13 at 11:59pm

- We are implementing methods in R; please come to my office hours if you have any questions about coding any method. It might be important...

---

class: middle, center

## The Bootstrap

---

## The Bootstrap

- The **bootstrap** is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or statistical learning method

- Example: can be used to estimate the standard errors of the $\beta$ coefficients in linear regression

--

- One goal of statistics: learn about a population.

  - Usually, population is not available, so must make inference from sample data

--

- Bootstrapping operates by *resampling* this sample data to create many simulated samples

---

## The Bootstrap

- Bootstrapping resamples the original dataset **with replacement**

- If the original datset has $n$ observations, then each bootstrap/resampled dataset also has $n$ observations

  - Each observation has equal probability of being included in resampled dataset
  - Can select an observation more than once for a resampled dataset
---


## Bootstrap: example

- Suppose a study on adult daily caffeine consumption (mg) collects 4 data points: 110, 130, 150, 200. I want to learn about the average consumption in adults.

- Create my first bootstrap sample:

```{r echo = T}
dat <- c(110, 130, 150, 200)
n <- length(dat)

samp1 <- sample(x = dat, size = n, replace = T)
samp1
```

--

- Obtain our first estimate for $\mu$, the population mean daily caffeine consumption in adults: $\hat{\mu}_{1} = `r mean(samp1)`$

---

## Bootstrap: example

- Take second sample:

```{r echo = T}
samp2 <- sample(x = dat, size = n, replace = T)
samp2
```
  
- $\hat{\mu}_{2} = `r mean(samp2)`$

--

- Repeat this process thousands of times!


---

## Bootstrap: example

```{r}
B <- 1000
samps <- t(replicate(B,  sample(x = dat, size = n, replace = T)))
mu_ests <- rowMeans(samps)
```

- After `r B` bootstrap samples, we end up with `r B` estimates for $\mu$

```{r fig.align="center", fig.height=4, fig.width=4}
ggplot(data.frame(mu = mu_ests), aes(x = mu)) +
  geom_histogram(bins = 10)

```

--

- Mean over all estimates is $\hat{\mu} = `r mean(mu_ests)`$

- Approximate 95% confidence interval for the mean are the 5% and 95% quantiles of the `r B` mean estimates: (`r quantile(mu_ests, c(0.025))`, `r quantile(mu_ests, c(0.975))`) 

  - Called a *bootstrap percentile* confidence interval
---

## The bootstrap

- Real world vs bootstrap world

--

- Pros:

  - No assumptions about distribution of your data 
  - Very general method that allows estimating sampling distribution of almost any statistic!
  - Cost-effective
--

- Cons:

  - In more complex scenarios, figuring out appropriate way to bootstrap may require thought
  - Can fail in some situations
  - Relies quite heavily on the original sample

---

### Can bootstrap estimate prediction error?


- Idea: take a bootstrap sample as training set, and the original sample as validation set. Repeat $B$ times.

--

- Issue: each bootstrap sample would have significant overlap (about 2/3) with the original data

  - Compare to $k$-fold CV which requires *no* overlap between the folds
  
  - Would result in underestimating true prediction error
  
---

### Can bootstrap estimate prediction error?

- Solution? Only use prediction for the observations that did not (by chance) occur in current bootstrap sample

- Gets complicated $\rightarrow$ CV is a more attractive approach for estimating prediction error


