---
title: "Math 218: Statistical Learning"
author: "Linear Model Selection"
date: "10/19/2022"
output: 
   xaringan::moon_reader:
    css: "math218-slides.css"
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
      slideNumberFormat: "%current%"  
editor_options: 
  chunk_output_type: console
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, messLC50 = F)

library(tidyverse)
library(NHANES)
library(ISLR)
library(leaps)
library(glmnet)
library(Rcpp)
library(pls)
set.seed(1)

library(lars)
data(diabetes)
z <- cbind(diabetes$x, y = diabetes$y)
z[,1:10] <- apply(z[,1:10], 2, scale)
diabetes <- as.data.frame(z) %>%
 rename("bp" = "map") 
```

class: center, middle

# Housekeeping

- Midterm 01: will grade within one week!! I'm sure you all did wonderfully!

- Midterm feedback results

---

### Linear Model Selection

- Recall our familiar linear model:

$$Y = \beta_{0} + \beta_{1} X_{1} + \ldots + \beta_{p} X_{p} + \epsilon$$

- We will discuss extending the linear model framework

--

- Alternative fitting procedures can

  - Improve prediction accuracy
  
  - Improve model interpretability
  
---

## Why alternatives?

- Prediction accuracy: if $n$ only slightly larger than $p$, least-squares fit will have high variability

  - If $p > n$, cannot use least-squares
  
--

- Interpretability: removing some irrelevant features/predictors can reduce complexity

  - How? Set corresponding coefficients to 0. This is known as **feature selection**
  
---

## Three popular classes of methods

1. **Subset selection**: identify a subset of the $p$ possible predictors that we believe to be related to $Y$, then fit least-squares model (focus of today)

--

2. **Shrinkage** or **regularization**: fit a model involving all $p$ predictors, but the estimated coefficients are shrunken towards 0 relative to least-squares estimates (focus of next week)

  - Can reduce variance and perform variable selection
  
--

3. **Dimension reduction**: project the $p$ predictors into a $q$-dimensional space where $q < p$ (maybe next week)

  - Achieved using $q$ linear combinations or projections of the $p$ predictors, then fit least squares on the $q$ projections
  
---

## Subset Selection

- Different methods to perform subset selection: best subset and stepwise

- **Best subset** selection fits a separate least-squares regression for each possible combination of the $p$ predictors
  - Look at all models with goal of identifying the *best*
  
--

- Fit one model with zero predictors, $p$ models with exactly one predictor, $\binom{p}{2} = p(p-1)/2$ models with exactly two, etc.

---

## Best subset selection

Algorithm:

1. Let $\mathcal{M}_{0}$ denote the *null model* with no predictors $(Y = \beta_{0}) + \epsilon$.

--
  
2. For $k = 1,2, \ldots, p$:

  a) Fit all $\binom{p}{k}$ models that contain exactly $k$ predictors
  
--

  b) Pick the best among these $\binom{p}{k}$ models (best according to $R^2$). Call it $\mathcal{M}_{k}$ 
  
--

3. Select a single best (**) model from the $\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}$ models 

---

## Example: diabetes data

```{r}
n <- nrow(diabetes)
```

- Researchers collected data on `r n` diabetes patients examining the disease progression one year after baseline

- Predictors: age, sex, BMI, blood pressure, total serum cholesterol, low-density lipoproteins, high-density lipoproteins, total cholestrol/HDL, log of serum triglycerides, glucose

- Response: disease progression $\color{blue}{\text{LC50}}$

--

- Goal: Find best subset model for $\color{blue}{\text{LC50}}$ from the ten possible predictors:

```{r echo =T}
p <- ncol(diabetes) - 1
# automatically includes intercept
sub_mods <- regsubsets(y ~ ., data = diabetes, nvmax = p) 
```

---

## Example: diabetes data

```{r}
summary(sub_mods)
```

---

## Best subset selection

- Presented for least squares, but same ideas apply to other models 
  
  - E.g.: logistic regression uses *deviance* in place of $R^2$ (smaller deviance preferred)
  
--

Issues:

  - For computational reasons, best subset selection cannot be applied for large $p$

  - When $p$ large, higher chance of finding models that fit training data well but might not have predictive power (overfitting)
  
---

## Stepwise methods

- Stepwise methods explore a more restricted set of models

- **Forward stepwise** selection: begins with null model with no predictors, then adds predictors to the model one at a time

  - At each step, the variable that gives the greatest *additional* improvement to the fit is added

---

## Forward stepwise selection

Algorithm:

1. Let $\mathcal{M}_{0}$ denote the null model with no predictors

--

2. For $k = 0, 1, \ldots, p-1$:

  a) Consider all $p-k$ models that augment the predictors in $\mathcal{M}_{k}$ with one additional predictor
  
--
  
  b) Choose the best (according to $R^2$) among these $p-k$ models, and call it $\mathcal{M}_{k+1}$
  
--
  
3. Select a single best (**) model from the models $\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}$

---

## Forward stepwise selection

- For example, pretend we have $p = 3$ possible predictors: $X_{1}, X_{2}, X_{3}$

--

- $\mathcal{M}_{0}$: $Y \approx \beta_{0}$

--

- At $k= 0$, we fit all $p-k = 3$ models that add an additional predictor to our current working model:


  - $Y \approx \beta_{0} + \beta_{1}X_{1} \Rightarrow R^2 = 0.2$
  
  - $Y \approx \beta_{0} + \beta_{1}X_{2}  \Rightarrow R^2 = 0.3$
  
  - $Y \approx \beta_{0} + \beta_{1}X_{3} \Rightarrow R^2 = 0.1$
  
--

- Set $\mathcal{M}_{1}$: $Y \approx \beta_{0} + \beta_{1}X_{2}$

--

- Now $k=1$, and consider all $p - k = 2$ models that add an additional predictor:

  - $Y \approx \beta_{0} + \beta_{1}X_{2} + \beta_{2} X_{1}$
  
  - $Y \approx \beta_{0} + \beta_{1}X_{2} + \beta_{2} X_{3}$
  
---

## Forward stepwise selection

```{r}
mod_fwd <- regsubsets(y ~. ,data = diabetes, nvmax = p)
summary(mod_fwd)
```
  
---

## Forward stepwise selection

- Computationally advantageous to best subset selection

- Works when $p > n$, but can only construct submodels $\mathcal{M}_{0}, \ldots, \mathcal{M}_{n-1}$

- Not guaranteed to find the best possible model out of all $2^{p}$ models -- why?



---

## Backward stepwise selection

- **Backward stepwise selection** is another alternative to best subset selection

- Begins with full least squares model containing all $p$ predictors, and iteratively removes the least useful predictor one at a time

---

## Backward stepwise selection

Algorithm:

1. Let $\mathcal{M}_{p}$ denote the full model with all $p$ predictors

--

2. For $k = p, p-1, \ldots, 1$:

  a) Consider all $k$ models that contain all but one of the predictors in $\mathcal{M}_{k}$, for a total of $k-1$ predictors
  
--
  
  b) Choose the best (according to RSS or $R^2$) among these $k$ models, and call it $\mathcal{M}_{k-1}$
  
--
  
3. Select a single best model (**) from the models $\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}$

---

## Backward stepwise selection

- Like forward selection, is not guaranteed to yield the *best* model containing a subset of the $p$ predictors

- Does not work when $p > n$

---

class: center, middle

### Choosing a final model

---

## Choosing optimal model

- Step 3 of all three methods require us to compare many models to choose the "best" model

--

- Recall, model containing all $p$ predictors will always yield lowest RSS/highest $R^2$

  - Related to training error
  
- We want a model with low test error $\rightarrow$ requires comparison across models with different number of predictors

---

## Estimating test error

- Approach 1: *indirectly* estimate test error by making an adjustment to the training error to account for the bias due to overfitting 

--

- Approach 2: *directly* estimate the test error (validation set or CV approach)

---

## Approach 1

- Techniques of Mallow's $C_{p}$, AIC, BIC, and adjusted $R^2$ 

- **Adjust** the training error for the model size (number of predictors)

  - Useful for comparing models of different sizes
  
--

- In the following slide, I display the adjusted $R^2$, BIC, and  $C_{p}$ for each model of size $j = 1, \ldots, p$ from best subset selection

---

## Diabetes example

```{r fig.align="center", fig.height=6, fig.width=9}
n <- nrow(diabetes)
mod_ls <- list(lm(y ~ bmi , diabetes),
               lm(y ~ bmi + ltg, diabetes),
               lm(y ~ bmi + bp + ltg , diabetes),
               lm(y ~ bmi + bp + tc + ltg, diabetes),
               lm(y ~ sex + bmi + bp+ hdl + ltg, diabetes),
               lm(y ~ sex + bmi + bp+ tc + ldl + ltg, diabetes),
               lm(y ~ sex + bmi + bp+ tc+ ldl + tch + ltg, diabetes),
               lm(y ~ sex + bmi + bp+ tc+ ldl + tch + ltg + glu, diabetes),
               lm(y ~ sex + bmi + bp+ tc+ ldl + hdl + tch + ltg + glu, diabetes),
               lm(y ~ ., diabetes  ))
pp <- length(mod_ls)
s2_hat <- (summary(mod_ls[[pp]])$sigma)^2
RSS_vec <- unlist(lapply(mod_ls, function(x){sum(resid(x)^2)}))

Cp <- (RSS_vec + 2*(1:pp)*s2_hat)/n
BIC <- (RSS_vec + log(n)*(1:pp)*s2_hat)/n

plot_df <- data.frame(k = 1:pp, mallows_Cp = Cp, adj_R2 = summary(sub_mods)$adjr2,
                      BIC = BIC) %>%
  pivot_longer(cols = -1, names_to = "metric") %>%
  mutate(label = round(value, 3)) %>%
  mutate(nudge_x = case_when(k <= 4 ~ 0.5, T ~ 0))
plot_df %>%
  ggplot(., aes(x = k, y = value))+
  geom_point()+
  geom_line()+
  facet_wrap(~metric, scales = "free_y")+
  theme(text = element_text(size = 20)) +
  # ggrepel::geom_text_repel(aes(label = label), nudge_x = plot_df$nudge_x,
  #                           segment.alpha = 0)+
 labs(x = "Number of predictors")+
 scale_x_continuous(breaks = 1:10)
```

---

### Mallow's $C_{p}$


- Mallow's $C_{p}$ compares the full model with subsets of models to determine amount of error left unexplained by partial model

- Small $C_{p}$ is preferred 

--

- For least squares model with $d$ predictors, 

$$C_{p} = \frac{1}{n}(\text{RSS} + 2d\hat{\sigma}^{2})$$

$\qquad$ and $\hat{\sigma}^{2}$ is estimate of the error variance of $\epsilon$ 

  - Throughout these slides, $\hat{\sigma}^{2}$ is estimated using full model containing all predictors


--

- Term $2d\hat{\sigma}^{2}$ is a **penalty**; increasing the number of predictors $d$ will increase $C_{p}$, holding everything else constant



---

## AIC

- The **AIC** (Akaike Information criterion) is defined for a large class of models fit by maximum likelihood

- Lower AIC preferred

--

$$\text{AIC} = -2\log (L) + 2d$$ 

$\qquad$ where $L$ is the maximimized value of the liklihood function for the given model

--

- For linear model with Normal errors, maximum likelihood and least squares are the same, so $C_{p}$ and AIC will lead to same conclusions

---

## BIC

- The **BIC** (Bayesian Information criterion) is derived from Bayesian point of view, but looks similar:

$$\text{BIC} = -2\log(L) + \log(n)d  = \frac{1}{n}(\text{RSS} + \log(n) d\hat{\sigma}^2)\quad^{**}$$

--

- Like $C_{p}$, BIC will tend to be small for a model with low test error, so lower BIC preferred

- What is the penalty for number of predictors in BIC?


---

## Mallow's $C_p$ vs BIC

- For least squares model with $d$ predictors,

$$
\begin{align*}
C_{p} &= \frac{1}{n}(\text{RSS} + \color{orange}{2}d\hat{\sigma}^{2})\\
\text{BIC} &= \frac{1}{n}(\text{RSS} + \color{orange}{\log(n)} d\hat{\sigma}^2)
\end{align*}
$$
--

- Note that $\log(n) > 2$ when $n > 7$

- Because most models have more than 7 observations, heavier penalty for BIC compared to $C_{p}$ when the model has many variables

--

- BIC tends to result in selection of smaller models than $C_{p}$ 
  

---

## Adjusted $R^2$

- Recall that usual $R^2 = 1- \frac{\text{RSS}}{\text{TSS}}$ where $\text{TSS} = \sum(y_{i} - \bar{y})^2$

  - $\text{RSS}$ always decreases as more variables are added to model $\rightarrow$ $R^2$ increases $\rightarrow$ $R^2$ not useful for comparing different-sized models
  
--

- For least squares model with $d$ variables: $$R^{2}_{adj} = 1 - \frac{\text{RSS}/(n-d-1)}{\text{TSS}/(n-1)}$$

--

- Why is $R^{2}_{adj}$ better for comparing models of different size?

--

  - Maximizing $R^{2}_{adj}$ is equivalent to minimizing $\text{RSS}/(n-d-1)$
  
  
---

## Approach 2

- An alternative to adjusting for model size is to directly estimate the test error using validation set/CV
  
--

- We have candidate models $\mathcal{M}_{0}, \mathcal{M}_{1}, \ldots, \mathcal{M}_{L}$

  - For each model $\mathcal{M}_{l}$, perform same validation set/CV procedure 
  - Select a model $\mathcal{M}_{\hat{l}}$ that has lowest estimated test error
  
--

- Does not require an estimate of $\sigma^{2}$

---

## Diabetes example

- For each model obtained by best subset selection, I am displaying the respective BIC, 10-fold CV test error estimate, and 50/50 validation test error estimate

```{r fig.align = "center", fig.width=9, fig.height=5}
set.seed(1)

## k-fold
K <- 10

ids <- split(sample(1:n), ceiling(seq_along(1:n)/(n/K))) 
cv_mse_df <- matrix(NA, nrow = K, ncol = pp)
for(k in 1:K){
  val_set_ids <- ids[[k]]
  for(p in 1:pp){
    mod <- lm(formula = mod_ls[[p]], data = diabetes[-val_set_ids,])
    preds <- predict(mod, diabetes[val_set_ids,])
    cv_mse_df[k,p] <- mean((preds - diabetes[val_set_ids,]$y)^2)
  }
}


# validation set
val_mse_df <- rep(NA, pp)
test_ids <- sample(n, round(n/2))
for(p in 1:pp){
  mod <- lm(formula = mod_ls[[p]], data = diabetes[-test_ids,])
  preds <- predict(mod, diabetes[test_ids,])
  val_mse_df[p] <- mean((preds - diabetes[test_ids,]$y)^2)
}

data.frame(k = 1:pp, CV_error = colMeans(cv_mse_df), val_set_error = val_mse_df, BIC = BIC ) %>%
 pivot_longer(cols = -1, names_to ="metric") %>%
 group_by(metric) %>%
 mutate(is_min = ifelse(value == min(value), T, F)) %>%
 ungroup() %>%
 ggplot(.,aes(x= k, y = value))+
 geom_line()+
 geom_point(aes(col = is_min, shape = is_min), size = 3) +
 facet_wrap(~metric, scales = "free")+
  scale_x_continuous(breaks = 1:10) +
 scale_color_manual(values = c("black", "blue"))+
 guides(color = "none", shape = "none") +
 theme(text = element_text(size = 20)) +
 labs(x = "Number of predictors")

```
  
---

## Summary

- Model selection methods are essential tools for data analysis

- High-dimensional data (small $n$, large $p$) are becoming more available
