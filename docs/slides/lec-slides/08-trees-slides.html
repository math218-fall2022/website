<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Tree-Based Methods" />
    <meta name="date" content="2022-10-31" />
    <script src="08-trees-slides_files/header-attrs-2.16/header-attrs.js"></script>
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Math 218: Statistical Learning
]
.author[
### Tree-Based Methods
]
.date[
### 10/31/2022
]

---






## Housekeeping


- Lab 05 due tomorrow Thursday, 11:59pm

- Office hours today 3-5 pm

- E-mail me about project partners by Saturday 11:59pm

- Second job candidate is giving talk on TODAY at 12:30-1:30pm in 75 Shannon Street Room 224
  
  - Candidate: Adam Waterbury
  
  - Talk title: “Reinforced Chains, Stochastic Approximation, and Coexistence in Ecological Systems.”


---

## Tree-based methods

- These methods use a series of if-then rules to divide/segment the predictor space into a number of simple regions

- The splitting rules can be summarized in a tree, so these approaches are known as **decision-tree** methods

- Can be simple and useful for interpretation

- Decision trees can be applied to both regression and classification problems


--

- Typically not competitive with the best supervised learning approaches in terms of prediction accuracy

--

- We will may later discuss **bagging** and **random forests**

---

## Diabetes data

- Diabetes disease progression colored from low (purple) to high (red)

&lt;img src="08-trees-slides_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

### Diabetes data: decision tree 

- A regression tree for predicting diabetes progression based on a person's age and total cholesterol

&lt;img src="08-trees-slides_files/figure-html/diabetes_simple_tree-1.png" style="display: block; margin: auto;" /&gt;

---


### Diabetes data: decision tree 


- Top split: observations with `\(\color{blue}{\text{total cholesterol}} \geq 0.126\)` are assigned to right branch

 - Predicted progression is mean response value for observations within this region: `\(\hat{y} =\)` 174.6
 
- Observations with `\(\color{blue}{\text{total cholesterol}} &lt; 0.126\)` assigned to left branch, and further subdivided by `\(\color{blue}{\text{age}}\)`

--

- This tree has two *internal nodes* and three *terminal nodes*

 - The value in each terminal node is the mean of training `\(Y\)` for the observations that fall there
 
---

## Terminology

- **Internal nodes**: points along the tree where the predictor space is split

 - First node is often referred to as **root node**

- **Terminal nodes** or **leaves**: regions where there is no further splitting

 - Decision trees are typically drawn upside down (leaves at bottom)
 
--

- What are the internal nodes and terminal nodes for these data?

--

- Any subnode of a given node is called a *child node*, and the given node, in turn, is the child's *parent node*.

---


### Diabetes data: results

- The tree stratifies the individuals into three regions of predictor space: `\(R_{1} = \{ X | \color{blue}{\text{total_chol} &gt; 0.126}\}\)`, `\(R_{2} = \{ X | \color{blue}{\text{total_chol}} &lt; 0.126, \color{blue}{\text{age}} &lt; 0.075\}\)`, and `\(R_{3} = \{ X | \color{blue}{\text{total_chol}} &lt; 0.126, \color{blue}{\text{age}} &gt; 0.075\}\)`

&lt;img src="08-trees-slides_files/figure-html/diabetes_regions-1.png" style="display: block; margin: auto;" /&gt;

---

### Diabetes data: interpretation of results

- `\(\color{blue}{\text{total cholesterol}}\)` is the most important factor for determining `\(\color{blue}{\text{diabetes progression}}\)`, and younger patients have less progression than older patients

--

- Given that a patient has higher `\(\color{blue}{\text{total cholesterol}}\)`, their age seems to play little role in their `\(\color{blue}{\text{diabetes progression}}\)`

- But among those who have cholesterol levels less than 0.126, the `\(\color{blue}{\text{age}}\)` of a patient does affect `\(\color{blue}{\text{diabetes progression}}\)`


--

- Likely an over-simplification of the true relationships between `\(\color{blue}{\text{total cholesterol}}\)`, `\(\color{blue}{\text{age}}\)`, and `\(\color{blue}{\text{diabetes progression}}\)`

 - But easy to display and interpret
 
---

## Building a regression tree

1. Divide predictor space (the set of possible values for `\(X_{1}, \ldots, X_{p}\)`) into `\(M\)` distinct and non-overlapping regions, `\(R_{1}, \ldots, R_{M}\)`

--

2. For every observation that lands in `\(R_{m}\)`, we output the same prediction: the mean of the training responses in `\(R_{m}\)`, `\(\hat{y}_{R_{m}} = \frac{1}{n_{m}} \sum_{i \in R_{m}} y_{i}\)`

--

What do these `\(R_{m}\)` look like?

---

## Building a regression tree

- In theory, regions `\(R_{1},\ldots, R_{M}\)` could have any shape. For simplicity, we divde predictor space into high-dimensional rectangle or *boxes*

--

- Goal: find boxes `\(R_{1},\ldots, R_{M}\)` that minimize RSS, given by

`$$\sum_{m=1}^{M} \sum_{i\in R_{m}} (y_{i} - \hat{y}_{R_{m}})^2$$`

--

- However, it is computationally infeasible to consider every possible partition of the feature space into `\(M\)` boxes!

---


## Building a regression tree

- We take a **top-down, greedy** approach known as *recursive binary splitting*

 - "Top-down": we begin at the top of tree where all observations belong to a single region, and then succesively partition
 
 - "Greedy": at each step, the *best* split is made rather than looking ahead and picking a split that would be better in some future step
 
--

- Making splits on the data as we go

---

## Details

- First, select the predictor `\(X_{j}\)` and the cutpoint `\(s\)` such that splitting the predictor space into the regions `\(\{X | X_{j} &lt; s\}\)` and `\(\{X | X_{j} \geq s\}\)` leads to lowest RSS

- That is, for any predictor `\(j\)` and cutpoint `\(s\)`, we define the pair

`$$R_l(j,s) = \{X | X_{j} &lt; s\} \text{ and } R_{r}(j,s) = \{X | X_{j} \geq s\}$$`
--

- We seek the values of `\((j, s)\)` that minimize

`$$\sum_{i:x_{i}\in R_l(j,s)} (y_{i} - \hat{y}_{R_{l}})^2 + \sum_{i:x_{i}\in R_r(j,s)} (y_{i} - \hat{y}_{R_{r}})^2$$`
where `\(\hat{y}_{R_{l}}\)` is the average of the training responses in `\(R_l(j,s)\)`

--

- In the diabetes example, the regions after the first split were `\(R_{l} = \{X | \color{blue}{\text{total cholesterol}} &lt; 0.126\}\)` and `\(R_{r} = \{X | \color{blue}{\text{total cholesterol}} \geq 0.126\}\)`
 
 
---

## Details (cont.)

- Then, repeat the process of looking for the best predictor and best cutpoint in order to split the data further so as to minimize RSS within each of the resulting regions

 - Instead of splitting entire predictor space, we split one of the two previously identified regions
 
 - Now we have three regions
 
--

- Again, split one of these further so as to minimize RSS. We continue this process until a stopping criterion is reached

---

## Details (cont.)

- We can also split on qualitative predictors. Then the regions would be 

$$
`\begin{align*}
R_l(j,s) &amp;= \{X | X_{j} = ``category \ A"\} \text{ and }\\
R_{r}(j,s) &amp;= \{X | X_{j} \neq ``category \ A"\}
\end{align*}`
$$

where `\(``category \ A"\)` is one of the possible categories of `\(X_{j}\)`

--

- If `\(X_{j}\)` has more than two levels, then we would need to choose the category that yields the best split

- E.g. if `\(X_{j}\)` takes values in  `\(\{A, B, C\}\)`, would need to consider:

  - `\(\{X | X_{j} = A\}\)` and `\(\{X | X_{j} \neq A\}\)`
  
  - `\(\{X | X_{j} = B\}\)` and `\(\{X | X_{j} \neq B\}\)`
  
  - `\(\{X | X_{j} = C\}\)` and `\(\{X | X_{j} \neq C\}\)`
---

## Splitting example

`swiss` data

---

## Possible issues

- This process may produce good predictions on the training set, but is likely to overfit the data. Why?

--

- A smaller tree with fewer splits/regions might lead to lower variance and better interpretation, at the cost of a little bias

--

- One possibile fix: build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold

 - Will result in smaller trees, but may be short-sighted
 
---

## Tree pruning

- A better strategy is to grow a very large tree `\(T_{0}\)`, and then **prune** it back in order to obtain a smaller **subtree**

 - Idea: remove sections that are non-critical
 
--

 - How to best prune the tree? A CV approach might be intuitive, but is expensive
 
--

- **Cost complexity pruning** or weakest link pruning: consider a sequence of trees indexed by a nonnegative tuning parameter `\(\alpha\)`. For each value of `\(\alpha\)`, there is a subtree `\(T \subset T_{0}\)` such that 

`$$\left(\sum_{m=1}^{|T|} \sum_{i: x_{i} \in R_{m}} (y_{i} - \hat{y}_{R_{m}})^2 \right)+ \alpha |T|$$`
 is as small as possible. 
 
--

 - `\(|T|\)` = number of terminal nodes of tree `\(T\)`
 
 - `\(R_{m}\)` is the rectangle corresponding to the `\(m\)`-th terminal node
 
---

## Cost-complexity pruning cont.

- `\(\alpha\)` controls trade-off between subtree's complexity and fit to the training data

--

 - What is the resultant tree `\(T\)` when `\(\alpha = 0\)`?

--
 - What happens as `\(\alpha\)` increases? 
 
--

- Feels similar to the Lasso, maybe?

  - Just like in Lasso and ridge, for every value of `\(\alpha\)`, we have a different fitted tree `\(\rightarrow\)` need to  choose a best `\(\alpha\)`

--

- Select an optimal `\(\alpha^{*}\)` using cross-validation, then return to full data set and obtain the subtree corresponding to  `\(\alpha^{*}\)`

---

## Algorithm for building tree

0. Split data into train and validation sets

--

1. Using recursive binary splitting to grow a large tree on the training data, stopping according to a pre-determined stopping rule

--

2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best trees as a function of `\(\alpha\)`

--

3. Use `\(k\)`-fold CV to choose `\(\alpha\)`: divide training data into `\(K\)` folds. For each `\(k = 1,\ldots, K\)`:

 a) Repeat Steps 1 and 2 on all but the `\(k\)`-th fold
 
 b) Evaluate mean squared prediction error on the data in held-out `\(k\)`-th fold, as a function of `\(\alpha\)`.  Average the result for each `\(\alpha\)`
 
--

4. Choose `\(\alpha^{*}\)` that minimizes the average error. Return the subtree from Step 2 that corresponds to `\(\alpha^{*}\)` and use that for predictions

---

## Diabetes example

- Split data into 50/50 train and validation set, using  all predictors to build the large tree on the train set


```
## 
## Regression tree:
## tree(formula = y ~ ., data = diabetes[train_ids, ], control = tree.control(nobs = length(train_ids), 
##     mindev = 0))
## Variables actually used in tree construction:
## [1] "ltg"        "bmi"        "hdl"        "total_chol" "age"       
## [6] "ldl"        "bp"         "glu"        "sex"       
## Number of terminal nodes:  35 
## Residual mean deviance:  1579 = 293700 / 186 
## Distribution of residuals:
##      Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
## -112.9000  -18.4300   -0.5556    0.0000   21.0000  116.2000
```

- I perform minimal cost-complexity pruning, selecting `\(\alpha^*\)` using 5-fold CV on the training data

  - For each `\(\alpha\)`, there is an associated CV-error estimate when fitting on the training data
  
  - For each `\(\alpha\)`, there is an associated test error on the held-out validation data

---

## Diabetes example cont.



&lt;img src="08-trees-slides_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

- `\(K\)`-fold CV with `\(K = 5\)`. Best CV MSE at 6 leaves

- Note: while the CV error is computed as a function of `\(\alpha\)`, we often display as a function of `\(|T|\)`, the number of leaves

---

## Diabetes example: final result

&lt;img src="08-trees-slides_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

class: middle, center

## Classification trees

---

## Classification trees

- Very similar to regression tree, but now we are predicting a qualitative response

- Now, we predict that each observation belongs to the *most commonly occurring class* of training observations in the region to which it belongs

 - Often time, also interested in class proportions among training observations that fall in the region
 
---

## Classification trees

- Just like regression trees, use recursive binary splitting to grow the classification tree

- Now, RSS cannot be used as a criterion for making the splits

- Alternative: **classification error rate**, the fraction of training observations in the region that belong to the most common class:

`$$E = 1 - \max_{k}(\hat{p}_{mk})$$`
 where `\(\hat{p}_{mk}\)` is the proportion of training observations in the region `\(R_{m}\)` that are from class `\(k\)`
 
--

- However, classification error is not sufficiently sensitive to tree-growing

---

## Gini index

- The **Gini index** is a measure of the total variance across the `\(K\)` classes

`$$G_{m} = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})$$`

- `\(G_{m}\)` is small if all the `\(\hat{p}_{mk}\)`'s are close zero or one

--

- For this reason, Gini index is referred to as a measure of node *purity*
 
 - A small `\(G_{l}\)` indicates that the node contains predominantly observations from a single class
 

---
## Gini index

`$$G_{m} = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})$$`

- Example: 3 classes and 9 observations in three regions


```
##   Region1 Region2 Region3
## 1       A       A       A
## 2       A       A       A
## 3       A       B       A
## 4       A       B       B
## 5       A       C       B
## 6       A       C       B
## 7       A       C       C
## 8       A       C       C
## 9       A       C       C
```

- In these three regions, what are the Gini indices `\(G_{1}, G_{2}, G_{3}\)`?

--

  - `\(G_{1} = 1(1-1) + 0(1-0) + 0(1-0) = 0\)`
  - `\(G_{2} = \frac{2}{9}(1-\frac{2}{9}) +\frac{2}{9}(1-\frac{2}{9}) + \frac{5}{9}(1-\frac{5}{9}) \approx 0.60\)`
  - `\(G_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 0.67\)`

---

## Entropy

- Alternative to Gini index is **cross-entropy**:

`$$D_{m} = -\sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}$$`

- Very similar to Gini index, so cross-entropy is also a measure of node purity

---

## Entropy

`$$D_{m} = -\sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}$$`

- Same example: 3 classes and 9 observations in three regions


```
##   Region1 Region2 Region3
## 1       A       A       A
## 2       A       A       A
## 3       A       B       A
## 4       A       B       B
## 5       A       C       B
## 6       A       C       B
## 7       A       C       C
## 8       A       C       C
## 9       A       C       C
```

- In these three regions, what are the cross-entropies `\(D_{1}, D_{2}, D_{3}\)`?

--

  - `\(D_{1} = -(1\log(1) + 0\log(0) + 0\log(0)) = 0\)`
  - `\(D_{2} = -(\frac{2}{9}\log(\frac{2}{9}) +\frac{2}{9}\log(\frac{2}{9}) + \frac{5}{9}\log(\frac{5}{9})) \approx 1\)`
  - `\(D_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 1.1\)`


---

### Seeds data: full tree

- Recall the seeds data, where we have seven characteristics of three different grains: Kama, Rosa, and Canadian

--

&lt;img src="08-trees-slides_files/figure-html/seeds-1.png" style="display: block; margin: auto;" /&gt;

---

## Seeds data: pruned tree

&lt;img src="08-trees-slides_files/figure-html/seeds_cv-1.png" style="display: block; margin: auto;" /&gt;


---

## Remarks

- The full classification tree for the seeds data has surprising characteristic: some splits yield two terminal nodes with the same predicted value

  - Why? The split increases node purity even if it may not reduce classification error
  
---

## Remarks

- We can also split on qualitative predictors. Consider again the abalone data, and we want to create a regression tree for `\(\color{blue}{\text{age}}\)` using the predictor `\(\color{blue}{\text{age}} \in \{I, F, M\}\)`

&lt;img src="08-trees-slides_files/figure-html/abalone_tree-1.png" style="display: block; margin: auto;" /&gt;
---

## Remarks cont.

- Trees vs. linear models: which is better? Depends on the true relationships between the response and the predictors


&lt;img src="figs/08-trees/tree_linear.png" width="50%" style="display: block; margin: auto;" /&gt;

.footnote[Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013). An introduction to statistical learning : with applications in R. New York :Springer]
---

## Remarks cont.

- Advantages: 

 - Easy to explain, and may more closely mirror human decision-making than other approaches we've seen
 
 - Can be displayed graphically and interpreted by non-expert
 
 - Can easily handle qualitative predictors without the need to create dummy variables
 
--

- Disadvantages:

 - Lower levels of predictive accuracy compared to some other approaches
 
 - Can be non-robust 
 
--

However, we may see that aggregating many trees can improve predictive performance!

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
