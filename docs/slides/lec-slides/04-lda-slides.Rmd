---
title: "Math 218: Statistical Learning"
author: "LDA/QDA"
date: "9/26/2022"
output: 
   xaringan::moon_reader:
    css: "math218-slides.css"
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
      slideNumberFormat: "%current%"  
editor_options: 
  chunk_output_type: console
---


```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F,
                      fig.align = "center", fig.width = 5, fig.height = 5)

library(tidyverse)
library(broom)
library(ISLR)
library(nnet)
library(MASS)
library(mvtnorm)
library(mnormt)
library(pROC)
library(kableExtra)
set.seed(1)
heart<- read.table("data/heart_cleveland.txt", header = T, sep = ",") %>% 
  mutate(cp = as.factor(cp),
         sex = ifelse(sex == 1, "M", "F"),
         target = ifelse(target == 0,0, 1))
seeds <- read.table("data/seeds_dataset.txt", header = T) %>%
  mutate(variety= case_when(variety == 1~ "Kama",
                          variety == 2 ~ "Rosa",
                          T ~ "Canadian"))
```

class: center, middle

# Housekeeping

---

## Classification

- Qualitative variables takes values in an unordered set $\mathcal{C}$, such as:

  - $\color{blue}{\text{wine}} \in \{\color{blue}{\text{red}},\color{blue}{\text{white}},\color{blue}{\text{rose}}\}$

  - $\color{blue}{\text{diagnosis}} \in \{\color{blue}{\text{yes}}, \color{blue}{\text{no}} \}$

--

- We have predictors $X$ and a qualitative response $Y$ that takes values in $\mathcal{C}$

  - Supervised learning task

- Goal of classification: build a function $C(X)$ that predicts a label for $Y$

--

  - We are often interested in the estimated *probabilities* that $X$ belongs to a given category in $\mathcal{C}$

---

## Probability warm-up: discuss!

A fair, six-sided die is rolled. Let $X$ denote the result of the die roll. 

1. What is $\text{Pr}(X = 1)$?

2. What is $\text{Pr}(X \text{ is even})$?

Now we are rolling two dice.  Let $X_{1}$ denote the result of the first, and $X_{2}$ the second. 

3. What is $\text{Pr}(X_{1} + X_{2} = 1)$?

4. What is $\text{Pr}(X_{1} + X_{2} < 13)$?
---

## Probability crash-course!

- The *multiplication rule of independent events* in probability: if two events $A$ and $B$ are independent, then the probability of them both occurring at the same time is equal to the product of the individual probabilities

  - $\text{Pr}(A \text{ and } B) = \text{Pr}(A, B) =  \text{Pr}(A) \times \text{Pr}(B)$

- The *conditional probability* is the probability of an event occurring, given that another event has already occurred

  - $\text{Pr}(A | B)$, read as "probability of $A$ given $B$"
---

## Discuss!

- Assume that we have two independent six-sided dice. What are:
  
  - $\text{Pr}(X_{1} = 1, X_{2} = 4)$?
  - $\text{Pr}(X_{1} \text{ and } X_{2} \text{ are even})$?

- Focusing on just the first die, what are:

  - $\text{Pr}(X_{1} = 1 | X_{1} \text{ is even})$?
  
  - $\text{Pr}(X_{1} = 1 | X_{1} \text{ is odd})$?
  
  - $\text{Pr}(X_{1} = \text{ is odd} | X_{1}  = 1)$?

---

## Heart attack data

- Health measurements from `r nrow(heart)` patients, along with record of the presence of heart disease in the patient

```{r fig.align ="center", fig.height=5, fig.width=8}
heart %>%
  mutate( target = as.factor(target)) %>%
  dplyr::select(c(target, age, chol, trestbps))%>%
  pivot_longer(cols = -1, names_to = "variable") %>%
  ggplot(.,aes(y = value, x = target))+
  geom_boxplot()+
  facet_wrap(~variable, scales= "free")+
  theme(text=element_text(size = 20))

```

---

## Seeds data

- Data come from UCI Machine Learning Repository 

- Measurements of geometrical properties of kernels belonging to three different varieties of wheat

```{r fig.align ="center", fig.height=5, fig.width=8}
seeds %>%
  ggplot(., aes(x = variety, y = area))+
  geom_boxplot() +
  theme(text =element_text(size = 20)) 

```

* Ask about seed banks

---

## Why not linear regression?

- For seeds data, $\mathcal{C} = \{\text{Canadian, Kama, Rosa}\}$

- Could we encode the values as a quantitative response, such as:

$$Y = \begin{cases}
1 & \text{ if } \color{blue}{\text{Canadian} }\\
2 & \text{ if } \color{blue}{\text{Kama}}\\
3 & \text{ if } \color{blue}{\text{Rosa}} \end{cases}$$

$\quad$ and then fit a linear regression for this $Y$?

---

## Why not linear regression?

The heart disease data is formatted as:

$$Y = \begin{cases}
0 & \text{ if } \color{blue}{\text{no heart disease} }\\
1 & \text{ if } \color{blue}{\text{heart disease}}\end{cases}$$

- Here, $Y$ is binary

--

- For binary response, could we use least squares to fit a linear regression model to $Y$?

  - Maybe fit a linear regression and predict $\color{blue}{\text{heart disease}}$ if $\hat{Y} > 0.5$
  
--
  
  - Can show that the estimate $\hat{\beta}_{0} + \hat{\beta}_{1} X_{1} + \ldots + \hat{\beta}_{p} X_{p}$ is an estimate of $Pr(Y = 1 | X) = Pr(\color{blue}{\text{heart disease}} | X)$
  

---

## Why not linear regression?

- Fit a linear regression line for $\color{blue}{\text{disease}} = \beta_{0} + \beta_{1}\color{blue}{\text{age}} + \beta_{2}\color{blue}{\text{sex}}$

```{r fig.align = "center", fig.width=8, fig.height=5}
heart_lm <- lm(target ~ age + sex, heart)

ggplot(heart, aes(x = age, y = target)) + 
  geom_point()+
  geom_abline(intercept = heart_lm$coefficients[1] , slope = heart_lm$coefficients[2],
              col = "blue") +
  ggtitle("Observed disease vs. age", subtitle = "Estimated regression line for males")+
  theme(text = element_text(size = 20))+
  ylim(c(-0.2, 1))
```

--

- In certain cases, linear regression might produce estimated probabilities less than $0$ or bigger than $1$

---

## "exp"

- $\exp(x)$ is shorthand for $e^{x}$

- $e \approx$ `r exp(1)` is a special number

```{r, fig.width=6, fig.height=4}
xseq <- seq(-3,3, 0.1)
data.frame(x = xseq) %>%
  mutate(exp = exp(x)) %>%
  ggplot(., aes(x = x, y = exp))+
  geom_line() +
  labs(y = expression(e^x)) +
  theme(text = element_text(size = 16))
```

---

## "exp"

- $e^{0} = 1$

- $e^{a} \times e^{b} = e^{a+b}$

- $\frac{e^{a}}{e^{b}} = e^{a-b}$

- $e^{a} + e^{b} = \ldots e^{a} + e^{b}$

---

class: center, middle

# Discriminant analysis

---

## Motivating example

- Suppose I have two coins in my pocket, $C_{1}$ and $C_{2}$

- I pull out a coin from random and flip it a bunch of times. Then repeat for a total of 7 iterations.  At each iteration, I record the following information:

  - Which coin?
  
  - Order of heads and tails (Heads = 0, Tails = 1)
  

$$
\begin{align*}
&C_{1}: \text{0 1 1 1 1} \\
&C_{1}: \text{1 1 0} \\
&C_{2}: \text{1 0 0 0 0 0 0 1} \\
&C_{1}: \text{0 1} \\
&C_{1}: \text{1 1 0 1 1 1} \\
&C_{2}: \text{0 0 1 1 0 1} \\
&C_{2}: \text{1 0 0 0 0} \\
\end{align*}
$$
---

## Motivating example

- Now you tell me that the next coin flip resulted in $\text{0 0 1}$. Did it come from $C_{1}$ or $C_{2}$?

--

- Let each coin flip be a covariate (specifically, an indicator)

  - e.g. $X_{1} = 1$ if the first coin flip is a 1, and $X_{1} = 0$ otherwise

- Writing $X = (X_{1}, X_{2}, X_{3})$, we want to know $\text{Pr}(C_{1} | X = (\text{0 0 1}))$

--

- This classification problem is challenging because

  - Different number of covariates for each observation
  
  - Number of covariates can be large
  
--

- Is there any structure at all? 

---

### Motivating example: discuss!

- Based on our data, regardless of the sequence of flips, what did you observe for:

  - $\text{Pr}(C_{1})$ 
  
  - $\text{Pr}(C_{2})$

--

- Now let's consider the values of the flips. Given that we are using coin $C_{1}$, what did you observe

  - $\text{Pr}(X_{1} = 1 |C_{1})$?

  - $\text{Pr}(X_{1} = 1 |C_{2})$?

---

## Motivating example cont.

- Assume that given a coin, the results from the flips are independent

- $\text{Pr}(X = (\text{0 0 1}) | C_{1}) = \text{Pr}(X_{1} = 0 | C_{1}) \text{Pr}(X_{2} = 0 | C_{1})\text{Pr}(X_{3} = 1 | C_{1})$

--

- In summary, we have the following info:

  - $\text{Pr}(C_{1})$, $\text{Pr}(C_{2})$
  
  - $\text{Pr}(\text{flips} | C_{1})$, $\text{Pr}(\text{flips} | C_{2})$

- But we want $\text{Pr}(C_{1} | \text{flips})$. How can we use these quantities to estimate this probability?

---


## Bayes theorem


- For events $A$ and $B$: 

$$\text{Pr}(A|B) = \frac{\text{Pr}(B|A)\text{Pr}(A)}{\text{Pr}(B)}$$
--

- Letting $A$ be the event of coin $C_{1}$ and $B$ the $\text{flips}$:


$$
\begin{align*}
\text{Pr}(C_{1}| \text{flips}) &= \frac{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(\text{flips})} \\
&= \frac{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1})}{\sum_{j=1}^{2} \text{Pr}(\text{flips} | C_{j})\text{Pr}(C_{j})} \\
&= \frac{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1}) + \text{Pr}(\text{flips} | C_{2}) \text{Pr}(C_{2})} 
\end{align*}
$$

---
## Bayes theorem

- Return to general setting with $K$ possible class labels for $Y$

- We observe some data $X$

- Want to obtain $\text{Pr}(Y = k | X)$. How? Using Bayes theorem

$$
\begin{align*}
\text{Pr}(Y = k | X) &= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X)}} \\
&= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X | Y = 1)}\text{Pr}(Y = 1) + \ldots + \text{Pr(X | Y = K)}\text{Pr}(Y = K)} 
\end{align*}
$$
---

## Notation

- For remaining slides, let

  - $\pi_{k}(x) = Pr(Y = k)$ is marginal or **prior** probability for class $k$
  - $f_{k}(x) = Pr(X = x | Y =k)$ is the **density** for $X$ in class $k$
  
  $$Pr(Y=k| X =x) = \frac{\pi_{k}(x) f_{k}(x)}{\sum_{l=1}^{K} \pi_{l}(x) f_{l}(x)}$$

--

- Recall Bayes classifier: classifies an observation $x$ to the class for which $p_{k}(x) = Pr(Y = k |X=x)$ is largest
  
  - Will need to estimate the $f_{k}(x)$ to approximate Bayes classifier


---

## Discriminant analysis

- Dashed line is Bayes decision boundary between the two classes

```{r fig.align="center", fig.width = 10, fig.height=6}
sd <- 1; mu1 <- 1.5; mu2 <- -1.5
xrange <- seq(-5,5,0.1)
pi1 <- pi2 <- 0.5
d1 <- ((mu1^2 - mu2^2)/2 + sd^2 * log(pi2/pi1))/(mu1-mu2)
p1 <- data.frame(x = xrange, k1 = pi1*dnorm(xrange, mu1, sd), k2 = pi2*dnorm(xrange, mu2, sd)) %>%
  pivot_longer(cols = 2:3, names_to = "group", values_to = "y") %>%
  ggplot(., aes(x=x , y = y, col = group))+
  geom_path()+
  labs(y = "posterior density") +
  theme(text = element_text(size = 20))+
  guides(col = "none")+
  ylim(c(0,0.35))+
  geom_vline(xintercept = d1, linetype = "dashed")+
  ggtitle(bquote(pi[1] == ~.(pi1) ~ and ~ pi[2] == ~.(pi2)))
  # ggtitle(expression(paste(pi[1]," = " ,pi1)))

pi1 <- 0.2; pi2 <- 0.8
d2 <- ((mu1^2 - mu2^2)/2 + sd^2 * log(pi2/pi1))/(mu1-mu2)
p2 <- data.frame(x = xrange, k1 = pi1*dnorm(xrange, mu1, sd), k2 = pi2*dnorm(xrange, mu2, sd)) %>%
  pivot_longer(cols = 2:3, names_to = "group", values_to = "y") %>%
  ggplot(., aes(x=x , y = y, col = group))+
  geom_path()+
    labs(y = "posterior density") +
  theme(text = element_text(size = 20))+
  guides(col = "none")+
   ylim(c(0,0.35))+
  geom_vline(xintercept = d2, linetype = "dashed")+
  ggtitle(bquote(pi[1] == ~.(pi1) ~ and ~ pi[2] == ~.(pi2)))

gridExtra::grid.arrange(p1,p2, ncol = 2)

```


---

## Normal distribution

---

## Linear Discriminant Analysis

- First, consider the case of $p =1$ (one predictor)

- **Assumption 1**: $f_{k}(x)$ is normal distribution. 

  - With mean $\mu_{k}$ and variance $\sigma_{k}^{2}$ for class $k$,

$$f_{k}(x) = \frac{1}{\sqrt{2\pi\sigma_{k}^2}} \exp\left\{{-\frac{1}{2\sigma_{k}^2}(x - \mu_{k})^2}\right\}$$

--

- **Assumption 2**: $\sigma_{k}^2= \sigma^{2}$ for all classes $k$

--

- Plugging into the formula: $$p_{k}(x) = \frac{\pi_{k} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac{1}{2\sigma^2}(x - \mu_{k})^2\right\}}{\sum_{l=1}^{K} \pi_{l} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac{1}{2\sigma^2}(x - \mu_{l})^2\right\}}$$


---

## Linear Discriminant Analysis

- Taking logs and simplifying terms that do no depend on $k$, classifying based on largest $p_{k}(x)$ is equivalent to classifying based on largest **discriminant score**

$$\delta_{k}(x) = x \cdot \frac{\mu_{k}}{\sigma^2} - \frac{\mu_{k}^{2}}{2\sigma^2} + \log(\pi_{k})$$
--

- Bayes decision boundary are values $x^*$ where the discriminant scores are equal across the $K$ labels: 

$$\delta_{1}(x^*) = \delta_{2}(x^*) = \ldots = \delta_{K}(x^*)$$


---

## Linear discriminant analysis (LDA)

- We will typically need to estimate the $\mu_{k}$ and $\sigma^2$ from the data


- LDA uses the following estimates:

\begin{align*}
\hat{\mu}_{k} &= \frac{1}{n_{k}} \sum_{i:y_{i}= k} x_{i} \\
\hat{\sigma}^{2} &= \frac{1}{n-K} \sum_{k=1}^{K} \sum_{i: y_{i} = k} (x_{i} - \hat{\mu}_{k})^2 \\
\hat{\pi}_{k} &= \frac{n_{k}}{n} \qquad (**)
\end{align*}

such that:

$$\hat{\delta}_{k}(x)= x\cdot\frac{\hat{\mu}_{k}}{\hat{\sigma}} -\frac{\hat{\mu}_{k}^{2}}{2\hat{\sigma}^{2}} + \log(\hat{\pi}_{k})$$

---

### LDA: example

- Simulated $n=30$ data points belonging to one of $K = 2$ classes with a single predictor $x$ according to the following:

  - $f_{1}(x) = N(`r mu1`, `r sd^2`)$

  - $f_{2}(x) = N(`r mu2`, `r sd^2`)$
  
  - $\pi_{1} = `r pi1`$
  
  - $\pi_{2} = `r 1- pi1`$

--

- In the following slide, blue is class 1 and red is class 2
---

### LDA: example

- Solid line: true Bayes decision boundary. Dashed line: estimated decision boundary.

```{r fig.align="center", fig.height=6, fig.width=6}
set.seed(32)
n <- 30
n1 <- rbinom(1,n,pi1); n2 <-n-n1

x1 <- rnorm(n1,mu1,sd); x2 <-  rnorm(n2,mu2,sd)
lda_dat <- data.frame(x = c(x1,x2),
           class = c(rep("1",n1), rep("2",n2)))
mu1_hat <- mean(x1); mu2_hat <- mean(x2)
s2_hat <- (sum((x1-mu1_hat)^2) + sum((x2-mu2_hat)^2))/(n-2)
pi1_hat <- n1/n; pi2_hat <- n2/n

d2_hat <- ((mu1_hat^2 - mu2_hat^2)/2 + s2_hat * log(pi2_hat/pi1_hat))/(mu1_hat-mu2_hat)

lda_dat %>%
  ggplot(.,aes(x = x, fill = class))+
  geom_histogram(bins = 10, alpha = 0.5, position = "identity")+
  guides(fill = "none")+
  geom_vline(xintercept = d2) +
  geom_vline(xintercept = d2_hat, linetype = "dashed")+
  theme(text = element_text(size = 16))
```

---

## Linear discriminant analysis

- What about when $p>1$? 

- Extend LDA classifier by assuming $X = (X_{1}, X_{2}, \ldots, X_{p})$ is drawn from **multivariate normal** distribution

--

- Multivariate normal is characterized by mean vector $\mathbf{\mu}$ and covariance matrix $\Sigma$
  
  - $\mathbf{\mu} = (\mu_{1}, \mu_{2}, \ldots, \mu_{p})$

  - $\Sigma$ is $p \times p$ matrix which describes correlations between each components in $X$
  
--

$$f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(x - \mathbf{\mu})'\Sigma^{-1} (x - \mathbf{\mu})}$$


---

## Multivariate Normal

- Contour plots of two multivariate densities with $p=2$ and $\mathbf{\mu} = (0,0)$ (i.e. $MVN_{2}(\mu = (0,0)', \Sigma)$)

- Left: the two components are uncorrelated. Right: the two variables have correlation of 0.75.

```{r fig.align="center", fig.height=5, fig.width=8}
# set.seed(0)
p <- 2
mu <- c(0,0)
Sigma1 <- matrix(c(2,0,
                  0,2),by = T, ncol = p )
Sigma2 <- matrix(c(2,1.5,
                  1.5,2),by = T, ncol = p )
#simulate bivariate normal distribution
x1 <- seq(-5,5,0.1); x2 <- seq(-5,5,0.1)
f     <- function(x, y) dmnorm(cbind(x, y), mu, Sigma1)
z1     <- outer(x1, x2, f)
f     <- function(x, y) dmnorm(cbind(x, y), mu, Sigma2)
z2     <- outer(x1, x2, f)



# contour(x1, x2, z)
contour1 <- data.frame(x1 = expand.grid(x1,x2)[,1], x2 = expand.grid(x1,x2)[,2], z = c(z1)) %>%
 ggplot(., aes(x=x1,y=x2, z=z))+
  geom_contour(aes(colour = after_stat(level))) +
  guides(col = "none")
contour2 <- data.frame(x1 = expand.grid(x1,x2)[,1], x2 = expand.grid(x1,x2)[,2], z = c(z2)) %>%
 ggplot(., aes(x=x1,y=x2, z=z))+
  geom_contour(aes(colour = after_stat(level))) +
  guides(col = "none")

gridExtra::grid.arrange(contour1, contour2, ncol = 2)

# persp1 <- plot_ly(x= x1, y= x2,z = z1, type = "surface")
# persp2 <- plot_ly(x= x1, y= x2,z = z2, type = "surface")
```

---

## Linear discriminant analysis

- For $p >1$, LDA assumes that observations in class $k$ are drawn from $MVN_{p}(\mathbf{\mu}_{k}, \Sigma)$

  - Here, $\mathbf{\mu}_{k} = (\mu_{k1}, \mu_{k2}, \ldots, \mu_{kp})$ and $\Sigma$ is $p \times p$
  
--

- Plugging in $f_{k}(x) = MVN_{p}(\mathbf{\mu}_{k}, \Sigma)$ into $p_{k}(x)$ yields discriminant score

$$\delta_{k}(x)= x^{T}\Sigma^{-1}\mathbf{\mu}_{k} - \frac{1}{2}\mathbf{\mu}_{k}^{T}\Sigma^{-1} \mathbf{\mu}_{k} + \log(\pi_{k})$$
--

  - Despite this complicated-looking form, $\delta_{k}(x)$ is still linear in predictors $x_{1}, \ldots, x_{p}$
  
---

## LDA example

- Simulated data with $p = 2$ predictors and $K = 3$ classes. Each class has 20 observations

```{r fig.align = "center",  fig.width=6, fig.height=6}
n <- 60
set.seed(1)
K <- 3
p <- 2
n1 <- n2 <- n3 <- n/K
pi1 <- pi2 <- pi3 <- 1/3

mu1 <- c(2,1)
mu2 <- c(-1.5,-1)
mu3 <- c(0, 3)
mu_mat <- rbind(mu1,mu2,mu3)
Sigma <- matrix(c(2,0.5, 0.5, 2), byrow = T, ncol = p)
prec <- solve(Sigma)

sim_dat <- data.frame(rmvnorm(n1, mu_mat[1,], Sigma), class = "1") %>%
  add_row(data.frame(rmvnorm(n2, mu_mat[2,], Sigma), class = "2")) %>%
  add_row(data.frame(rmvnorm(n3, mu_mat[3,], Sigma), class = "3")) 

sim_dat %>%
  ggplot(., aes(x = X1, y = X2))+
  geom_point(aes(col = class)) +
  theme(text = element_text(size = 20))

```


---

### LDA example

```{r fig.align = "center",  fig.width=6, fig.height=6}
x1_min <- min(sim_dat$X1)-0.2; x1_max<- max(sim_dat$X1) + 0.2
x2_min <- min(sim_dat$X2)-0.2; x2_max <- max(sim_dat$X2) + 0.2

x2_range <- seq(x2_min, x2_max, 0.1)
dbs <- matrix(c(1,2, 2,3,1,3), byrow = T, ncol = p)
db_ls <- list()
for(j in 1:K){
  mu1_curr <- mu_mat[dbs[j,1],]
  mu2_curr <- mu_mat[dbs[j,2],]
  c1 <- 0.5*(t(mu1_curr - mu2_curr) %*% prec %*% (mu1_curr+mu2_curr) )[1,1]
  c2 <- (prec %*% (mu1_curr - mu2_curr))[2]
  c3 <-  (prec %*% (mu1_curr - mu2_curr))[1]
  db_ls[[j]] <- data.frame(x1 = (c1 - c2*x2_range )/c3, x2 = x2_range, 
                          boundary = paste0(dbs[j,1], "-", dbs[j,2])) %>%
    filter(x1 <= x1_max, x2 <= x2_max, x1 >= x1_min, x2 >= x2_min)
}

x1_x2_grid <- as.matrix(expand.grid(seq(x1_min, x2_max, 0.2), seq(x2_min, x2_max, 0.2)))
delta_ls <- list()
for(j in 1:K){
  delta_ls[[j]] <- data.frame(x1 = x1_x2_grid[,1], x2 = x1_x2_grid[,2],
                              delta = x1_x2_grid %*% prec %*% mu_mat[j,] -(0.5 *t(mu_mat[j,]) %*% prec %*%mu_mat[j,])[1,1],
                         class = j)
}

assign_df <- do.call(rbind, delta_ls) %>% 
  group_by(x1,x2) %>%
  mutate(max = max(delta)) %>% 
  # arrange(x1,x2) %>%
  ungroup() %>%
  filter(delta == max) %>%
  mutate(class = as.factor(class))

coeffs1 <- coefficients(lm(db_ls[[1]][,2]~ db_ls[[1]][,1]))
coeffs2 <- coefficients(lm(db_ls[[2]][,2]~ db_ls[[2]][,1]))
coeffs3 <- coefficients(lm(db_ls[[3]][,2]~ db_ls[[3]][,1]))
x_int <- (coeffs2[1]-coeffs1[1])/(coeffs1[2] - coeffs2[2])
y_int <- coeffs3[2]*x_int + coeffs3[1]

db_df <- rbind(db_ls[[1]] %>% 
 arrange(x2) %>%
  slice(1),
  db_ls[[2]] %>% 
  arrange(desc(x2)) %>% 
  slice(1), db_ls[[3]] %>% 
  arrange(desc(x2)) %>%
  slice(1)) %>%
  mutate(xend = x_int, yend = y_int)

lda_p1 <- sim_dat %>%
  ggplot(., aes(x = X1, y = X2))+
  geom_point(aes(col = class)) +
  geom_point(data = assign_df, aes(x = x1, y = x2, col = class), size = 0.2)+
  geom_segment(data = db_df, aes(x = x1, y = x2, xend = xend, yend = yend, group = boundary), 
               linetype = "dashed")+
  xlim(x1_min, x1_max)+
  ylim(x2_min,x2_max)+
  theme_minimal()+
  theme(text = element_text(size = 20))
lda_p1

```

Dashed lines are the Bayes decision boundaries. If known, would yield the fewest misclassification errors.

---

### LDA example

```{r lda_ests, fig.align = "center", fig.width=6, fig.height=6}
mu_ests <- sim_dat %>%
  group_by(class) %>%
  mutate(x1_mean = mean(X1), x2_mean = mean(X2)) %>%
  ungroup() %>%
  distinct(x1_mean, x2_mean, class) %>%
  dplyr::select(-1) %>%
  as.matrix

nvec <- c(n1,n2,n3)
Sp <- 0
for(j in 1:K){
  Sp <- Sp + nvec[j]* sim_dat %>%
    filter(class == as.character(j)) %>%
    dplyr::select(-class) %>%
    as.matrix %>%
    cov
}
Sp <- Sp/(n-K)
prec_est <- solve(Sp)

db_est_ls <- list()
for(j in 1:K){
  mu1_curr <- mu_ests[dbs[j,1],]
  mu2_curr <- mu_ests[dbs[j,2],]
  c1 <- 0.5*(t(mu1_curr - mu2_curr) %*% prec_est %*% (mu1_curr+mu2_curr) )[1,1]
  c2 <- (prec_est %*% (mu1_curr - mu2_curr))[2]
  c3 <-  (prec_est %*% (mu1_curr - mu2_curr))[1]
  db_est_ls[[j]] <- data.frame(x1 = (c1 - c2*x2_range )/c3, x2 = x2_range, 
                          boundary = paste0(dbs[j,1], "-", dbs[j,2])) %>%
    filter(x1 <= x1_max, x2 <= x2_max, x1 >= x1_min, x2 >= x2_min)
}


coeffs1 <- coefficients(lm(db_est_ls[[1]][,2]~ db_est_ls[[1]][,1]))
coeffs2 <- coefficients(lm(db_est_ls[[2]][,2]~ db_est_ls[[2]][,1]))
coeffs3 <- coefficients(lm(db_est_ls[[3]][,2]~ db_est_ls[[3]][,1]))
x_int <- (coeffs2[1]-coeffs1[1])/(coeffs1[2] - coeffs2[2])
y_int <- coeffs3[2]*x_int + coeffs3[1]

db_est_df <- rbind(db_est_ls[[1]] %>% 
 arrange(x2) %>%
  slice(1),
  db_ls[[2]] %>% 
  arrange(desc(x2)) %>% 
  slice(1), db_ls[[3]] %>% 
  arrange(desc(x2)) %>%
  slice(1)) %>%
  mutate(xend = x_int, yend = y_int)

lda_p1 +
  geom_segment(data = db_est_df, aes(x = x1, y = x2, xend = xend, yend = yend, group = boundary))
  
```

Solid lines are the estimated Bayesian decision boundaries.

---

## LDA: heart data


**Confusion matrix** of predicted (rows) vs true (columns) heart disease status using LDA

  - Predictors: $\color{blue}{\text{age}}, \color{blue}{\text{sex}}, \color{blue}{\text{chest pain}}$:

```{r}
heart_lda <- lda(target ~ age + sex + cp, heart)
heart_preds <- predict(heart_lda, heart)
heart_df <- data.frame(true = heart$target, pred = heart_preds$class, prob1 = heart_preds$posterior[,2])
n <- nrow(heart); n0 <- sum(heart$target==0); n1 <- n - n0

heart_df%>%
  dplyr::select(-prob1) %>%
  table() %>%
  t() %>%
  knitr::kable() %>%
  add_header_above(header = c(" " =1, "True" = 2)) 

fpr <- round(heart_df %>% filter(true == 0, pred==1) %>% nrow() / n0 , 3)
fnr <- round(heart_df %>% filter(true == 1, pred==0) %>% nrow() / n1 , 3)

```

- Misclassification rate: (`r heart_df %>% filter(true == 1, pred==0) %>% nrow()` + `r heart_df %>% filter(true == 0, pred==1) %>% nrow()`)/`r n` = `r round((heart_df %>% filter(true == 1, pred==0) %>% nrow() + heart_df %>% filter(true == 0, pred==1) %>% nrow())/ n,3)`

- Of the observations with true $\color{blue}{\text{heart disease}}$, we make `r heart_df %>% filter(true == 1, pred==0) %>% nrow()`/`r n1` = `r fpr` errors

- Of the observation with true $\color{blue}{\text{no heart disease}}$, we make `r heart_df %>% filter(true == 0, pred==1) %>% nrow()`/`r n0` = `r fnr` errors

---

## Types of errors

- **False positive rate** (FPR): fraction of negative observations incorrectly classified as positive -- `r fpr` in our example

- **False negative rate** (FNR): fraction of positive observations incorrectly classified as negative example -- `r fnr` in our example

--

- Is a false positive or a false negative worse? In the heart data, maybe a false negative?

--

- This table was produced by classifying an observation at $\color{blue}{\text{heart disease}}$ if $\hat{Pr}(\color{blue}{\text{heart disease}} | \color{blue}{\text{age}}, \color{blue}{\text{sex}}, \color{blue}{\text{chest pain}}) \geq 0.5$

  - Here, 0.5 is the threshold for assigning an observation to $\color{blue}{\text{heart disease}}$ class
  
  - Can change threshold to any value in $[0,1]$, which will affect error rates
  
---

## Varying threshold

```{r lda_threshold, fig.align="center", fig.width=8, fig.height=5}
th <- seq(0, 1, 0.025)
TT <- length(th)
err_mat <- matrix(NA, nrow = TT, ncol = 3)
for(t in 1:length(th)){
  fpr <- heart_df %>%
    mutate(pred = ifelse(prob1 >= th[t], 1, 0 )) %>%
    filter(true == 0, pred == 1)  %>%
    nrow() / n0
  fnr <- heart_df %>%
    mutate(pred = ifelse(prob1 >= th[t], 1, 0 )) %>%
    filter(true == 1, pred == 0)  %>%
    nrow() / n1
  
  oe <- heart_df %>%
    mutate(pred = ifelse(prob1 >= th[t], 1, 0 )) %>%
    filter(true != pred)  %>%
    nrow() / n
  err_mat[t,] <- c(fpr, fnr, oe)
}

as.data.frame(err_mat) %>%
  add_column(threshold = th) %>%
  rename("False positive" = 1, "False negative" = 2, "Overall error" = 3) %>%
  pivot_longer(cols = 1:3, names_to = "type", values_to = "error_rate") %>%
  ggplot(., aes(x = threshold, y = error_rate, col = type))+
  geom_path() +
  labs(y = "Error rate")+
  theme(legend.title = element_blank())
```

- Overall error rate minimized at threshold of 0.5
- How to decide a threshold rate?

---

## ROC Curve


- **sensitivity** (true positive rate) = probability of a positive, conditioned on truly being positive = 1 - FNR

- **specificity** (true negative rate) = probability of a negative, conditioned on truly being negative = 1 - FPR

- The **ROC curve** plots both simultaneously

```{r fig.align="center", fig.height=4, fig.width=4}
heart_roc <- roc(response = heart_df$true, predictor= as.numeric(heart_df$pred)-1, plot = F)
auc <- round(heart_roc$auc, 3)
ggroc(heart_roc)+
  ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')')) +
  geom_abline(slope = 1, intercept = 1, linetype = "dashed")
```

- **AUC** (area under the curve) summarizes overall performance

---

## Quadratic Discriminant Analysis (QDA)

- Similar to LDA, but lessens one assumption: allow a different covariance matrix (or variance) for each class $k$ 

  - Now, $f_{k}(X) =  MVN_{p}(\mathbf{\mu}_{k},\Sigma_{k})$ 

  - With QDA, discriminant function $\delta_{k}(x)$ has $x$ appearing as a quadratic function:
  
  $$\delta_{k}(x) = -\frac{1}{2}x^{T}\Sigma_{k}^{-1}x + x^{T}\Sigma_{k}^{-1} \mathbf{\mu}_{k} -\frac{1}{2} \mathbf{\mu}_{k}^{T}\Sigma_{k}^{-1} \mathbf{\mu}_{k} - \frac{1}{2}\log|\Sigma_{k}| + \log(\pi_{k})$$
  
--

- More flexible than LDA

---

## Example: simulated data

Simulated data under QDA model with $K=2$ and $p=2$ with $n_{1} = n_{2} = 50$.

```{r fns}
get_qda_db <- function(yrange, mu1, mu2, Sigma1, Sigma2){
 cc <- -0.5*(t(mu1) %*% Sigma1 %*% mu1 - t(mu2) %*% Sigma2 %*% mu2)-
 0.5*(log(det(Sigma1)) - log(det(Sigma2)))
 Vm <- c(solve(Sigma1) %*% mu1 - solve(Sigma2) %*% mu2)
 Sigma_star <- solve(Sigma1) - solve(Sigma2)
 a <- Sigma_star[1,1]
 bayes_df <- matrix(nrow = length(yrange), ncol = 2)
 for(i in 1:length(yrange)){
  xx2 <- yrange[i]
  b <-2*(xx2*Sigma_star[1,2]) -2 * Vm[1]
  c <- Sigma_star[2,2] * xx2^2 - 2*xx2 *Vm[2] - 2*cc
  xx1 <- (-b - sqrt(b^2 - 4*a*c))/(2*a)
  bayes_df[i,] <- c(xx1,xx2)
 }
 return(bayes_df)
}
get_lda_db <- function(yrange, mu1, mu2, Sigma){
 prec <- solve(Sigma)
 kk <- 0.5*t(mu1) %*% prec %*% mu1 - 0.5*t(mu2) %*% prec%*% mu2
 Vv <- c(prec %*% mu1 - prec %*% mu2)
 df <- matrix(0, nrow = length(yrange), ncol = 2)
 for(i in 1:length(yrange)){
  xx2 <- yrange[i]
  xx1 <- (kk - xx2*Vv[2])/Vv[1]
  df[i,] <- c(xx1,xx2)
 }
 df
}

.delta_qda <- function(x, mu, Sigma){
  prec <- solve(Sigma)
  -0.5*t(x) %*% prec %*% x + t(x) %*% prec %*% mu - 0.5*t(mu) %*% prec %*% mu -0.5*log(det(Sigma))
}
```


```{r fig.align="center", fig.width=5, fig.height=5}
set.seed(24)
K <- 2; p <-2
n <- 100
n1 <- n2 <- n/K
mu1 <- c(1,2); mu2 <- c(-1,0)
Sigma1 <- matrix(c(1,0.5,0.5,1),nrow = p); prec1 <- solve(Sigma1)
Sigma2 <- matrix(c(0.75,-.25,-.25,0.75),nrow = p); prec2 <- solve(Sigma2)

x1 <- rmvnorm(n1, mu1, Sigma1); x2 <-  rmvnorm(n2, mu2, Sigma2)
pi1 <- pi2 <- 0.5

mu1_hat <- colMeans(x1); mu2_hat <- colMeans(x2)
Sigma1_hat <- cov(x1); Sigma2_hat <- cov(x2)

points_df <- data.frame(rbind(x1, x2)) %>%
 mutate(class = c(rep("1", n1), rep("2", n2))) 

yrange <- seq(min(points_df$X2), max(points_df$X2)+0.1, 0.01)
xrange2 <- seq(min(points_df$X1)-0.2, max(points_df$X1)+0.2, 0.1)
yrange2 <- seq(min(points_df$X2)-0.2, max(points_df$X2)+0.2, 0.1)

plot_df <- data.frame(get_qda_db(sort(union(yrange,yrange2)), mu1,mu2,Sigma1,Sigma2)) # true Bayes QDA
qda_plot_df <- data.frame(get_qda_db(sort(union(yrange,yrange2)),  mu1_hat,mu2_hat,Sigma1_hat,Sigma2_hat)) # estiamted db under QDA


Sp <- 0
nvec <- c(n1,n2)
for(j in 1:K){
  Sp <- Sp + nvec[j]* points_df %>%
    filter(class == as.character(j)) %>%
    dplyr::select(-class) %>%
    as.matrix %>%
    cov
}
Sp <- Sp/(n-K)


lda_plot_df <- data.frame(get_lda_db(yrange, colMeans(x1), colMeans(x2), Sp)) # est db under LDA

background_df <-expand.grid(xrange2,yrange2) %>%
 data.frame() %>%
 left_join(., plot_df %>% rename( "Var2" = 2), by = "Var2") %>% 
 mutate(class = ifelse(Var1 >= X1, "1", "2"))
 

points_df %>%
 ggplot(., aes(x=X1, y = X2))+
 geom_point(aes(col = class), size = 1.5)+
  geom_point(data = background_df, aes(x = Var1, y = Var2, col = class), size = 0.1)+
 geom_path(data = plot_df , aes(x = X1, y = X2), col = "purple") +
 geom_path(data = qda_plot_df , aes(x = X1, y = X2), linetype = "dashed")+
 geom_path(data = lda_plot_df,  aes(x = X1, y = X2), linetype = "dashed", col = "blue")+
 xlim(min(points_df$X1)-0.2, max(points_df$X1)+0.2) +
 ylim(min(points_df$X2)-0.2, max(points_df$X2)+0.2) +
 guides(col = "none") +
 ggtitle("Decision boundaries")
```

- Purple curve is Bayes decision boundary, dashed black curve is estimated decision boundary under QDA, and dashed blue line is estimate under LDA

---

## Naive Bayes

- Up until now, we have assumed that for $p>1$, $f_{k}(x)$ is $p$-dimensional distribution

- Under **naive Bayes**, we do not assume a specific family of distribution. Instead, assume that within class $k$, the $p$ predictors are independent:

$$f_{k}(x) = f_{k1}(x_{1}) \times f_{k2}(x_{2}) \times \cdots \times f_{kp}(x_{p}) = \prod_{j=1}^{p}f_{kj}(x_{j})$$

--

- Useful when $p$ is large


---

## Naive Bayes

- If we assume each $f_{kj}$ is Normal, then this is QDA with diagonal $\Sigma_{k}$

  - Under Normal (Gaussian) naive Bayes:

$$\delta_{k}(x) = -\frac{1}{2}\sum_{j=1}^{p}\left(\frac{(x_{j}-\mu_{kj})^2}{\sigma_{kj}^{2}} + \log\sigma_{kj}^{2}  \right) + \log(\pi_{k})$$

--

- Allows for *mixed feature types* (qualitative and quantitative)

---

## Logistic Regression vs LDA

- Return back to $K=2$ setting (binary classification) 

- Recall in logistic regression: $$\log\left(\frac{p_{x}}{1 - p_{x}}\right) = \log\left( \frac{Pr(Y=1 | X=x)}{Pr(Y=2 | X=x)}\right)= \beta_{0} + \beta_{1}x_{1} + \ldots + \beta_{p}x_{p}$$
--

- In LDA, 
\begin{align*}
\log\left(\frac{Pr(Y= 1 | X=x)}{Pr(Y= 2 | X=x)}\right) &= \log \left(\frac{\pi_{1} f_{1}(x)}{\pi_{2}f_{2}(x)}\right) \\
&= \log\left(\frac{\pi_{1} \exp\left(-\frac{1}{2}(x - \mathbf{\mu}_{1})^{T} \Sigma^{-1}(x - \mathbf{\mu}_{1})\right)}{\pi_{2} \exp\left(-\frac{1}{2}(x - \mathbf{\mu}_{2})^{T} \Sigma^{-1}(x - \mathbf{\mu}_{2})\right))}\right) \\
&= \ldots \\
&= c_{0} + c_{1}x_{1}+ c_{2}x_{2} + \ldots + c_{p}x_{p}
\end{align*}

---

## Logistic Regression vs LDA

- LDA has same form of logistic regression for log-odds

- Differ in how parameters  are estimated 

  - Logistic regression uses conditional likelihood $Pr(Y|X)$
  - LDA uses full likelihood $Pr(X,Y)$
  
- Often similar results
  
---

## Summary

- Logistic regression is very commonly used when $K=2$

- LDA useful when $n$ small, or the classes are well
separated, and Gaussian assumptions are reasonable

- Naive Bayes useful when $p$ very large
