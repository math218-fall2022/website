---
title: "Math 218: Statistical Learning"
author: "Unupervised learning: Hierarchical clustering"
date: "9/14/2022"
output: 
   xaringan::moon_reader:
    css: "math218-slides.css"
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
      slideNumberFormat: "%current%"  
editor_options: 
  chunk_output_type: console
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, fig.align = "center", fig.width = 5, fig.height = 5)

library(tidyverse)
library(ISLR)
library(cluster)    # clustering algorithms
library(factoextra) # clustering algorithms & visualization
library(ggdendro)
```

## Houskeeping

---

## Unsupervised learning goals

- Discover interesting things about the predictors/features
  
  - Can we discover subgroups among the variables or among the observations?
  
  - Is there an informative way to visualize the data?

  
--

- Two common methods: 

  - **Clustering**: broad class of methods to discover unknown subgroups in your data
  
  - **Principal components analysis (PCA)**:  


---

## Clustering

- We want to find *subgroups* or *clusters* in a data set

- This means separating observations into distinct groups such that:

  - observations with each group are similar
  
  - observation across groups are different
  

--

- How to define "similar" and "different"?

--

- Two main methods: **K-means clustering** (last week) and **hierarchical clustering** (this week)


---
class: center, middle

# Hierarchical Clustering

---

## Hierarchical Clustering

- Disadvantage of $K$-means is pre-specifying $K$!

- What if we don't want to commit to a single choice?

- We will construct a (upside-down) tree-based representation of the observations *dendrogram* 

- Specifically focus on *bottom-up* hierarchical clustering, by constructing leaves first and then the trunk

---

## Intuition/idea

- Rather than clustering everything at beginning, we will build iteratively 


---

### Hierarchical clustering "algorithm"

The approach in words:

- Start with each point in its own cluster

- Identify the "closest" two clusters and merge them together

- Repeat

- Finish when all points have eventually been combined into a single cluster

---

## Dendrogram

- Each *leaf* of the dendrogram is one of the $n$ observations

- As we move up the tree, some leaves fuse into branches

  - These correspond to observations that similar to each other

- Branches and/or leaves will fuse as we move the tree



---

## Dendrogram

- The earlier (lower in the tree) fusions occur, the more similar the groups

- For any two observations, look for the point in tree where branches containing these two observations are first fused

  - The height of this fusion on *vertical* axis indicates how *different* they are
  
--

- **Caution!** Cannot draw conclusions about similar based on proximity of observations on the *horizontal* axis 

  - Why?
  
---

## Identifying clusters

- To identify clusters based on dendrogram, simply make a horizontal cut across dendrogram

- Distinct sets of observations beneath the cut are interpreted as clusters


- This makes hierarchical clustering attractive: one single dendrogram can be used to obtain *any* number of clusters

  - What do people do in practice?
  
---

## "Hierarchical"

- Clusters obtained by cutting at a given are *nested* within the clusters obtained by cutting at any greater height

- Is this realistic?

---

### Hierarchical clustering algorithm

- Need to define some sort of *dissimilarity* measure between pairs of observations

  - e.g. Euclidean distance

--

1. Begin with $n$ observations and a measure of all the $\binom{n}{2}$ pairwise dissimilarities. Treat each observation as its own cluster.

--

2. For $i = n, n-1, n-2,\ldots, 2$:

  i) Examine all pairwise *inter-cluster* dissimilarities among the $i$ clusters and identify the pair of clusters that are least dissimilar. Fuse these two clusters. Their dissimiliarity is the height in the dendrogram where the fusion should be placed.
  
  ii) Compute the new pairwise inter-cluster dissimilarities among the remaining $i-1$ remaining clusters
  
---

## Dissimilarity between groups

- How do we define dissimilarity between two clusters if one or both contains multiple observations?

  - i.e. How did we determine that $\{A,C\}$ should be fused with $\{B\}$?

--

- Develop the notion of *linkage*, which defines dissimilarity between two groups of observations

---

## Common linkage types

- Complete: maximal intercluster dissimilarity. Compute all pairwise dissimilarities between observations in cluster $A$ and observations in cluster $B$. Record the *largest* of these dissimilarities.

--

- Single: minimal intercluster dissimilarity.  Compute all pairwise dissimilarities between observations in cluster $A$ and observations in cluster $B$. Record the *smallest* of these dissimilarities. 

--

- Average: mean intercluster dissimilarity.  Compute all pairwise dissimilarities between observations in cluster $A$ and observations in cluster $B$. Record the *average* of these dissimilarities.

--

- Centroid: dissimilarity between the centroid for cluster $A$ and the centroid for cluster $B$

---

## Common linkage types

- Average and complete are generally preferred over single linkage

  - Tend to yield more balanced dendrograms
  
- Centroid linkage often used in genomics

  - Drawback of inversion, where two clusters are fused at a height *below* either of the individual clusters
  
---

## Example

- Generated `n=50` observations, evenly split between two true clusters

```{r}
set.seed(22)
n <- 50
X <- matrix(rnorm(n*2), ncol = 2) 
X[1:25, 1] <- X[1:25, 1] + 1
X[1:25, 2] <- X[1:25, 2] - 2
```

- `hclust()` performed hierarchical clustering. Requires a dissimilarity structure `d` for the features obtained by `dist()`, and the `method` of linkage

```{r eval = F, echo = T}
# X is our matrix of features
hc_complete <- hclust(d = dist(X), method = "complete")
```

---

```{r fig.align = "center", fig.width = 9, fig.height=7, cache = T}
library(ggdendro)
set.seed(1)
hc_complete <- hclust(dist(X), method = "complete")
hc_avg <- hclust(dist(X), method = "average")
hc_single <- hclust(dist(X), method = "single")

ggdendrogram(hc_complete, rotate = FALSE, size = 2) +
  theme(text =element_text(size = 20))+
  labs(title = "Complete linkage")
```

---


```{r fig.align = "center", fig.width = 9, fig.height=7}
ggdendrogram(hc_avg, rotate = FALSE, size = 2)+
  theme(text =element_text(size = 20))+
  labs(title = "Average linkage")
```

---


```{r fig.align = "center", fig.width = 9, fig.height=7}

ggdendrogram(hc_single, rotate = FALSE, size = 2)+
  theme(text =element_text(size = 20)) +
  labs(title = "Single linkage")
```

---

## Cutting dendrogram

```{r echo = T}
cutree(hc_complete, 4)
cutree(hc_avg, 4)
cutree(hc_single, 4)
```



---

## Consensus clustering

- We cannot validate clusters, so how can we "feel good" about our cluster assignments?

- Want to measure the *stability* of our clusterings; how robust are they to small perturbations?

  - If the data represent a sample of items drawn from distinct sub-populations, and if we were to observe a different sample
drawn from the same sub-populations, the induced cluster composition and number should
not be radically different

- One way might be to use **consensus clustering**, which combines/aggregates multiple clusterings together

---

## Consensus clustering

- Unfortunately, we cannot obtain a different sample, BUT we can **resample**

- We can perturb or resample datasets, apply the same clustering algorithm, and see if there is agreement among the clustering runs over the datasets

  - Kind of like bagging for trees

--

- How? Perturb by adding a little bit of noise, or resample using the bootstrap!

---


## Consensus clustering

- How would we use these ideas to compare models?

- For repeated clusterings, record the proportion of times observation $i$ and $i'$ $(i \neq i')$ are in the same cluster 

--

- If our clusterings are stable, what values would these proportions take on?


---

## Consensus clustering: discuss!

- Let's say we are performing hierarchical clustering and need to decide how many clusters to report. 

- Discuss: how would we perturb the data to perform consensus clustering?

- Discuss: how would we use the bootstrap to perform consensus clustering?

---

## Example: perturbing

- Also known as semiparametric bootstrapping

```{r cache = T}
data("iris")

B <- 3000
n<- nrow(iris)
match_mat2 <- match_mat3 <- match_mat4 <- matrix(0, nrow= n, ncol = n)

# semiparametric bootstrap
for(b in 1:B){
  # dat_jitter <- scale(iris[,1:4]) + rnorm(50*4, 0, 0.05)
    dat_jitter <- iris[,1:4] + rnorm(50*4, 0, 0.05)
  hc_out <- hclust(dist(dat_jitter), method = "complete")
  clusts2 <- cutree(hc_out, 2)
  clusts3 <- cutree(hc_out, 3)
  clusts4 <- cutree(hc_out, 4)
  for(i in 1:(n-1)){
      for(j in (i+1):n){
        match_mat2[i,j] <- match_mat2[i,j] + 1*(clusts2[i] == clusts2[j])
        match_mat3[i,j] <- match_mat3[i,j] + 1*(clusts3[i] == clusts3[j])
        match_mat4[i,j] <- match_mat4[i,j] + 1*(clusts4[i] == clusts4[j])

      }
  }
}

M2<- match_mat2/B; M3 <- match_mat3/B; M4 <- match_mat4/B
```

```{r fig.align="center", fig.width=9, fig.height=6}
df <- data.frame(K2 = c(M2[upper.tri(M2)]), K3 = c(M3[upper.tri(M3)]), K4 = c( M4[upper.tri(M4)])) %>%
  pivot_longer(cols = 1:3, names_to = "K", values_to = "proportion")  %>%
  mutate(K = case_when(K == "K2" ~ "2",
                       K == "K3" ~ "3",
                       K == "K4" ~ "4"))
df %>%
  ggplot(., aes(x = proportion)) +
  geom_histogram(bins = 20) +
  facet_wrap(~ K) +
  ggtitle("Semiparametric bootstrap Consensus Clustering", subtitle = "Same cluster assignment") +
  theme(text =element_text(size=20))
```

--

- Is this plot "fair"?

---

```{r fig.align="center", fig.width=9, fig.height=6}
M2_sort <- sort(M2[upper.tri(M2)])
M3_sort <- sort(M3[upper.tri(M3)])
M4_sort <- sort(M4[upper.tri(M4)])
e2 <- ecdf(M2[upper.tri(M2)]); e3 <-  ecdf(M3[upper.tri(M3)]); e4 <- ecdf(M4[upper.tri(M4)])
auc_mat <- matrix(0, nrow = length(M2_sort), ncol = 3)
for(i in 2:length(M2_sort)){
  auc_mat[i,] <- c((M2_sort[i] - M2_sort[i-1])*e2(M2_sort[i]),
                   (M3_sort[i] - M3_sort[i-1])*e3(M3_sort[i]),
                   (M4_sort[i] - M4_sort[i-1])*e4(M4_sort[i]))
}
aucs <- colSums(auc_mat)

ggplot(df, aes(x = proportion, col = K)) +
  stat_ecdf(geom = "step")  +
  ggtitle("Semiparametric Bootstrap: Empirical CDFs", subtitle = "For proportion of cluster aggreement") +
  labs(y = "Probability")+
  annotate("text", x=0.7, y=0.3, label= round(aucs[3], 3), col = "#619CFF" ) +
  annotate("text", x=0.7, y=0.2, label= round(aucs[2], 3), col = "#00BA38" ) +
  annotate("text", x=0.7, y=0.1, label= round(aucs[1], 3), col = "#F8766D" ) +
  annotate("text", x=0.75, y=0.4, label= "Area under curve:")+
   theme(text =element_text(size=20))
```


```{r eval = F, echo = F}

## proportion increase
colSums(auc_mat)[1]

(colSums(auc_mat)[2] - colSums(auc_mat)[1])/(colSums(auc_mat)[1])
(colSums(auc_mat)[3] - colSums(auc_mat)[2])/(colSums(auc_mat)[2])

```

---

## Example: 0.632 bootstrap

```{r cache = T}
set.seed(15)
B <- 3000
match_mat2 <- match_mat3 <- match_mat4 <- counts2 <- counts3 <- counts4 <-  matrix(0, nrow= n, ncol = n)
# .632 bootstrap
for(b in 1:B){
  ids <- sample(1:n, round((1-1/exp(1))*n), replace = F)
  dat<- iris[ids,1:4] 
  hc_out <- hclust(dist(dat), method = "complete")
  clusts2 <- cutree(hc_out, 2)
  clusts3 <- cutree(hc_out, 3)
  clusts4 <- cutree(hc_out, 4)
  for(i in 1:(n-1)){
    if(i %in% ids){
      i_id <- which(ids == i); 
      for(j in (i+1):n){
        if(j %in% ids){
          j_id <- which(ids == j)
          match_mat2[i,j] <- match_mat2[i,j] + 1*(clusts2[j_id] == clusts2[i_id])
          counts2[i,j] <- counts2[i,j] + 1
          match_mat3[i,j] <- match_mat3[i,j] + 1*(clusts3[j_id] == clusts3[i_id])
          counts3[i,j] <- counts3[i,j] + 1
          match_mat4[i,j] <- match_mat4[i,j] + 1*(clusts4[j_id] == clusts4[i_id])
          counts4[i,j] <- counts4[i,j] + 1          
        }
        
      }
    }

  }
}
M2 <-  match_mat2/counts2; M3 <- match_mat3/counts3; M4 <- match_mat4/counts4

```

```{r fig.align="center", fig.width=9, fig.height=6}
df <- data.frame(K2 = c(M2[upper.tri(M2)]), K3 = c(M3[upper.tri(M3)]), K4 = c( M4[upper.tri(M4)])) %>%
  pivot_longer(cols = 1:3, names_to = "K", values_to = "proportion")  %>%
  mutate(K = case_when(K == "K2" ~ "2",
                       K == "K3" ~ "3",
                       K == "K4" ~ "4"))
df %>%
  ggplot(., aes(x = proportion)) +
  geom_histogram(bins = 20) +
  facet_wrap(~ K) +
  ggtitle("0.632 bootstrap Consensus Clustering", subtitle = "Same cluster assignment")+
   theme(text =element_text(size=20))
```


---

```{r fig.align="center", fig.width=9, fig.height=6}
M2_sort <- sort(M2[upper.tri(M2)])
M3_sort <- sort(M3[upper.tri(M3)])
M4_sort <- sort(M4[upper.tri(M4)])
e2 <- ecdf(M2[upper.tri(M2)]); e3 <-  ecdf(M3[upper.tri(M3)]); e4 <- ecdf(M4[upper.tri(M4)])

auc_mat <- matrix(0, nrow = length(M2_sort), ncol = 3)
for(i in 2:length(M2_sort)){
  auc_mat[i,] <- c((M2_sort[i] - M2_sort[i-1])*e2(M2_sort[i]),
                   (M3_sort[i] - M3_sort[i-1])*e3(M3_sort[i]),
                   (M4_sort[i] - M4_sort[i-1])*e4(M4_sort[i]))
}
aucs <- colSums(auc_mat)

ggplot(df, aes(x = proportion, col = K)) +
  stat_ecdf(geom = "step")  +
  ggtitle("0.632 Bootstrap: Empirical CDFs", subtitle = "For proportion of cluster aggreement") +
  labs(y = "Probability")+
  annotate("text", x=0.7, y=0.3, label= round(aucs[3], 3), col = "#619CFF" ) +
  annotate("text", x=0.7, y=0.2, label= round(aucs[2], 3), col = "#00BA38" ) +
  annotate("text", x=0.7, y=0.1, label= round(aucs[1], 3), col = "#F8766D" ) +
  annotate("text", x=0.75, y=0.4, label= "Area under curve:")+
   theme(text =element_text(size=20))
```

---

## Example: circle data again

```{r}
set.seed(3)
n <- 200
r <- 3
alpha <- 2 * pi * runif(n)
x1 <- r * cos(alpha) + 0 + rnorm(n, 0, 0.2)
y1 <- r * sin(alpha) + 0 + rnorm(n, 0, 0.2)
r <- 0.5
alpha <- 2 * pi * runif(n)
x2 <- r * cos(alpha) + 0 + rnorm(n, 0, 0.2)
y2 <- r * sin(alpha) + 0 + rnorm(n, 0, 0.2)
df <- data.frame(x = c(x1,x2), y = c(y1,y2))
df %>%
  ggplot(., aes(x=x,y=y)) +
  geom_point()
```

---

```{r}
hc_out <- hclust(dist(df[,1:2]), method = "complete")
df %>%
  mutate(cluster = factor(cutree(hc_out, 2))) %>%
  ggplot(., aes(x =x, y=y)) +
  geom_point(aes(col = cluster)) +
  geom_point(data = data.frame(km_out$centers), mapping = aes(x = x, y = y),
             pch = 4, size = 4) +
  guides(col = "none") +
  ggtitle("Complete Linkage")
```
---

```{r}
hc_out <- hclust(dist(df[,1:2]), method = "single")
df %>%
  mutate(cluster = factor(cutree(hc_out, 2))) %>%
  ggplot(., aes(x =x, y=y)) +
  geom_point(aes(col = cluster)) +
  geom_point(data = data.frame(km_out$centers), mapping = aes(x = x, y = y),
             pch = 4, size = 4) +
  guides(col = "none") +
  ggtitle("Single Linkage")
```

---

## Considerations

- We have mostly seen Euclidean distance as the choice of dissimilarity measure

- *Correlation-based distance* considers two observations to be similar if their features are highly correlated (will see in lab)

  - Focuses on the shapes of observation profiles, rather than their magnitudes
  
--

- Choice of dissimilarity measure has strong effect on resulting dendrogram!

  - Therefore, must think critically about the type of data being clustered AND the scientific question at hand
  
--

- Should we scale the variables?

---

## Summary: decisions

- Should the features be standardized?


- What dissimilarity measured should we use?

- What type of linkage should we use?

- Where should we cut the dendrogram?

---

## Summary: ongoing considerations

- How do we validate the clusters we obtained?

  - Are we truly discovering subgroups, or are we simply clustering the noise?

- Do all observations belong in a cluster? Or are some actually "outliers"

- Clustering methods generally not robust to small changes in data

---

## Summary: recommendations

- Small decisions can lead to large changes!

- Recommend performing clustering with different choices of these parameters/options, and looking to see if/which patterns consistently appear

- Because clustering not robust, maybe we consider clustering subsets of the data 

- Caution: be careful about how the results of a clustering analysis are reported. These results should not be taken as the absolute truth about a data set!!!