<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Linear Model Selection &amp; Regularization" />
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Math 218: Statistical Learning
### Linear Model Selection &amp; Regularization
### 9/12/2022

---




class: center, middle

# Housekeeping

---

### Linear Model Selection &amp; Regularization

- Recall linear model:

`$$Y = \beta_{0} + \beta_{1} X_{1} + \ldots + \beta_{p} X_{p} + \epsilon$$`

- We will discuss extending the linear model framework

--

- Alternative fitting procedures can

  - Improve prediction accuracy
  
  - Improve model interpretability
  
---

## Why alternatives?

- Prediction accuracy: if `\(n\)` only slightly larger than `\(p\)`, leasy-squares fit will have high variability

  - If `\(p &gt; n\)`, cannot use least-squares
  
--

- Interpretability: removing some irrelevant features/predictors can reduce complexity

  - How? Set corresponding coefficients to 0. This is known as **feature selection**
  
---

## Three popular classes of methods

1. **Subset selection**: identify a subset of the `\(p\)` possible predictors that we believe to be related to `\(Y\)`, then fit least-squares model

--

2. **Shrinkage** or **regularization**: fit a model involving all `\(p\)` predictors, but the estimated coefficients are shrunken towards 0 relative to least-squares estimates

  - Can reduce variance and perform variable selection
  
--

3. **Dimension reduction**: project the `\(p\)` predictors into a `\(q\)`-dimensional space where `\(q &lt; p\)`

  - Achieved using `\(q\)` linear combinations or projections of the `\(p\)` predictors, then fit least squares on the `\(q\)` projections
  
---

## Subset Selection

- Different methods to perform subset selection: best subset and stepwise

- **Best subset** selection fits a separate least-squares regression for each possible combination of the `\(p\)` predictors
  - Look at all models with goal of identifying the *best*
  
--

- Fit one model with zero predictors, `\(p\)` models with exactly one predictor, `\(\binom{p}{2} = p(p-1)/2\)` models with exactly two, etc.

---

## Best subset selection

Algorithm:

1. Let `\(\mathcal{M}_{0}\)` denote the *null model* with no predictors `\((Y = \beta_{0}) + \epsilon\)`.

--
  
2. For `\(k = 1,2, \ldots, p\)`:

  a) Fit all `\(\binom{p}{k}\)` models that contain exactly `\(k\)` predictors
  
--

  b) Pick the best among these `\(\binom{p}{k}\)` models (best according to `\(R^2\)`). Call it `\(\mathcal{M}_{k}\)` 
  
--

3. Select a single best (**) model from the `\(\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}\)` models 

---

## Example: diabetes data



- Researchers collected data on 442 daibetes patients examining the disease progression one year after baseline

- Predictors: age, sex, BMI, blood pressure, total serum cholesterol, low-density lipoproteins, high-density lipoproteins, total cholestrol/HDL, log of serum triglycerides, glucose

- Response: disease progression
---

## Example: diabetes data

Find best subset model for `\(\color{blue}{\text{LC50}}\)` from the six possible predictors:


```
## Subset selection object
## Call: regsubsets.formula(y ~ ., data = diabetes, nvmax = p)
## 10 Variables  (and intercept)
##     Forced in Forced out
## age     FALSE      FALSE
## sex     FALSE      FALSE
## bmi     FALSE      FALSE
## bp      FALSE      FALSE
## tc      FALSE      FALSE
## ldl     FALSE      FALSE
## hdl     FALSE      FALSE
## tch     FALSE      FALSE
## ltg     FALSE      FALSE
## glu     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: exhaustive
##           age sex bmi bp  tc  ldl hdl tch ltg glu
## 1  ( 1 )  " " " " "*" " " " " " " " " " " " " " "
## 2  ( 1 )  " " " " "*" " " " " " " " " " " "*" " "
## 3  ( 1 )  " " " " "*" "*" " " " " " " " " "*" " "
## 4  ( 1 )  " " " " "*" "*" "*" " " " " " " "*" " "
## 5  ( 1 )  " " "*" "*" "*" " " " " "*" " " "*" " "
## 6  ( 1 )  " " "*" "*" "*" "*" "*" " " " " "*" " "
## 7  ( 1 )  " " "*" "*" "*" "*" "*" " " "*" "*" " "
## 8  ( 1 )  " " "*" "*" "*" "*" "*" " " "*" "*" "*"
## 9  ( 1 )  " " "*" "*" "*" "*" "*" "*" "*" "*" "*"
## 10  ( 1 ) "*" "*" "*" "*" "*" "*" "*" "*" "*" "*"
```



---

## Best subset selection

- Presented for least squares, but same ideas apply to other models 
  
  - E.g.: logistic regression uses *deviance* in place of `\(R^2\)` or RSS (smaller deviance)
  
--

Issues:

  - For computational reasons, best subset selection cannot be applied for large `\(p\)`

  - When `\(p\)` large, higher chance of finding models that fit training data well but might not have predictive power (overfitting)
  
---

## Stepwise methods

- Stepwise methods explore a more restricted set of models

- **Forward stepwise** selection: begins with null model with no predictors, then adds predictors to the model one at a time

  - At each step, the variable that gives the greatest *additional* improvement to the fit is added

---

## Forward stepwise selection

Algorithm:

1. Let `\(\mathcal{M}_{0}\)` denote the null model with no predictors

--

2. For `\(k = 0, 1, \ldots, p-1\)`:

  a) Consider all `\(p-k\)` models that augment the predictors in `\(\mathcal{M}_{k}\)` with one additional predictor
  
--
  
  b) Choose the best (according to RSS or `\(R^2\)`) among these `\(p-k\)` models, and call it `\(\mathcal{M}_{k+1}\)`
  
--
  
3. Select a single best (**) model from the models `\(\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}\)`

---

## Forward stepwise selection

- For example, pretend we have `\(p = 3\)` possible predictors: `\(X_{1}, X_{2}, X_{3}\)`

- `\(\mathcal{M}_{0}\)`: `\(Y \approx \beta_{0}\)`

- Now `\(k= 0\)`, and we consider all `\(p-k = 3\)` models that add an additional predictor to our current working model

  - `\(Y \approx \beta_{0} + \beta_{1}X_{1} \Rightarrow R^2 = 0.2\)`
  
  - `\(Y \approx \beta_{0} + \beta_{1}X_{2}  \Rightarrow R^2 = 0.3\)`
  
  - `\(Y \approx \beta_{0} + \beta_{1}X_{3} \Rightarrow R^2 = 0.1\)`
  
--

- Set `\(\mathcal{M}_{1}\)`: `\(Y \approx \beta_{0} + \beta_{1}X_{2}\)`

- Now `\(k=1\)`, and consider all `\(p - k = 2\)` models that add an additional predictor:

  - `\(Y \approx \beta_{0} + \beta_{1}X_{2} + \beta_{2} X_{1}\)`
  
  - `\(Y \approx \beta_{0} + \beta_{1}X_{2} + \beta_{2} X_{3}\)`
  
---

## Forward stepwise selection


```
## Subset selection object
## Call: regsubsets.formula(y ~ ., data = diabetes, nvmax = p)
## 10 Variables  (and intercept)
##     Forced in Forced out
## age     FALSE      FALSE
## sex     FALSE      FALSE
## bmi     FALSE      FALSE
## bp      FALSE      FALSE
## tc      FALSE      FALSE
## ldl     FALSE      FALSE
## hdl     FALSE      FALSE
## tch     FALSE      FALSE
## ltg     FALSE      FALSE
## glu     FALSE      FALSE
## 1 subsets of each size up to 10
## Selection Algorithm: exhaustive
##           age sex bmi bp  tc  ldl hdl tch ltg glu
## 1  ( 1 )  " " " " "*" " " " " " " " " " " " " " "
## 2  ( 1 )  " " " " "*" " " " " " " " " " " "*" " "
## 3  ( 1 )  " " " " "*" "*" " " " " " " " " "*" " "
## 4  ( 1 )  " " " " "*" "*" "*" " " " " " " "*" " "
## 5  ( 1 )  " " "*" "*" "*" " " " " "*" " " "*" " "
## 6  ( 1 )  " " "*" "*" "*" "*" "*" " " " " "*" " "
## 7  ( 1 )  " " "*" "*" "*" "*" "*" " " "*" "*" " "
## 8  ( 1 )  " " "*" "*" "*" "*" "*" " " "*" "*" "*"
## 9  ( 1 )  " " "*" "*" "*" "*" "*" "*" "*" "*" "*"
## 10  ( 1 ) "*" "*" "*" "*" "*" "*" "*" "*" "*" "*"
```
  
---

## Forward stepwise selection

- Computationally advantous to best subset selection

- Works when `\(p &gt; n\)`, but can only ocnstruct submodels `\(\mathcal{M}_{0}, \ldots, \mathcal{M}_{n-1}\)`

- Not guaranteed to find the best possible model out of all `\(2^{p}\)` models -- why?



---

## Backward stepwise selection

- **Backward stepwise selection** is another alternative to best subset selection

- Begins with full least squares model containing all `\(p\)` predictors, and iteratively removes the least useful predictor one at a time

---

## Backward stepwise selection

Algorithm:

1. Let `\(\mathcal{M}_{p}\)` denote the full model with all `\(p\)` predictors

--

2. For `\(k = p, p-1, \ldots, 1\)`:

  a) Consider all `\(k\)` models that contain all but one of the predictors in `\(\mathcal{M}_{k}\)`, for a total of `\(k-1\)` predictors
  
--
  
  b) Choose the best (according to RSS or `\(R^2\)`) among these `\(k\)` models, and call it `\(\mathcal{M}_{k-1}\)`
  
--
  
3. Select a single best model (**) from the models `\(\mathcal{M}_{0}, \ldots, \mathcal{M}_{p}\)`

---

## Backward stepwise selection

- Like forward selection, is not guaranteed to yield the *best* model containing a subset of the `\(p\)` predictors

- Does not work when `\(p &gt; n\)`



---

## Choosing optimal model

- Step 3 of all three methods require us to compare many models to choose the "best" model

- Recall, model containing all `\(p\)` predictors will always yield lowest RSS/highest `\(R^2\)`

  - Related to training error
  
- We want a model with low test error `\(\rightarrow\)` require comparison across models with different number of predictors

---

## Estimating test error

- Approach 1: *indirectly* estimate test error by making an adjustment to the training error to account for the bias due to overfitting 

--

- Approach 2: *directly* estimate the test error (validation set or CV approach)

---

## Approach 1

- Techniques of Mallow's `\(C_{p}\)`, AIC, BIC, and adjusted `\(R^2\)` 

- **Adjust** the training error for the model size (number of predictors)

  - Useful for comparing models of different sizes
  
--

- Best subset selection for diabetes data
  
&lt;img src="06-selection-regularization_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

### Mallow's `\(C_{p}\)`


- Mallow's `\(C_{p}\)` compares full model with subsets of models to determine amount of error left unexplained by partial model

- For least squares model with `\(d\)` predictors , 

`$$C_{p} = \frac{1}{n}(RSS + 2d\hat{\sigma}^{2})$$`

and `\(\hat{\sigma}^{2}\)` is estimate of the error variance of `\(\epsilon\)` 


--
- Small `\(C_{p}\)` is preferred 

--

- Term `\(2d\hat{\sigma}^{2}\)` is a  *penalty*; increasing number of predictors `\(d\)` will increase `\(C_{p}\)` holding everything else constant


.footnote[throughout these slides, `\(\hat{\sigma}^{2}\)` is estimated using full model containing all predictors]


---

## AIC

- The **AIC* (Akaike Information criterion) is defined for a large class of models fit by maximum likelihood:

`$$\text{AIC} = -2\log (L) + 2d$$` where `\(L\)` is the maximimized value of the liklihood function for the given model

--

- For linear model with Normal errors, maximum likelihood and least squares are the same, so `\(C_{p}\)` and AIC will lead to same conclusions

---

## BIC

- The **BIC** (Bayesian Information criterion) is derived from Bayesian point of view, but looks similar:

`$$\text{BIC} = -2\log(L) + \log(n)d  = \frac{1}{n}(RSS + \log(n) d\hat{\sigma}^2)\quad^{**}$$`

--

- Like `\(C_{p}\)`, BIC will tend to be small for a model with low test error, so lower BIC preferred

- BIC penalty has `\(\log(n)d\hat{\sigma}^2\)`, wherease `\(C_{p}\)` penalty has `\(2d\hat{\sigma}^2\)`

  - Since `\(\log(n) &gt; 2\)` when `\(n &gt; 7\)`, heavier penalty for BIC compared to `\(C_{p}\)` when the model has many variables
  
  - BIC tends to result in selection of smaller models than `\(C_{p}\)` 
  
---

## Ajusted `\(R^2\)`

- Recall that usual `\(R^2 = 1- \frac{\text{RSS}}{\text{TSS}}\)` where `\(\text{TSS} = \sum(y_{i} - \bar{y})^2\)`

  - `\(\text{RSS}\)` always decreases as more variables are added to model `\(\rightarrow\)` `\(R^2\)` increases
  
--

- For least squares model with `\(d\)` variables: `$$R^{2}_{adj} = 1 - \frac{\text{RSS}/(n-d-1)}{\text{TSS}/(n-1)}$$`

--

- Why is `\(R^{2}_{adj}\)` better for comparing models of different size?

--

  - Maximizing `\(R^{2}_{adj}\)` is equivalent to minimizing `\(\text{RSS}/(n-d-1)\)`
  
---

## Approach 2

- Alternative to adjusting for model size is directly estimating the test error using validation set/CV
  
--

- We have candidate models `\(\mathcal{M}_{0}, \mathcal{M}_{1}, \ldots, \mathcal{M}_{L}\)`

  - For each model `\(\mathcal{M}_{l}\)`, perform same validation set/CV procedure 
  - Select a model `\(\mathcal{M}_{\hat{l}}\)` that has lowest estimated test error
  
--

- Does not require an estimate of `\(\sigma^{2}\)`

---

![](06-selection-regularization_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
  
---

# Shrinkage

---

## Shrinkage methods

- Subset selection methods use least squares to fit a linear model that contains a subset of the predictors 

- Alternatively, could fit a model containing all `\(p\)` predictors that *constrains/regularizes* the coefficient estimates

 - **Shrinks** coefficient estimates towards zero
 
--

- Two common methods: ridge regression and the Lasso

---

## Ridge regression

- Least-squares procedures estimates coefficients by using values that minimize

`$$\text{RSS} = \sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2$$`
--
 
- The **ridge regression** coefficient estimates `\(\hat{\beta}^{R}\)` are obtained by minimizing

`$$\sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 + \lambda \sum_{j=1}^{p} \beta_{j}^2 = \text{RSS} +  \lambda \sum_{j=1}^{p} \beta_{j}^2$$`

 where `\(\lambda \geq 0\)` is a **tuning parameter** that is determined separately
 
---

## Ridge regression

Want to minimize: 
`$$\text{RSS} +  \lambda \sum_{j=1}^{p} \beta_{j}^2$$`

- RSS: minimized when coefficient estimates fit the data well

- `\(\lambda \sum_{j=1}^{p} \beta_{j}^2\)`: **shrinkage penalty**, minimized when the estimates are close to 0

  - Has effect of *shrinking* the estimates of `\(\beta_{j}\)`
  
  - Note: `\(\beta_{0}\)` note subject to penalty
  
--
  
- `\(\lambda\)` controls relative impact of the two components

 - What happens when `\(\lambda = 0\)`? When `\(\lambda \rightarrow \infty\)`?
 
---

## Ridge example: diabetes data


```
## Note: Using an external vector in selections is ambiguous.
## ℹ Use `all_of(pp)` instead of `pp` to silence this message.
## ℹ See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.
## This message is displayed once per session.
```

&lt;img src="06-selection-regularization_files/figure-html/ridge_ex-1.png" style="display: block; margin: auto;" /&gt;

---

## Ridge example: diabetes data

- Plot on left: estimated ridge regression coefficient for each variable, plotted as a function of `\(\lambda\)`

- Plot on right: same ridge coefficient estimates, plotted as a function of `\(||\hat{\beta}_{\lambda}^{R}||_{2} / ||\hat{\beta} ||_{2}\)`, where `\(\hat{\beta}\)` is vector of least-squares estimates

 - `\(||\beta||_{2}\)` is the ""$l_{2}$-norm of a vector: `\(||\beta||_{2} = \sqrt{\sum_{j=1}^{p} \beta_{j}^2}\)`
 
--

 - `\(x\)`-axis can be thought of as amount of shrinkage
 
---

## Ridge regression: scaling

- The standard least-squares coefficient estimates are *scale equivariant*: if I multiply predict `\(X_{j}\)` by a factor of `\(c\)`, then estimated `\(\hat{\beta_{j}}\)` gets divided by `\(c\)`

 - Enters `\(RSS\)` as `\(X_{j}\beta_{j} = (c X_{j})(\beta_{j}/c)\)`

 
--

- In contrast, ridge regression coefficients estimates can change *substantially* when multiplying a given predictor by a constant

 - This is due to shrinkage penalty on the `\(\beta\)`'s
 
--

- Therefore, we usually apply ridge regression after **standardizing** the predictors:

`$$\tilde{x}_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n} \sum_{i=1}^{n}(x_{ij} = \bar{x}_{j})^2}}$$`

---

&lt;img src="06-selection-regularization_files/figure-html/ridge_mse-1.png" style="display: block; margin: auto;" /&gt;

---

## The Lasso

- One disadvantage of ridge regression: model will include all `\(p\)` predictors

 - Shrinkage penalty `\(\lambda \sum \beta_{j}^2\)` will shrink, but not set, coefficients to 0
 
 - Difficult for model interpretation

--

- The **Lasso** is a relatively recent alternative to ridge that overcomes this disadvantage

- Lasso coefficients `\(\hat{\beta}_{\lambda}^{L}\)` minimize 

`$$\sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 + \lambda \sum_{j=1}^{p} |\beta_{j}| = \text{RSS} +  \lambda \sum_{j=1}^{p} |\beta_{j}|$$`


---

## The Lasso

- Difference from ridge regression is the penalty: Lasso uses an `\(l_{1}\)` penalty

 - `\(l_{1}\)` norm of vector `\(\beta\)` is `\(||\beta||_{1} = \sum |\beta_{j}|\)`
 
--

- This `\(l_{1}\)` penalty forces some of the coefficients estimates to be exactly 0 when `\(\lambda\)` is sufficiently large

- Thus, `\(Lasso\)` performs *variable selection*

--

 - We say that the Lasso yelds *sparse* models -- models that involve only a subset of the variables
 
--

- Need a select a good value for `\(\lambda\)`

---

## Lasso: diabetes data


```
## Note: Using an external vector in selections is ambiguous.
## ℹ Use `all_of(pp)` instead of `pp` to silence this message.
## ℹ See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.
## This message is displayed once per session.
```

&lt;img src="06-selection-regularization_files/figure-html/lasso_diabetes-1.png" style="display: block; margin: auto;" /&gt;

--

- RHS: Depending on value of `\(\lambda\)`, the Lasso can produce a model involving any number of predictors
 
---

## Lasso variable selection

- Lasso coefficients estimates solve the problem

`$$\min_{\beta}\left\{ \sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 \right\} \quad \text{ subject to } \sum_{j=1}^{p} |\beta_{j}| \leq s$$`

 - For every value of `\(\lambda\)` there is some `\(s\)` that will give same lasso coefficients as equation from previous slide

--


- Ridge coefficients solve the problem 

`$$\min_{\beta}\left\{ \sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 \right\} \quad \text{ subject to } \sum_{j=1}^{p} \beta_{j}^2 \leq s$$`

--

- `\(s\)` can be thought of as a budget

---

## Lasso variable selection

&lt;img src="figs/06-selection/lasso_ridge.png" width="100%" style="display: block; margin: auto;" /&gt;

- Each ellipse is a *contour*; all points of a particular ellipse have the same RSS

---

## Lasso vs Ridge

Data simulated with `\(p = 45\)` predictors and `\(n = 50\)`, but only 2 predictors actually associated with response

&lt;img src="06-selection-regularization_files/figure-html/lasso_mse-1.png" style="display: block; margin: auto;" /&gt;

---

## Lasso vs Ridge

- Neither approach will universally dominant the other

- In general, we expect the lasso to perform better when the response is a function of only a relatively small number of prodictors

- Rdige might perform beter when the response is a function of many predictors, all with coefficients of roughly equal size

--

- The number of predictors related to `\(Y\)` is never known *a priori* for real data

 - Using techniques such as CV can be used to determine which approach is better for a given dataset
 
---

## How to select the tuning parameter?

- We require a method to determine which of the models under consideration is "best"

 - How to select `\(\lambda\)` (or constraint `\(s\)`)?
 
--

- Cross-validation gives us a simple solution: choose a range/grid of `\(\lambda\)` values, and compute the CV error rate for each value

 - Select the value of tuning parameter `\(\lambda^{*}\)` for which CV error is smallest
 
- The re-fit the model using all of the available observations and `\(\lambda^{*}\)`

---

## CV: diabetes data

&lt;img src="06-selection-regularization_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;
 
- Left: 10-fold CV errors from applying ridge regression to the diabetes data under various `\(\lambda\)`

- Right: coefficients estimates as a function of `\(\lambda\)`

--

- What do these plots tell us?


---

## LOOCV: simulated data

&lt;img src="06-selection-regularization_files/figure-html/lasso_loocv-1.png" style="display: block; margin: auto;" /&gt;

- Left: LOOCV MSE for the lasso, applied to sparse simulated data where only 2 predictors are associated with response

- Right: corresponding lasso coefficients

---

# Dimension Reduction Methods

---

## Dimension reduction

- Subset selection and shrinkage have involved fitting linear regression models using original predictors `\(X_{1}, X_{2}, \ldots, X_{p}\)`

--

- Now, consider *transforming* the predictors and then fitting a least-squares model using the transformed variables

 - We will refer to these techniques as **dimension reduction** methods
 
---

## Dimension reduction

- Let `\(Z_{1}, Z_{2}, \ldots, Z_{M}\)` represent `\(M &lt; p\)` linear combinations of the original predictors. That is,

`$$Z_{m} = \sum_{j=1}^{p} \phi_{jm} X_{j}$$`

 for some constants `\(\phi_{1m}, \phi_{2m}, \ldots, \phi_{pm}\)`, `\(m = 1,\ldots, M\)`
 
--

- Can then fit a linear regression model

`$$y_{i} = \theta_{0} + \sum_{m=1}^{M} \theta_{m}z_{im} + \epsilon_{i}, \quad i = 1,\ldots,n$$` 

 using least squares
 
--

- Here, the regression coefficients are the `\(\theta\)`'s

- If the `\(\phi_{mj}\)` are chosen well, dimension reductions can often outperform OLS

---

## Dimension reduction

- We are reducing the problem of estimating `\(p+1\)` coefficients `\((\beta_{0}, \beta_{1}, \ldots, \beta_{p})\)` to instead estimating `\(M+1\)` `\((\theta_{0}, \theta_{1}, \ldots, \theta_{M})\)`

--

`$$\sum_{m=1}^{M} \theta_{m} z_{im} = \sum_{m=1}^{M} \theta_{m} \sum_{j=1}^{p} \phi_{jm}x_{ij} = \sum_{j=1}^{p} \sum_{m=1}^{M} \theta_{m} \phi_{jm}x_{ij} = \sum_{j=1}^{p} \beta_{j} x_{ij}$$`

 where `\(\beta_{j} = \sum_{m=1}^{M} \theta_{m}\phi_{jm}\)`

- So the linear regression model from the previous slide can be thought of as special case of original linear regression model

--

- Notice that the `\(\beta_{j}\)` are constrained
 
 - Has potential for bias
 
 - When `\(p\)` is large relative to `\(n\)`, dimension reduction to reduce variance of the estimated coefficients
 
---

## Dimension reduction

- Generally two steps:

 1. Obtain transformed predictors `\(Z_{1}, \ldots, Z_{M}\)`
 
 2. Fit the model to the `\(Z\)`'s

--

- How we select the `\(\phi_{jm}\)`'s will affect the model

---

## Principal Components Regression

- Principal components regression relies on the technique of principal components analysis (PCA)

- The first principal component is the (normalized) linear combination of the variables with the largest variance

- The second principal component has largest variance, subject to being uncorrelated with the first

- Etc.



---

## PCA pictures

&lt;img src="06-selection-regularization_files/figure-html/pca_fig1-1.png" style="display: block; margin: auto;" /&gt;

- Purple line is the first principal component direction of the data

- Orange line is the second principal component

---

## PCA

- Recall: `\(Z_{m} = \sum_{j=1}^{p} \phi_{jm} X_{j}\)`

--

- In this example,

`$$Z_{1} = 0.624\times (\color{blue}{\text{glu}} - \bar{\color{blue}{\text{glu}}}) + 0.781\times (\color{blue}{\text{ltg}} - \bar{\color{blue}{\text{ltg}}})$$`

 - `\(\phi_{11} = 0.624\)` and `\(\phi_{21} = 0.781\)` are the principal component loadings
 
 
---

## PCA pictures


&lt;img src="06-selection-regularization_files/figure-html/pca_fig2-1.png" style="display: block; margin: auto;" /&gt;

- Subset of the diabetes data

- Left: first principal component minimizes sum of squared perpendicular distances to each point, with dashed line segemnts representing distances

- Right: rotated plot on left so first principal compoent coincides with x-axis


---


## PCA pictures

&lt;img src="06-selection-regularization_files/figure-html/pca_scores1-1.png" style="display: block; margin: auto;" /&gt;

- Plots of the first principal component scores `\(z_{i1}\)` versus `\(\color{blue}{\text{glucose}}\)` and `\(\color{blue}{\text{tryglycerides}}\)`

--

- Think of the `\(z_{i1}\)` values as single-number summaries of the joint `\(\color{blue}{\text{glucose}}\)` and `\(\color{blue}{\text{tryglycerides}}\)` for each individual

 - If `\(z_{i1} &lt; 0\)`, indicates an individual with below-average glucose and triglycerides level

---

## PCA pictures cont.

&lt;img src="06-selection-regularization_files/figure-html/pca_scores2-1.png" style="display: block; margin: auto;" /&gt;

- Plots of the second principal component scores `\(z_{i2}\)` versus `\(\color{blue}{\text{glucose}}\)` and `\(\color{blue}{\text{tryglycerides}}\)`

`$$Z_{2} = 0.781\times (\color{blue}{\text{glu}} - \bar{\color{blue}{\text{glu}}}) + -0.624\times (\color{blue}{\text{ltg}} - \bar{\color{blue}{\text{ltg}}})$$`

---

## Principal components cont.

- In general, can have up to `\(p\)` distinct principal components

 - In this example, `\(p=2\)` so we can only have two components
 
--
 
- By construction, `\(Z_{1}\)` explains more variance than `\(Z_{2}\)`

 - Examine variability on plot of `\(z_{i1}\)` vs `\(z_{i2}\)`, and strength of relationships between `\(z_{i2}\)` and the two original variables
 
---

## Principal Components Regression

- **PCR** (not to be confused with polymerase chain reaction) approach:

 1. Construct the first `\(M\)` principle components, `\(Z_{1}, \ldots, Z_{M}\)`
 
 2. Use the components as the predictors in a linear regression model fit using least squares
 
--

- Intuition: often only a small number of principal components are needed to explain most of variability in data, as well as relationship with `\(Y\)`

 - Assume that the directions in which `\(X_{1},\ldots,X_{p}\)` exhibit the most variation are the directions that are associated with `\(Y\)`
 
---

&lt;img src="06-selection-regularization_files/figure-html/pcr_sim_dat-1.png" style="display: block; margin: auto;" /&gt;
 
- Left: simulated data similar to ridge 
- Right: simulated data similar to lasso

---

## PCR remarks

- Note that PCR is not a feature selection method

 - Each of the `\(M\)` components uses all `\(p\)` original predictors
 
 - In a sense, PCR is similar to ridge regression
 
--

- The number of components `\(M\)` is typically chosen by CV

--

- Generally recommend standardizing each predictor prior to generating the principal components

--

  - Otherwise, high-variance predictors will tend to play a larger role in the `\(Z_{m}\)` 

---

## PCR: diabetes data


```
## Note: Using an external vector in selections is ambiguous.
## ℹ Use `all_of(p)` instead of `p` to silence this message.
## ℹ See &lt;https://tidyselect.r-lib.org/reference/faq-external-vector.html&gt;.
## This message is displayed once per session.
```

&lt;img src="06-selection-regularization_files/figure-html/diabetes_pcr-1.png" style="display: block; margin: auto;" /&gt;

- Left: PCR standardized coefficient estimates for different values of `\(M\)`

- Right: The 10-fold CV MSE obtained using PCR as a function of `\(M\)`

---

## PCR: remarks cont.

- PCR identifies linear combinations (directions) that best represent the predictors `\(X_{1}, \ldots, X_{p}\)`

- The directions are obtained in an *unsupervised* fashion; the response `\(Y\)` is not used

--

- As a result, in PCR there is no guarantee that the direction that best explain `\(X\)` will also be best directions for `\(Y\)`

--

## Partial least squares

- **Partial least squares ** (PCL) is another dimnesion reduction method, and is a *supervised* alternative to PCR

- The new features `\(Z_{1}, \ldots, Z_{M}\)` will approximate the original `\(X\)`'s well, and will also be related to the response!

--

- Roughly speaking, PLS attempts to find directions that help explain both `\(X\)` adnd `\(Y\)`

--

## Partial least squares

- Recall: `\(Z_{m} = \sum_{j=1}^{p}\phi_{jm} X_{j}\)`

--

- First, standardize the `\(p\)` predictors `\(X_{1},\ldots, X_{p}\)` (and often the response `\(Y\)` as well)

- Compute first direction `\(Z_{1}\)` by setting each `\(\phi_{1j}\)` equal to the coefficient from the simle linear regression of `\(Y\)` on `\(X_{j}\)`

--

 - This coefficient is proportional to the correlation between `\(X_{j}\)` and `\(Y\)`
 
--

 - So in PLS, `\(Z_{1} = \sum_{j=1}^{p} \phi_{1j}X_{j}\)` places highest weight on variables that are most strongly related to the response

---

## Partial least squares

- Subsequent directions found by taking *residuals* and repeating this process

 - The residuals are interepreted as the remaining information not expalined by the previous PLS directions
 
--


- Finally, after obtaing `\(Z_{1}, \ldots, Z_{M}\)`, fit a linear model for `\(Y\)` as before


--

- `\(M\)` is typically determined using CV

---


&lt;img src="06-selection-regularization_files/figure-html/diabetes_pls-1.png" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
