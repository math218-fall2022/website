<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Unupervised learning: Hierarchical clustering" />
    <meta name="date" content="2022-11-30" />
    <script src="10-hierarchical-clust-slides_files/header-attrs-2.17/header-attrs.js"></script>
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Math 218: Statistical Learning
]
.author[
### Unupervised learning: Hierarchical clustering
]
.date[
### 11/30/2022
]

---




## Houskeeping

- Sign-up for project meetings via Calendly, found on project description page

- K-means lab due tomorrow at 11:59pm

- Office hours today 3-5pm

---

## Clustering

- We want to find *subgroups* or *clusters* in a data set

- This means separating observations into distinct groups such that:

  - observations with each group are similar
  
  - observation across groups are different
  


- How to define "similar" and "different"?

--

- Two main methods: **K-means clustering** (before break) and **hierarchical clustering** (this week)


---
class: center, middle

# Hierarchical Clustering

---

## Hierarchical Clustering

- Disadvantage of `\(K\)`-means is pre-specifying `\(K\)`!

- What if we don't want to commit to a single choice?

- We will construct a (upside-down) tree-based representation of the observations *dendrogram* 

- Specifically focus on *bottom-up* hierarchical clustering, by constructing leaves first and then the trunk

--

- Intuition/idea: rather than clustering everything at beginning, why don't we build iteratively 


---

### Hierarchical clustering "algorithm"

The approach in words:

- Start with each point in its own cluster

- Identify the "closest" two clusters and merge them together

- Repeat

- Finish when all points have eventually been combined into a single cluster

---

## Dendrogram

- Each *leaf* of the dendrogram is one of the `\(n\)` observations

- As we move up the tree, some leaves fuse into branches

  - These correspond to observations that similar to each other

- Branches and/or leaves will fuse as we move up the tree



---

## Dendrogram

- The earlier (lower in the tree) fusions occur, the more similar the groups

- For any two observations, look for the point in tree where branches containing these two observations are first fused

  - The height of this fusion on *vertical* axis indicates how *different* they are
  
--

- **Caution!** Cannot draw conclusions about similar based on proximity of observations on the *horizontal* axis 

  - Why?
  
---

## Identifying clusters

- To identify clusters based on dendrogram, simply make a horizontal cut across dendrogram

- Distinct sets of observations beneath the cut are interpreted as clusters

--

- This makes hierarchical clustering attractive: one single dendrogram can be used to obtain *any* number of clusters

  - What do people do in practice?
  
---

## "Hierarchical"

- Clusters obtained by cutting at a given height are *nested* within the clusters obtained by cutting at any greater height

- Is this realistic?

---

### Hierarchical clustering algorithm

- Need to define some sort of *dissimilarity* measure between pairs of observations (e.g. Euclidean distance)

--

1. Begin with `\(n\)` observations and a measure of all the `\(\binom{n}{2}\)` pairwise dissimilarities. Treat each observation as its own cluster.

--

2. For `\(i = n, n-1, n-2,\ldots, 2\)`:

  i) Examine all pairwise *inter-cluster* dissimilarities among the `\(i\)` clusters and identify the pair of clusters that are least dissimilar. Fuse these two clusters. Their dissimiliarity is the height in the dendrogram where the fusion should be placed.
  
  ii) Compute the new pairwise inter-cluster dissimilarities among the remaining `\(i-1\)` remaining clusters
  
---

## Dissimilarity between groups

- How do we define dissimilarity between two clusters if one or both contains multiple observations?

  - i.e. How did we determine that `\(\{A,C\}\)` should be fused with `\(\{B\}\)`?

--

- Develop the notion of *linkage*, which defines dissimilarity between two groups of observations

---

## Common linkage types

- .vocab[Complete]: maximal intercluster dissimilarity. Compute all pairwise dissimilarities between observations in cluster `\(A\)` and observations in cluster `\(B\)`. Record the *largest* of these dissimilarities.

--

- .vocab[Single]: minimal intercluster dissimilarity.  Compute all pairwise dissimilarities between observations in cluster `\(A\)` and observations in cluster `\(B\)`. Record the *smallest* of these dissimilarities. 

--

- .vocab[Average]: mean intercluster dissimilarity.  Compute all pairwise dissimilarities between observations in cluster `\(A\)` and observations in cluster `\(B\)`. Record the *average* of these dissimilarities.

--

- .vocab[Centroid]: dissimilarity between the centroid for cluster `\(A\)` and the centroid for cluster `\(B\)`

---

## Common linkage types

- Average and complete are generally preferred over single linkage

  - Tend to yield more balanced dendrograms
  
- Centroid linkage often used in genomics

  - Drawback of inversion, where two clusters are fused at a height *below* either of the individual clusters
  
---

class: center, middle

## Your turn!

---

## Example

- Generated `n=50` observations, evenly split between two true clusters



- `hclust()` performed hierarchical clustering. Requires a dissimilarity structure `d` for the features obtained by `dist()`, and the `method` of linkage


```r
# X is our matrix of features
hc_complete &lt;- hclust(d = dist(X), method = "complete")
```

---

&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---


&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---


&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

---

## Cutting dendrogram


```r
c &lt;- 4
cutree(hc_complete, c)
```

```
##  [1] 1 1 2 1 1 3 2 1 2 1 3 1 1 1 2 4 3 1 2 1 2 1 2 2 1 4 1 4 1 3 1 3 3 3 3 1 4 4
## [39] 4 4 1 4 3 3 3 3 1 4 3 4
```

```r
cutree(hc_avg, c)
```

```
##  [1] 1 2 2 1 1 3 2 1 2 2 3 2 1 1 2 1 3 1 2 1 2 1 2 2 1 4 1 4 1 4 1 4 3 4 4 1 4 4
## [39] 4 4 1 1 4 3 4 3 1 4 4 4
```

```r
cutree(hc_single, c)
```

```
##  [1] 1 2 1 1 1 1 1 2 1 2 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1
## [39] 1 1 1 1 1 4 1 4 1 1 1 1
```

---




&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-10-1.png" style="display: block; margin: auto;" /&gt;

---

&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;


---

## Example: circle data again

&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

---

.pull-left[
&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-13-1.png" style="display: block; margin: auto;" /&gt;
]
--

.pull-right[
&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-14-1.png" style="display: block; margin: auto;" /&gt;
]

---

## Considerations

- We have mostly seen Euclidean distance as the choice of dissimilarity measure

- *Correlation-based distance* considers two observations to be similar if their features are highly correlated (will see in lab)

  - Focuses on the shapes of observation profiles, rather than their magnitudes
  
--

- Choice of dissimilarity measure has strong effect on resulting dendrogram!

  - Therefore, must think critically about the type of data being clustered AND the scientific question at hand
  
--

- Should we scale the variables?

---

## Discuss

.question[Discuss: Which (if any) of the linkage methods do you believe are sensitive to outliers?]

--
&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-15-1.png" style="display: block; margin: auto;" /&gt;

---



&lt;img src="10-hierarchical-clust-slides_files/figure-html/plots-1.png" style="display: block; margin: auto;" /&gt;&lt;img src="10-hierarchical-clust-slides_files/figure-html/plots-2.png" style="display: block; margin: auto;" /&gt;



---

## Summary: decisions

- Should the features be standardized?


- What dissimilarity measured should we use?

- What type of linkage should we use?

- Where should we cut the dendrogram?

---

## Summary: ongoing considerations

- How do we validate the clusters we obtained?

  - Are we truly discovering subgroups, or are we simply clustering the noise?

- Do all observations belong in a cluster? Or are some actually "outliers"

- Clustering methods generally not robust to small changes in data

---

## Summary: recommendations

- Small decisions can lead to large changes!

- Recommend performing clustering with different choices of these parameters/options, and looking to see if/which patterns consistently appear

- Because clustering not robust, maybe we consider clustering subsets of the data 

- Caution: be careful about how the results of a clustering analysis are reported. These results should not be taken as the absolute truth about a data set!!!

---

class: middle, center

## Extra: consensus clustering



---

## Consensus clustering

- We cannot validate clusters, so how can we "feel good" about our cluster assignments?

- Want to measure the *stability* of our clusterings; how robust are they to small perturbations?

  - If the data represent a sample of items drawn from distinct sub-populations, and if we were to observe a different sample
drawn from the same sub-populations, the induced cluster composition and number should
not be radically different

- One way might be to use **consensus clustering**, which combines/aggregates multiple clusterings together

---

## Consensus clustering

- Unfortunately, we cannot obtain a different sample, BUT we can **resample**

- We can perturb or resample datasets, apply the same clustering algorithm, and see if there is agreement among the clustering runs over the datasets

  - Kind of like bagging for trees

--

- How? Perturb by adding a little bit of noise, or resample using the bootstrap!

---


## Consensus clustering

- How would we use these ideas to compare models?

- For repeated clusterings, record the proportion of times observation `\(i\)` and `\(i'\)` `\((i \neq i')\)` are in the same cluster 

--

- If our clusterings are stable, what values would these proportions take on?


---

## Consensus clustering: discuss!

- Let's say we are performing hierarchical clustering and need to decide how many clusters to report. 

- Discuss: how would we perturb the data to perform consensus clustering?

- Discuss: how would we use the bootstrap to perform consensus clustering?

---

## Example: perturbing

- Also known as semiparametric bootstrapping



&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

--

- Is this plot "fair"?

---

&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;




---

## Example: 0.632 bootstrap



&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;


---

&lt;img src="10-hierarchical-clust-slides_files/figure-html/unnamed-chunk-21-1.png" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
