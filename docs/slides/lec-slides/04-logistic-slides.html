<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Logistic Regression" />
    <script src="04-logistic-slides_files/header-attrs-2.16/header-attrs.js"></script>
    <script src="04-logistic-slides_files/kePrint-0.0.1/kePrint.js"></script>
    <link href="04-logistic-slides_files/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Math 218: Statistical Learning
]
.author[
### Logistic Regression
]
.date[
### 9/26/2022
]

---





class: center, middle

# Housekeeping

---

## Classification

- Qualitative variables takes values in an unordered set `\(\mathcal{C}\)`, such as:

  - `\(\color{blue}{\text{wine}} \in \{\color{blue}{\text{red}},\color{blue}{\text{white}},\color{blue}{\text{rose}}\}\)`

  - `\(\color{blue}{\text{diagnosis}} \in \{\color{blue}{\text{yes}}, \color{blue}{\text{no}} \}\)`

--

- We have predictors `\(X\)` and a qualitative response `\(Y\)` that takes values in `\(\mathcal{C}\)`

  - Supervised learning task

- Goal of classification: build a function `\(C(X)\)` that predicts a label for `\(Y\)`

--

  - We are often interested in the estimated *probabilities* that `\(X\)` belongs to a given category in `\(\mathcal{C}\)`

---

## Probability warm-up: discuss!

A fair, six-sided die is rolled. Let `\(X\)` denote the result of the die roll. 

1. What is `\(\text{Pr}(X = 1)\)`?

2. What is `\(\text{Pr}(X \text{ is even})\)`?

Now we are rolling two dice.  Let `\(X_{1}\)` denote the result of the first, and `\(X_{2}\)` the second. 

3. What is `\(\text{Pr}(X_{1} + X_{2} = 1)\)`?

4. What is `\(\text{Pr}(X_{1} + X_{2} &lt; 13)\)`?

---

## Heart attack data

- Health measurements from 303 patients, along with record of the presence of heart disease in the patient

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

## Seeds data

- Data come from UCI Machine Learning Repository 

- Measurements of geometrical properties of kernels belonging to three different varieties of wheat

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

* Ask about seed banks

---

## Why not linear regression?

- For seeds data, `\(\mathcal{C} = \{\text{Canadian, Kama, Rosa}\}\)`

- Could we encode the values as a quantitative response, such as:

`$$Y = \begin{cases}
1 &amp; \text{ if } \color{blue}{\text{Canadian} }\\
2 &amp; \text{ if } \color{blue}{\text{Kama}}\\
3 &amp; \text{ if } \color{blue}{\text{Rosa}} \end{cases}$$`

`\(\quad\)` and then fit a linear regression for this `\(Y\)`?

---

## Why not linear regression?

The heart disease data is formatted as:

`$$Y = \begin{cases}
0 &amp; \text{ if } \color{blue}{\text{no heart disease} }\\
1 &amp; \text{ if } \color{blue}{\text{heart disease}}\end{cases}$$`

- Here, `\(Y\)` is binary

--

- For binary response, could we use least squares to fit a linear regression model to `\(Y\)`?

  - Maybe fit a linear regression and predict `\(\color{blue}{\text{heart disease}}\)` if `\(\hat{Y} &gt; 0.5\)`
  
--
  
  - Can show that the estimate `\(\hat{\beta}_{0} + \hat{\beta}_{1} X_{1} + \ldots + \hat{\beta}_{p} X_{p}\)` is an estimate of `\(Pr(Y = 1 | X) = Pr(\color{blue}{\text{heart disease}} | X)\)`
  

---

## Why not linear regression?

- Fit a linear regression line for `\(\color{blue}{\text{disease}} = \beta_{0} + \beta_{1}\color{blue}{\text{age}} + \beta_{2}\color{blue}{\text{sex}}\)`

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

--

- In certain cases, linear regression might produce estimated probabilities less than `\(0\)` or bigger than `\(1\)`

---

## Logistic regression

- Fitting a *logistic* regression:

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

## "exp"

- `\(\exp(x)\)` is shorthand for `\(e^{x}\)`

- `\(e \approx\)` 2.7182818 is a special number

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

--

- `\(e^{0} = 1\)`

- `\(e^{a} \times e^{b} = e^{a+b}\)`

- `\(\frac{e^{a}}{e^{b}} = e^{a-b}\)`

- Output is always positive

---

## log

- We will work with the natural log, which is the inverse function to exp

  - `\(\log(\exp(x)) = x\)` and `\(\exp(\log(x)) = x\)`

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

--

- Can only take log of positive numbers

- Output can be negative or positive


---


## Logistic regression

- Let `\(p(X) = \text{Pr}(Y = 1 | X)\)`. Need to somehow restrict `\(0 \leq p(X) \leq 1\)`

- **Logistic** regression uses *logistic* function:

`$$p(X) = \frac{e^{\beta_{0} + \beta_{1}X}}{1 + e^{\beta_{0} + \beta_{1}X}}$$`

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;


---

## Logistic regression

- Rearranging this equation yields the **odds**:

`$$\frac{p(X)}{1 - p(X)} = e^{\beta_{0} + \beta_{1}X}$$`

--

- Furthermore, we can obtain the **log-odds** or **logit**: 

`$$\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}X$$`


---

## Logistic regression


- Why is it called logistic "regression" if we are using it for classification?

--

- The logistic regression model has a logit that is linear in `\(X\)`


`$$\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}X$$`



---

## Logistic regression

`$$\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}X$$`

- Interpretation of `\(\beta_{1}\)`: for every one-unit increase in `\(X\)`, we expect an average change of `\(\beta_{1}\)` in the log-odds (or average multiple of `\(e^{\beta_{1}}\)` in the odds)

--
  
  - `\(\beta_{1}\)` does *not* correspond to the change in `\(p(X)\)` associated with one-unit increase in `\(X\)` (i.e., not a linear relationship between `\(X\)` and `\(p(X)\)`)
  
--

  - If `\(\beta_{1} &gt; 0\)`, then increasing `\(X\)` is associated with increasing `\(p(X)\)`
  
---

### Estimating the Regression Coefficients

- Use the general method of *maximum likelihood* to estimate `\(\beta_{0}\)` and `\(\beta_{1}\)`
    
- *Likelihood function* `\(\ l()\)` describes the probability of the observed data as a function of model parameters. For logistic regression model:

`$$l(\beta_{0}, \beta_{1}) = \prod_{i: y_{i} = 1}p(x_{i}) \prod_{i:y_{i} = 0}(1 - p(x_{i}))$$`

- We pick `\(\beta_{0}\)` and `\(\beta_{1}\)` that will maximize this likelihood

---

## Logistic regression: heart data

- `\(\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}\color{blue}{\text{age}}\)`, where `\(p(X) = \text{Pr}(\color{blue}{\text{heart disease}})\)`:


```
## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)  -3.01      0.759      -3.96 0.0000750
## 2 age           0.0520    0.0137      3.80 0.000143
```

--

- Interpretation of `\(\hat{\beta}_{0}\)`: the average log-odds of heart disease of someone who is 0 years old is -3.01

- Interpretation of `\(\hat{\beta}_{1}\)`: for every one-unit increase in `\(\color{blue}{\text{age}}\)`, we expect an average increase of `\(0.052\)` in the log-odds  of heart disease

---

## Logistic regression: heart data

- `\(\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}\color{blue}{\text{age}}\)`, where `\(p(X) = \text{Pr}(\color{blue}{\text{heart disease}})\)`:


```
## # A tibble: 2 × 5
##   term        estimate std.error statistic   p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)  -3.01      0.759      -3.96 0.0000750
## 2 age           0.0520    0.0137      3.80 0.000143
```

--

What is the estimated probability of `\(\color{blue}{\text{heart disease}}\)` for someone who is 50 years old?

--

$$
`\begin{align*}
&amp;\hat{p}(X) = \frac{e^{\hat{\beta}_{0} + \hat{\beta}_{1}X}}{1 + e^{\hat{\beta}_{0} + \hat{\beta}_{1}X}} \\
&amp;\hat{p}(X = 50) = \frac{e^{-3.006 + 0.052 \times 50}}{1+e^{-3.006 + 0.052 \times 50}} = 0.3998716
\end{align*}`
$$

---

## Logistic regression: heart data

- `\(\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}\color{blue}{\text{sexM}}\)`, where `\(p(X) = \text{Pr}(\color{blue}{\text{heart disease}})\)`:


```
## # A tibble: 2 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)    -1.06     0.232     -4.56 0.00000520
## 2 sexM            1.27     0.271      4.69 0.00000271
```


- Interpretation: females have an average log-odds of heart disease of -1.06 

- Interpretation: the log-odds of heart disease for males is on average 1.27 greater than the log-odds for females

---

## Logistic regression: heart data

- `\(\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}\color{blue}{\text{sexM}}\)`, where `\(p(X) = \text{Pr}(\color{blue}{\text{heart disease}})\)`:


```
## # A tibble: 2 × 5
##   term        estimate std.error statistic    p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1 (Intercept)    -1.06     0.232     -4.56 0.00000520
## 2 sexM            1.27     0.271      4.69 0.00000271
```

--

- `\(\hat{\text{Pr}}(\color{blue}{\text{heart disease}}|\color{blue}{\text{sex = Male}})=\frac{e^{-1.058 + 1.272 \times 1}}{1+e^{-1.058 + 1.272 \times 1}} = 0.5532968\)` 

- `\(\hat{\text{Pr}}(\color{blue}{\text{heart disease}}|\color{blue}{\text{sex = Female}})=\frac{e^{-1.058 + 1.272 \times 0}}{1+e^{-1.058 + 1.272 \times 0}} = 0.2576918\)` 

--

- While the predicted probabilities are often of most interest, sometimes we do need to actually classify

  - Recall Bayes classifier: classify new observation as label `\(k\)` if `\(\text{Pr}(Y = k | X= x)\)` is largest

---

## Multiple logistic regression

- Extend from simple to multiple logistic regression similar to linear model:

`$$\log\left(\frac{p(X)}{1-p(X)} \right)= \beta_{0} + \beta_{1}X_{1} + \beta_{2} X_{2} + \ldots + \beta_{p}X_{p}$$`
--

`$$p(X) = \frac{e^{\beta_{0} + \beta_{1}X_{1} + \beta_{2} X_{2} + \ldots + \beta_{p}X_{p}}}{1 + e^{\beta_{0} + \beta_{1}X_{1} + \beta_{2} X_{2} + \ldots + \beta_{p}X_{p}}}$$`

--

- `\(\log\left(\frac{p(X)}{1 - p(X)}\right) = \beta_{0} + \beta_{1}\color{blue}{\text{age}}+ \beta_{2}\color{blue}{\text{sexM}}\)`


```
## # A tibble: 3 × 5
##   term        estimate std.error statistic      p.value
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 (Intercept)  -4.81      0.898      -5.35 0.0000000866
## 2 age           0.0657    0.0148      4.43 0.00000956  
## 3 sexM          1.50      0.289       5.18 0.000000223
```

---


class: middle, center

## Model assessment

---

## Model assessment

- How well are we doing in our predictions?

- With binary response, it is common to create a **confusion matrix**

&lt;img src="figs/04-classification/confusion_matrix.png" style="display: block; margin: auto;" /&gt;
---

## Example:

- 10 observations, with true class and predicted class

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; pred &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; true &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Make a confusion matrix!

---

## Heart disease data

- Fit a logistic regression to predict heart `\(\color{blue}{\text{disease}}\)` using predictors `\(\color{blue}{\text{age}}, \color{blue}{\text{sex}}, \color{blue}{\text{chest pain}}\)`

- Confusion matrix of predicted (rows) vs true (columns) heart disease status 

&lt;table&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="empty-cells: hide;border-bottom:hidden;" colspan="1"&gt;&lt;/th&gt;
&lt;th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2"&gt;&lt;div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; "&gt;True&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 0 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 132 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 38 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 32 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 101 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Misclassification rate: (38 + 32)/303 = 0.231

- Of the observations with true `\(\color{blue}{\text{heart disease}}\)`, we make 38/139 = 0.195 errors

- Of the observation with true `\(\color{blue}{\text{no heart disease}}\)`, we make 32/164 = 0.273 errors

---

## Types of errors

- **False positive rate** (FPR): fraction of negative observations incorrectly classified as positive -- 0.195 in our example

- **False negative rate** (FNR): fraction of positive observations incorrectly classified as negative example -- 0.273 in our example

--

- Is a false positive or a false negative worse? In the heart data, maybe a false negative?

--

- This table was produced by classifying an observation at `\(\color{blue}{\text{heart disease}}\)` if `\(\hat{Pr}(\color{blue}{\text{heart disease}} | \color{blue}{\text{age}}, \color{blue}{\text{sex}}, \color{blue}{\text{chest pain}}) \geq 0.5\)`

  - Here, 0.5 is the threshold for assigning an observation to `\(\color{blue}{\text{heart disease}}\)` class
  
  - Can change threshold to any value in `\([0,1]\)`, which will affect error rates
  
---

## Varying threshold

&lt;img src="04-logistic-slides_files/figure-html/lda_threshold-1.png" style="display: block; margin: auto;" /&gt;

- Overall error rate minimized at threshold near 0.4
- How to decide a threshold rate?

---

## ROC Curve


- **sensitivity** (true positive rate) = probability of a positive, conditioned on truly being positive = 1 - FNR

- **specificity** (true negative rate) = probability of a negative, conditioned on truly being negative = 1 - FPR

- The **ROC curve** plots both simultaneously

&lt;img src="04-logistic-slides_files/figure-html/unnamed-chunk-16-1.png" style="display: block; margin: auto;" /&gt;

- **AUC** (area under the curve) summarizes overall performance

---

## Summary

- Logistic regression is very commonly used when `\(K=2\)`

- Does poorly in certain scenarios

---

class: middle, center

## Optional material

---

## Alternative: softmax

- Alternative encoding: **softmax**

  - Easily extends to `\(K &gt; 2\)` classes (*multinomial* regression)

--

- In softmax, we do not choose a baseline but rather, treat all classes symmetrically. Each class gets its own set of coefficients.

- For each `\(k = 1,\ldots, K\)`:

`$$Pr(Y= k | X =x ) =  \frac{e^{\beta_{0,k} + \beta_{1,k}x_{1} + \ldots + \beta_{p,k}x_{p}}}{ \sum_{l=1}^{K} e^{\beta_{0,l} + \beta_{1,l}x_{1} + \ldots + \beta_{p,l}x_{p}}}$$`
---

## Softmax

- In binary case with one predictor (for simplicity):

`$$Pr(Y= 0 | X =x ) =  \frac{e^{\beta_{0,0} + \beta_{1,0}x_{1}}}{ e^{\beta_{0,0} + \beta_{1,0}x_{1}} + e^{\beta_{0,1} + \beta_{1,1}x_{1}}}$$`
`$$Pr(Y= 1 | X =x ) =  \frac{e^{\beta_{0,1} + \beta_{1,1}x_{1}}}{ e^{\beta_{0,0} + \beta_{1,0}x_{1}} + e^{\beta_{0,1} + \beta_{1,1}x_{1}}}$$`

- Notice that these probabilities sum to one

- Your homework has a bonus question that compares logistic encoding and softmax encoding 
---

## Multinomial regression

- What if we have `\(K &gt; 2\)` classes? Can extend the logistic regression model to *multiple logistic* or **multinomial** regression

- One method is to choose a single class to serve as *baseline* (it doesn't matter which, so we will choose the `\(K\)`-th class)

--

- Then for class `\(k = 1,\ldots, K-1\)`:

`$$Pr(Y = k | X= x) = \frac{e^{\beta_{k0} + \beta_{k1}x_{1} + \ldots + \beta_{kp}x_{p}}}{1 + \sum_{l=1}^{K-1} e^{\beta_{l0} + \beta_{l1}x_{1} + \ldots + \beta_{lp}x_{p}}}$$`
--

`\(\quad\)` and by default, 

`$$Pr(Y = K | X= x) = \frac{1}{1 + \sum_{l=1}^{K-1} e^{\beta_{l0} + \beta_{l1}x_{1} + \ldots + \beta_{lp}x_{p}}}$$`

--

- Here, there is a function  for *each* class

---

## Multinomial regression

- For `\(k = 1, \ldots, K-1\)`:

`$$\log\left(\frac{Pr(Y = k | X =x)}{Pr(Y = K | X = x)}\right) = \beta_{k0} + \beta_{k1}x_{1} + \ldots + \beta_{kp} x_{p}$$`

  - i.e. log-odds between any pair of classes is linear in the predictors
  
---

## Seed data

- Fit multinomial regression for the three classes of seeds: Kama, Rosa, and Canadian using `\(\color{blue}{\text{area}}\)` as a predictor:


```
## # weights:  9 (4 variable)
## initial  value 230.708581 
## iter  10 value 62.449477
## iter  20 value 61.097530
## iter  30 value 61.008977
## iter  40 value 61.008049
## final  value 61.008036 
## converged
```

```
## # A tibble: 4 × 6
##   y.level term        estimate std.error statistic  p.value
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;
## 1 Kama    (Intercept)   -30.6      5.20      -5.88 4.06e- 9
## 2 Kama    area            2.36     0.405      5.83 5.40e- 9
## 3 Rosa    (Intercept)   -67.0      9.10      -7.36 1.84e-13
## 4 Rosa    area            4.63     0.620      7.47 8.32e-14
```

---

## Multinomial regression

- Predicted probabilities of each class for seed with area of 14 `\(mm^2\)`?

  - `\(\hat{\text{Pr}}(\color{blue}{\text{Kama}} | X = 14) = \frac{e^{-30.56 +2.36\times 14}}{1 + e^{-30.56 +2.36\times 14} + e^{-66.98 +4.63\times 14}} = 0.915\)`
  
  - `\(\hat{\text{Pr}}(\color{blue}{\text{Rosa}} | X = 14) = \ldots = 0.009\)`
  
  - `\(\hat{\text{Pr}}(\color{blue}{\text{Canadian}} | X = 14) = \ldots = \frac{1}{1 + e^{-30.56 +2.36\times 14} + e^{-66.98 +4.63\times 14}} =  0.077\)`

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
