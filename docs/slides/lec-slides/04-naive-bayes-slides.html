<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Naive Bayes" />
    <script src="04-naive-bayes-slides_files/header-attrs-2.16/header-attrs.js"></script>
    <script src="04-naive-bayes-slides_files/htmlwidgets-1.5.4/htmlwidgets.js"></script>
    <link href="04-naive-bayes-slides_files/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="04-naive-bayes-slides_files/datatables-binding-0.24/datatables.js"></script>
    <script src="04-naive-bayes-slides_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <link href="04-naive-bayes-slides_files/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="04-naive-bayes-slides_files/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="04-naive-bayes-slides_files/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
    <link href="04-naive-bayes-slides_files/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
    <script src="04-naive-bayes-slides_files/crosstalk-1.2.0/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Math 218: Statistical Learning
]
.author[
### Naive Bayes
]
.date[
### 9/28/2022
]

---





class: center, middle

# Housekeeping

---


## Seeds data

- Data come from UCI Machine Learning Repository 

- Measurements of geometrical properties of kernels belonging to three different varieties of wheat

&lt;img src="04-naive-bayes-slides_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

* Ask me about seed banks
---

## Probability crash-course!

- The *multiplication rule of independent events* in probability: if two events `\(A\)` and `\(B\)` are independent, then the probability of them both occurring at the same time is equal to the product of the individual probabilities

  - `\(\text{Pr}(A \text{ and } B) = \text{Pr}(A, B) =  \text{Pr}(A) \times \text{Pr}(B)\)` (assuming independence)

- The *conditional probability* is the probability of an event occurring, given that another event has already occurred

  - `\(\text{Pr}(A | B)\)`, read as "probability of `\(A\)` given `\(B\)`"
---

## Discuss!

- Assume that we have two independent, fair, six-sided dice. What are:
  
  - `\(\text{Pr}(X_{1} = 1, X_{2} = 4)\)`?
  - `\(\text{Pr}(X_{1} \text{ even}, X_{2} \text{ even})\)`?

- Focusing on just the first die, what are:

  - `\(\text{Pr}(X_{1} = 1 | X_{1} \text{ is even})\)`?
  
  - `\(\text{Pr}(X_{1} = 1 | X_{1} \text{ is odd})\)`?
  
  - `\(\text{Pr}(X_{1} = \text{ is odd} | X_{1}  = 1)\)`?

---


class: center, middle

# Naive Bayes

---

## Motivating example

- Suppose I have two coins in my pocket, `\(C_{1}\)` and `\(C_{2}\)`

- I pull out a coin from random and flip it a bunch of times. Then repeat for a total of 7 iterations.  At each iteration, I record the following information:

  - The coin: `\(C_{1}\)` or `\(C_{2}\)`
  
  - Order of heads and tails (Heads = 0, Tails = 1)
  
--

$$
`\begin{align*}
&amp;C_{1}: \text{0 1 1 1 1} \\
&amp;C_{1}: \text{1 1 0} \\
&amp;C_{2}: \text{1 0 0 0 0 0 0 1} \\
&amp;C_{1}: \text{0 1} \\
&amp;C_{1}: \text{1 0 0 1 1 1} \\
&amp;C_{2}: \text{0 1 1 1 0 1} \\
&amp;C_{2}: \text{1 0 0 0 0} \\
\end{align*}`
$$
---

## Motivating example

- Now you tell me that the next coin flip resulted in `\(\text{0 0 1}\)`. Did it come from `\(C_{1}\)` or `\(C_{2}\)`?

  - Classification problem

--

- Let each coin flip be a predictor (specifically, an indicator)

  - e.g. `\(X_{1} = 1\)` if the first coin flip is a 1, and `\(X_{1} = 0\)` otherwise

- Writing `\(X = (X_{1} \ X_{2} \ X_{3})\)`, we want to know `\(\text{Pr}(C_{1} | X = (\text{0 0 1}))\)`

--

- This classification problem is challenging because

  - Different number of predictors (flips) for each observation
  
  - Number of predictors can be large
  
--

- Is there any structure at all? 

---

### Motivating example: discuss!

- Based on our data, regardless of the sequence of flips, what did you observe for:

  - `\(\text{Pr}(C_{1})\)` 
  
  - `\(\text{Pr}(C_{2})\)`

--

- Now let's consider the values of the flips. Given that we are using coin `\(C_{1}\)`, what did you observe for:

  - `\(\text{Pr}(X_{1} = 1 |C_{1})\)`, `\(\text{Pr}(X_{2} = 1 |C_{1})\)`, `\(\text{Pr}(X_{3} = 1 |C_{1})\)`?

  - `\(\text{Pr}(X_{1} = 1 |C_{2})\)`, `\(\text{Pr}(X_{2} = 1 |C_{2})\)`, `\(\text{Pr}(X_{3} = 1 |C_{2})\)`?

---

## Motivating example cont.

- Assume that given a coin, the results from the flips are independent:

`$$\text{Pr}(X = (\text{0 0 1}) | C_{1}) = \text{Pr}(X_{1} = 0 | C_{1}) \text{Pr}(X_{2} = 0 | C_{1})\text{Pr}(X_{3} = 1 | C_{1})$$`

--

- In summary, we have the following info:

  - `\(\text{Pr}(C_{1})\)`, `\(\text{Pr}(C_{2})\)`
  
  - `\(\text{Pr}(X| C_{1})\)`, `\(\text{Pr}(X | C_{2})\)`

- But we want `\(\text{Pr}(C_{1} | X)\)`. How can we use these quantities to estimate this probability?

---


## Bayes theorem


- For events `\(A\)` and `\(B\)`: 

`$$\text{Pr}(A|B) = \frac{\text{Pr}(B|A)\text{Pr}(A)}{\text{Pr}(B)}$$`
--

- Letting `\(A\)` be the event of coin `\(C_{1}\)` and `\(B\)` the `\(X\)`:


$$
`\begin{align*}
\text{Pr}(C_{1}| X) &amp;= \frac{\text{Pr}(X | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(X)} \\
&amp;= \frac{\text{Pr}(X | C_{1}) \text{Pr}(C_{1})}{\sum_{k=1}^{K} \text{Pr}(X | C_{k})\text{Pr}(C_{k})} \\
&amp;= \frac{\text{Pr}(X | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(X | C_{1}) \text{Pr}(C_{1}) + \text{Pr}(X | C_{2}) \text{Pr}(C_{2})} 
\end{align*}`
$$

--

- Returning to our question: given coin flips of `\(X = \text{ 0 0 1}\)`, did it come from coin `\(C_{1}\)` or `\(C_{2}\)`?

---

## Bayes theorem

- Return to general setting with `\(K\)` possible class labels for `\(Y\)`

- We observe some data `\(X\)`

- Want to obtain `\(\text{Pr}(Y = k | X)\)`. How? Using Bayes theorem

$$
`\begin{align*}
\text{Pr}(Y = k | X) &amp;= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X)}} \\
&amp;= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X | Y = 1)}\text{Pr}(Y = 1) + \ldots + \text{Pr(X | Y = K)}\text{Pr}(Y = K)} 
\end{align*}`
$$
---

## Notation

- For remaining slides, let

  - `\(\pi_{k}(x) = Pr(Y = k)\)` is marginal or **prior** probability for class `\(k\)`
  - `\(f_{k}(x) = Pr(X = x | Y =k)\)` is the **density** for `\(X\)` in class `\(k\)`
  
  `$$Pr(Y=k| X =x) = \frac{\pi_{k}(x) f_{k}(x)}{\sum_{l=1}^{K} \pi_{l}(x) f_{l}(x)}$$`

--

- Recall Bayes classifier: classifies an observation `\(x\)` to the class for which `\(p_{k}(x) = Pr(Y = k |X=x)\)` is largest
  
  - Will need to estimate the `\(f_{k}(x)\)` to approximate Bayes classifier


---

## Naive Bayes

- With `\(p\)` predictors, `\(f_{k}(x)\)` is `\(p\)`-dimensional distribution

- **Naive Bayes** is a non-parametric method for classification tasks 

  - Uses the previous formula to classify
  
  - Does not assume a specific distribution for `\(f_{k}(x)\)`. Instead, assumes that within class `\(k\)`, the `\(p\)` predictors are independent:

`$$f_{k}(x) = f_{k1}(x_{1}) \times f_{k2}(x_{2}) \times \cdots \times f_{kp}(x_{p}) = \prod_{j=1}^{p}f_{kj}(x_{j})$$`

--

  - Like in the coin flip example, flips (covariates) `\(X_{1}, X_{2}, \ldots\)` were independent given the coin
  
  - Useful when `\(p\)` is large
  
  --
  
  - We get to decide each `\(f_{kj}(x)\)`


---

## Naive Bayes

- Once we have made the naive Bayes assumption, we just plug into the equation

`$$Pr(Y=k| X =x) = \frac{\pi_{k}(x) f_{k}(x)}{\sum_{l=1}^{K} \pi_{l}(x) f_{l}(x)}$$`
- Will need to estimate the marginal probabilities `\(\pi_{k}(x)\)` and the one-dimensional density function `\(f_{kj}(x)\)` using training data

- How do we estimate the one-dimensional density function `\(f_{kj}(x)\)` using training data? 

- First, note that we will only use the `\(j\)`-th predictor `\(x_{1j}, x_{2j}, \ldots, x_{nj}\)` to estimate `\(f_{kj}(x)\)`

---

## Estimating marginal probs

- We can estimate the `\(\pi_{k}(x)\)` as follows. Let:

  - `\(n\)` be the total number of observations
  
  - `\(n_{k}\)` be the number of observations where `\(Y = k\)`
  
    - Then `\(\sum_{k=1}^{K} n_{k} = n\)`
  
--

`$$\hat{\pi}_{k}(x) = \frac{n_{k}}{n}$$`



---

## Estimating density function


- If `\(X_{j}\)` is quantitative, we often assume that 

`$$(X_{j} | Y = k ) \sim N(\mu_{jk}, \sigma^2_{jk})$$`

  `\(\quad\)` - i.e. within each class `\(k\)`, the `\(j\)`-th predictor is drawn from a normal distribution with mean and variance `\(\mu_{jk},  \sigma^2_{jk}\)` specific to the class and predictor
  
--

- If `\(X_{j}\)` is qualitative,  then we can simply count the proportion of training observations for the `\(j\)`-th predictor corresponding to each class `\(k\)`

---

## Normal distribution

- Recall that normal distribution is symmetric, bell-shaped curve. Also called the **Gaussian** distribution

- Two parameters:

  - Mean `\(\mu\)` denotes center of the distribution
  
  - Variance `\(\sigma^2\)` explains the spread of the distribution
  
--

- We say `\(X \sim N(\mu, \sigma^2)\)` if `\(f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x- \mu)^2\right\}\)`

  - In `R`, the function `dnorm()` automatically evaluates this function for us!
---

## Gaussian naive Bayes

If we assume that the quantitative `\(X_{j}\)` have a normal distribution `\(f_{kj} = N(\mu_{kj}, \sigma^{2}_{kj})\)`, then we use the following estimates:

$$  
`\begin{align*}
\hat{\mu}_{jk} &amp;= \frac{1}{n_{k}} \sum_{i:y_{i}= k} x_{ij} \\
\hat{\sigma}^{2}_{jk} &amp;= \frac{1}{n_{k} - 1} \sum_{i:y_{i} = k}(x_{ij} - \hat{\mu}_{jk})^2
\end{align*}`
$$


---

### Example 1: easy



- Simulated `\(n= 30\)` data points belonging to one of `\(K = 2\)` classes with a single quantitative predictor `\(x\)` according to the following:

  - `\(f_{1}(x) = N(-1, 0.25)\)`

  - `\(f_{2}(x) = N(3, 1)\)`
  
  - `\(\pi_{1} = 0.7\)`
  
  - `\(\pi_{2} = 0.3\)`

--

- In the following slide, red is class 1 and blue is class 2
---

### Example 1

&lt;img src="04-naive-bayes-slides_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

### Example 1

True and estimated posterior probabilities of Class 1, along with predictor `\(x\)` and true class label

<div id="htmlwidget-3c84e8b1e9b3cd2fc7c9" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-3c84e8b1e9b3cd2fc7c9">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30"],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.999,0.999,0.998,0.994,0.988,0.798,0,0,0,0,0,0,0,0,0],[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0.999,0.998,0.997,0.965,0.004,0,0,0,0,0,0,0,0],[-2.239,-2.184,-1.726,-1.355,-1.35,-1.321,-1.295,-1.283,-1.056,-1.013,-0.963,-0.894,-0.845,-0.801,-0.644,-0.521,-0.499,-0.341,-0.157,-0.052,0.354,1.623,2.061,2.088,2.801,3.163,3.654,3.662,3.874,4.235],["1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","2","2","2","2","2","2","2","2","2"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>true_prob<\/th>\n      <th>pred_prob<\/th>\n      <th>x<\/th>\n      <th>true_class<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

### Example 2: more challenging



- Simulated `\(n= 30\)` data points belonging to one of `\(K = 3\)` classes with a single quantitative predictor `\(x\)` according to the following:

  - `\(f_{1}(x) = N(-1, 0.25)\)`

  - `\(f_{2}(x) = N(2, 1)\)`
  
  - `\(f_{3}(x) = N(0, 0.5625)\)`
  
  - `\(\pi_{1} = 0.3333333\)`
  
  - `\(\pi_{2} = 0.3333333\)`
  
  - `\(\pi_{3} = 0.3333333\)`
  
---

## Example 2

&lt;img src="04-naive-bayes-slides_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---

### Example 2

True and estimated posterior probabilities of Classes A and B, along with predictor `\(x\)` and true class label

<div id="htmlwidget-a1f97f2a9d15fa0a1915" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a1f97f2a9d15fa0a1915">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30"],[0.873,0.867,0.851,0.821,0.431,0.412,0.38,0.285,0.282,0.206,0.163,0.14,0.136,0.087,0.067,0.059,0.034,0.022,0.006,0.006,0.001,0,0,0,0,0,0,0,0,0],[0.994,0.991,0.983,0.958,0.36,0.341,0.31,0.233,0.231,0.181,0.156,0.144,0.141,0.115,0.105,0.101,0.087,0.079,0.065,0.065,0.042,0.036,0.006,0,0,0,0,0,0,0],[0.001,0.002,0.002,0.003,0.027,0.029,0.033,0.047,0.047,0.063,0.075,0.084,0.085,0.111,0.127,0.135,0.172,0.204,0.314,0.317,0.521,0.563,0.837,0.995,0.996,0.998,0.999,0.999,0.999,1],[0.004,0.005,0.007,0.009,0.027,0.028,0.029,0.032,0.032,0.036,0.039,0.042,0.043,0.052,0.059,0.063,0.084,0.107,0.223,0.227,0.593,0.675,0.979,1,1,1,1,1,1,1],[-1.548,-1.47,-1.326,-1.151,-0.381,-0.358,-0.318,-0.196,-0.193,-0.083,-0.012,0.029,0.037,0.151,0.212,0.24,0.356,0.439,0.669,0.675,0.995,1.055,1.515,2.525,2.575,2.783,2.93,2.942,2.976,3.399],["A","A","A","A","A","C","C","C","C","C","A","C","C","A","C","C","B","C","B","C","B","C","B","B","B","B","B","B","B","B"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>true_probA<\/th>\n      <th>pred_probA<\/th>\n      <th>true_probB<\/th>\n      <th>pred_probB<\/th>\n      <th>x<\/th>\n      <th>true_class<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

## Seeds data

*Contingency table* of predicted (rows) vs true (columns) seed variety using naive Bayes with `\(\color{blue}{\text{area}}\)` and `\(\color{blue}{\text{perimeter}}\)` as predictors:


```
##           
##            Canadian Kama Rosa
##   Canadian       65   11    0
##   Kama            5   55    7
##   Rosa            0    4   63
```

---

class: middle, center

## Your turn!

---


## Discriminative vs Generative

- Discriminative models draw boundaries in the data space
  
  - Focuses on predicting the labels of the data

- Generative models try to model how data is placed throughout the space

  - Focuses on explaining how the data was generated, while a discriminative model 
---

## Summary

- Both logistic regression and Naive Bayes are linear classifiers

- Logistic regression (discriminative)

  - Very commonly used when `\(K=2\)`
  
  - Makes prediction for probability using a direct functional form that relates `\(X\)` to `\(Y\)`

- Naive Bayes ("generative")

  - Assumption of conditional independence in features usually not true, but still works great
  
  - Figures out how data were generated given the results 
  
  - Doesn't tell you how the predictors relate to the response

  - Useful when `\(p\)` very large
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
