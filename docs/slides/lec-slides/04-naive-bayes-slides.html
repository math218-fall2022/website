<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Naive Bayes" />
    <script src="04-naive-bayes-slides_files/header-attrs-2.16/header-attrs.js"></script>
    <script src="04-naive-bayes-slides_files/htmlwidgets-1.5.4/htmlwidgets.js"></script>
    <link href="04-naive-bayes-slides_files/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="04-naive-bayes-slides_files/datatables-binding-0.24/datatables.js"></script>
    <script src="04-naive-bayes-slides_files/jquery-3.6.0/jquery-3.6.0.min.js"></script>
    <link href="04-naive-bayes-slides_files/dt-core-1.11.3/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="04-naive-bayes-slides_files/dt-core-1.11.3/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="04-naive-bayes-slides_files/dt-core-1.11.3/js/jquery.dataTables.min.js"></script>
    <link href="04-naive-bayes-slides_files/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
    <script src="04-naive-bayes-slides_files/crosstalk-1.2.0/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Math 218: Statistical Learning
]
.author[
### Naive Bayes
]
.date[
### 9/28/2022
]

---





class: center, middle

# Housekeeping

---


## Seeds data

- Data come from UCI Machine Learning Repository 

- Measurements of geometrical properties of kernels belonging to three different varieties of wheat

&lt;img src="04-naive-bayes-slides_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

* Ask me about seed banks
---

## Probability crash-course!

- The *multiplication rule of independent events* in probability: if two events `\(A\)` and `\(B\)` are independent, then the probability of them both occurring at the same time is equal to the product of the individual probabilities

  - `\(\text{Pr}(A \text{ and } B) = \text{Pr}(A, B) =  \text{Pr}(A) \times \text{Pr}(B)\)` (assuming independence)

- The *conditional probability* is the probability of an event occurring, given that another event has already occurred

  - `\(\text{Pr}(A | B)\)`, read as "probability of `\(A\)` given `\(B\)`"
---

## Discuss!

- Assume that we have two independent, fair, six-sided dice. What are:
  
  - `\(\text{Pr}(X_{1} = 1, X_{2} = 4)\)`?
  - `\(\text{Pr}(X_{1} \text{ even}, X_{2} \text{ even})\)`?

- Focusing on just the first die, what are:

  - `\(\text{Pr}(X_{1} = 1 | X_{1} \text{ is even})\)`?
  
  - `\(\text{Pr}(X_{1} = 1 | X_{1} \text{ is odd})\)`?
  
  - `\(\text{Pr}(X_{1} = \text{ is odd} | X_{1}  = 1)\)`?

---


class: center, middle

# Naive Bayes

---

## Motivating example

- Suppose I have two coins in my pocket, `\(C_{1}\)` and `\(C_{2}\)`

- I pull out a coin from random and flip it a bunch of times. Then repeat for a total of 7 iterations.  At each iteration, I record the following information:

  - The coin: `\(C_{1}\)` or `\(C_{2}\)`
  
  - Order of heads and tails (Heads = 0, Tails = 1)
  
--

$$
`\begin{align*}
&amp;C_{1}: \text{0 1 1 1 1} \\
&amp;C_{1}: \text{1 1 0} \\
&amp;C_{2}: \text{1 0 0 0 0 0 0 1} \\
&amp;C_{1}: \text{0 1} \\
&amp;C_{1}: \text{1 0 0 1 1 1} \\
&amp;C_{2}: \text{0 1 1 1 0 1} \\
&amp;C_{2}: \text{1 0 0 0 0} \\
\end{align*}`
$$
---

## Motivating example

- Now you tell me that the next coin flip resulted in `\(\text{0 0 1}\)`. Did it come from `\(C_{1}\)` or `\(C_{2}\)`?

  - Classification problem

--

- Let each coin flip be a predictor (specifically, an indicator)

  - e.g. `\(X_{1} = 1\)` if the first coin flip is a 1, and `\(X_{1} = 0\)` otherwise

- Writing `\(X = (X_{1} \ X_{2} \ X_{3})\)`, we want to know `\(\text{Pr}(C_{1} | X = (\text{0 0 1}))\)`

--

- This classification problem is challenging because

  - Different number of predictors (flips) for each observation
  
  - Number of predictors can be large
  
--

- Is there any structure at all? 

---

### Motivating example: discuss!

- Based on our data, regardless of the sequence of flips, what did you observe for:

  - `\(\text{Pr}(C_{1})\)` 
  
  - `\(\text{Pr}(C_{2})\)`

--

- Now let's consider the values of the flips. Given that we are using coin `\(C_{1}\)`, what did you observe for:

  - `\(\text{Pr}(X_{1} = 1 |C_{1})\)`, `\(\text{Pr}(X_{2} = 1 |C_{1})\)`, `\(\text{Pr}(X_{3} = 1 |C_{1})\)`?

  - `\(\text{Pr}(X_{1} = 1 |C_{2})\)`, `\(\text{Pr}(X_{2} = 1 |C_{2})\)`, `\(\text{Pr}(X_{3} = 1 |C_{2})\)`?

---

## Motivating example cont.

- Assume that given a coin, the results from the flips are independent:

`$$\text{Pr}(X = (\text{0 0 1}) | C_{1}) = \text{Pr}(X_{1} = 0 | C_{1}) \text{Pr}(X_{2} = 0 | C_{1})\text{Pr}(X_{3} = 1 | C_{1})$$`


- What are:

  - `\(\text{Pr}(X |C_{1})\)`?
  - `\(\text{Pr}(X |C_{2})\)`?
---

## Motivating example cont.

- In summary, we have the following info:

  - `\(\text{Pr}(C_{1})\)`, `\(\text{Pr}(C_{2})\)`
  
  - `\(\text{Pr}(X| C_{1})\)`, `\(\text{Pr}(X | C_{2})\)`

- But we want `\(\text{Pr}(C_{1} | X)\)`. How can we use these quantities to estimate this probability?

---


## Bayes theorem


- For events `\(A\)` and `\(B\)`: 

`$$\text{Pr}(A|B) = \frac{\text{Pr}(B|A)\text{Pr}(A)}{\text{Pr}(B)}$$`
--

- Letting `\(A\)` be the event of coin `\(C_{1}\)` and `\(B\)` the `\(X\)`:


$$
`\begin{align*}
\text{Pr}(C_{1}| X) &amp;= \frac{\text{Pr}(X | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(X)} \\
&amp;= \frac{\text{Pr}(X | C_{1}) \text{Pr}(C_{1})}{\sum_{k=1}^{K} \text{Pr}(X | C_{k})\text{Pr}(C_{k})} \\
&amp;= \frac{\text{Pr}(X | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(X | C_{1}) \text{Pr}(C_{1}) + \text{Pr}(X | C_{2}) \text{Pr}(C_{2})} 
\end{align*}`
$$

--

- Returning to our question: given coin flips of `\(X = \text{ 0 0 1}\)`, did it come from coin `\(C_{1}\)` or `\(C_{2}\)`?

---

## Bayes theorem

- Return to general setting with `\(K\)` possible class labels for `\(Y\)`

- We observe some data `\(X\)`

- Want to obtain `\(\text{Pr}(Y = k | X)\)`. How? Using Bayes theorem

$$
`\begin{align*}
\text{Pr}(Y = k | X) &amp;= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X)}} \\
&amp;= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X | Y = 1)}\text{Pr}(Y = 1) + \ldots + \text{Pr(X | Y = K)}\text{Pr}(Y = K)} 
\end{align*}`
$$
---

## Notation

- For remaining slides, let

  - `\(\pi_{k}(x) = Pr(Y = k)\)` is marginal or **prior** probability for class `\(k\)`
  - `\(f_{k}(x) = Pr(X = x | Y =k)\)` is the **density** for `\(X\)` in class `\(k\)`
  
  `$$Pr(Y=k| X =x) = \frac{\pi_{k}(x) f_{k}(x)}{\sum_{l=1}^{K} \pi_{l}(x) f_{l}(x)}$$`

--

- Recall Bayes classifier: classifies an observation `\(x\)` to the class for which `\(p_{k}(x) = Pr(Y = k |X=x)\)` is largest
  
  - Will need to estimate the `\(f_{k}(x)\)` to approximate Bayes classifier


---

## Naive Bayes

- With `\(p\)` predictors, `\(f_{k}(x)\)` is `\(p\)`-dimensional distribution

- **Naive Bayes** is a non-parametric method for classification tasks 

  - Uses the previous formula to classify
  
  - Assumes that within class `\(k\)`, the `\(p\)` predictors are independent:

`$$f_{k}(x) = f_{k1}(x_{1}) \times f_{k2}(x_{2}) \times \cdots \times f_{kp}(x_{p}) = \prod_{j=1}^{p}f_{kj}(x_{j})$$`

--

  - Like in the coin flip example, flips (covariates) `\(X_{1}, X_{2}, \ldots\)` were independent given the coin
  
  - What does this mean?
  
  - Useful when `\(p\)` is large
  
--
  
  - We get to decide each `\(f_{kj}(x)\)`


---

## Naive Bayes

- Once we have made the naive Bayes assumption, we just plug into the equation

`$$Pr(Y=k| X =x) = \frac{\pi_{k}(x) f_{k}(x)}{\sum_{l=1}^{K} \pi_{l}(x) f_{l}(x)}$$`
- Will need to estimate the marginal probabilities `\(\pi_{k}(x)\)` and the one-dimensional density functions `\(f_{kj}(x)\)` using training data

---

## Estimating marginal probs

- We can estimate the `\(\pi_{k}(x)\)` as the proportion of the observed data falling into each class. Let:

  - `\(n\)` be the total number of observations
  
  - `\(n_{k}\)` be the number of observations where `\(Y = k\)`
  
--

`$$\hat{\pi}_{k}(x) = \frac{n_{k}}{n}$$`


---

## Normal distribution

- Recall that normal distribution is symmetric, bell-shaped curve. Also called the **Gaussian** distribution

- Two parameters:

  - Mean `\(\mu\)` denotes center of the distribution
  
  - Variance `\(\sigma^2\)` explains the spread of the distribution
  
--

- We say `\(X \sim N(\mu, \sigma^2)\)` if its density is `\(f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left\{-\frac{1}{2\sigma^2}(x- \mu)^2\right\}\)`

  - In `R`, the function `dnorm()` automatically evaluates this function for us!
---

## Normal distribution

&lt;img src="04-naive-bayes-slides_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

---

## Estimating density functions

- First, note that we will only use the `\(j\)`-th predictor `\(x_{1j}, x_{2j}, \ldots, x_{nj}\)` to estimate `\(f_{kj}(x)\)`

--

- If `\(X_{j}\)` is *quantitative*, we often assume that 

`$$f_{kj}(x) = N(\mu_{kj}, \sigma^2_{kj})$$`
  - i.e. within each class `\(k\)`, the `\(j\)`-th predictor is drawn from a normal distribution with mean and variance `\(\mu_{kj},  \sigma^2_{kj}\)` 
  
  - These parameters are specific to the class `\(k\)` and predictor `\(j\)`
  
--

- If `\(X_{j}\)` is *categorical*,  then we can simply count the proportion of training observations for the `\(j\)`-th predictor corresponding to each class `\(k\)`

---

## Gaussian naive Bayes

If we assume that the quantitative `\(X_{j}\)` have a normal distribution `\(f_{kj}(x) = N(\mu_{kj}, \sigma^{2}_{kj})\)`, then we use the following estimates:

$$  
`\begin{align*}
\hat{\mu}_{kj} &amp;= \frac{1}{n_{k}} \sum_{i:y_{i}= k} x_{ij} \\
\hat{\sigma}^{2}_{kj} &amp;= \frac{1}{n_{k} - 1} \sum_{i:y_{i} = k}(x_{ij} - \hat{\mu}_{jk})^2
\end{align*}`
$$


---

### Example 1: easy



- Simulated `\(n= 30\)` data points belonging to one of `\(K = 2\)` classes with a single quantitative predictor `\(x\)` according to the following:

  - `\(f_{1}(x) = N(-1, 0.25)\)`

  - `\(f_{2}(x) = N(5, 4)\)`
  
  - `\(\pi_{1} = 0.7\)`
  
  - `\(\pi_{2} = 0.3\)`

--

- In the following slide, red is class 1 and blue is class 2
---

### Example 1

&lt;img src="04-naive-bayes-slides_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

### Example 1



- Assuming Gaussian Naive Bayes, my estimates are:

  - `\(\hat{\pi}_{1} =\)` 21/30 = 0.7 and  `\(\hat{\pi}_{2} =\)` 9/30 = 0.3

  - `\(\hat{\mu}_{11} =\)` -0.961
  
  - `\(\hat{\mu}_{21} =\)` 5.036
  
  - `\(\hat{\sigma}^{2}_{11}=\)`  0.426
  
  - `\(\hat{\sigma}^{2}_{21}=\)`  3.414

---

### Example 1

True and estimated posterior probabilities of Class 1, along with predictor `\(x\)` and true class label

<div id="htmlwidget-3c84e8b1e9b3cd2fc7c9" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-3c84e8b1e9b3cd2fc7c9">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30"],["1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","1","2","2","2","2","2","2","2","2","2"],[-2.184,-1.295,-1.321,-0.341,-1.726,-1.283,-0.157,-1.056,-0.894,-0.644,0.354,-1.013,-0.521,-0.499,-0.963,-1.355,-0.801,-0.052,-0.845,-2.239,-1.35,3.176,2.246,7.47,5.326,6.323,6.749,4.602,3.121,6.307],[0.997,0.999,0.999,0.993,0.999,0.999,0.984,0.999,0.999,0.997,0.78,0.999,0.996,0.996,0.999,0.999,0.998,0.974,0.998,0.997,0.999,0,0,0,0,0,0,0,0,0],[1,1,1,0.997,1,1,0.994,0.999,0.999,0.998,0.956,0.999,0.998,0.998,0.999,1,0.999,0.991,0.999,1,1,0,0,0,0,0,0,0,0,0]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>class<\/th>\n      <th>x<\/th>\n      <th>prob1<\/th>\n      <th>prob1_hat<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-center","targets":[0,1,2,3,4]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

### Example 2: more challenging



- Simulated `\(n= 30\)` data points belonging to one of `\(K = 3\)` classes (`A`, `B`, `C`) with a single quantitative predictor `\(x\)` according to the following:

  - `\(f_{1}(x) = N(-1, 0.25)\)`

  - `\(f_{2}(x) = N(0, 1)\)`
  
  - `\(f_{3}(x) = N(1, 0.5625)\)`
  
  - `\(\pi_{1} = 0.3333333\)`
  
  - `\(\pi_{2} = 0.3333333\)`
  
  - `\(\pi_{3} = 0.3333333\)`
  
---

## Example 2

&lt;img src="04-naive-bayes-slides_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;

--

- Why is this scenario more challenging?

---
### Example 2



- Assuming Gaussian Naive Bayes, my estimates are:

  - `\(\hat{\pi}_{A} =\)` 7/30 = 0.233, `\(\hat{\pi}_{B} =\)` 11/30 = 0.367, and `\(\hat{\pi}_{C} =\)` 12/30 = 0.4

  - `\(\hat{\mu}_{A1} =\)` -0.82
  
  - `\(\hat{\mu}_{B1} =\)` 0.151
  
  - `\(\hat{\mu}_{C1} =\)` 1.128
  
  - `\(\hat{\sigma}^{2}_{A1}=\)`  0.518
  
  - `\(\hat{\sigma}^{2}_{B1}=\)`  1.135
  
  - `\(\hat{\sigma}^{2}_{C1}=\)`  0.18
---

### Example 2

True and estimated posterior probabilities of Classes A and B, along with predictor `\(x\)` and true class label

<div id="htmlwidget-a1f97f2a9d15fa0a1915" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a1f97f2a9d15fa0a1915">{"x":{"filter":"none","vertical":false,"data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30"],["A","A","A","A","A","A","A","B","B","B","B","B","B","B","B","B","B","B","C","C","C","C","C","C","C","C","C","C","C","C"],[-0.012,-1.151,-1.326,-1.548,-1.47,0.151,-0.381,-1.005,0.942,0.575,0.976,1.399,0.525,-0.485,0.93,-1.331,-1.644,0.783,1.212,0.682,0.804,1.675,1.24,0.917,1.439,2.055,0.807,1.037,0.642,1.029],[0.156,0.781,0.791,0.782,0.788,0.077,0.442,0.757,0.001,0.007,0,0,0.01,0.522,0.001,0.792,0.769,0.002,0,0.003,0.001,0,0,0.001,0,0,0.001,0,0.005,0],[0.321,0.641,0.658,0.668,0.666,0.241,0.469,0.622,0.014,0.064,0.012,0.003,0.078,0.502,0.015,0.658,0.669,0.027,0.005,0.042,0.025,0.001,0.005,0.016,0.003,0.001,0.025,0.01,0.049,0.01],[0.549,0.211,0.203,0.215,0.208,0.539,0.442,0.228,0.325,0.424,0.318,0.245,0.44,0.395,0.328,0.203,0.228,0.365,0.272,0.393,0.359,0.217,0.268,0.331,0.24,0.196,0.358,0.305,0.404,0.306],[0.632,0.359,0.342,0.332,0.334,0.636,0.528,0.378,0.23,0.413,0.221,0.183,0.448,0.497,0.234,0.342,0.331,0.291,0.184,0.344,0.281,0.231,0.182,0.238,0.186,0.445,0.28,0.207,0.369,0.209]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>class<\/th>\n      <th>x<\/th>\n      <th>probA<\/th>\n      <th>probA_hat<\/th>\n      <th>probB<\/th>\n      <th>probB_hat<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"columnDefs":[{"className":"dt-center","targets":[0,1,2,3,4,5,6]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

## Seeds data

**Contingency table** of predicted vs true seed variety on training set using naive Bayes with `\(\color{blue}{\text{area}}\)` and `\(\color{blue}{\text{perimeter}}\)` as predictors:


```r
library(e1071)
nb_mod &lt;- naiveBayes(variety ~ area + perimeter, data = seeds)
table(preds = predict(nb_mod, seeds), true = seeds$variety)
```

```
##           true
## preds      Canadian Kama Rosa
##   Canadian       65   11    0
##   Kama            5   55    7
##   Rosa            0    4   63
```

---

## Seeds data


```r
nb_mod$apriori
```

```
## Y
## Canadian     Kama     Rosa 
##       70       70       70
```

```r
nb_mod$tables
```

```
## $area
##           area
## Y              [,1]      [,2]
##   Canadian 11.87386 0.7230036
##   Kama     14.33443 1.2157036
##   Rosa     18.33429 1.4394963
## 
## $perimeter
##           perimeter
## Y              [,1]      [,2]
##   Canadian 13.24786 0.3401956
##   Kama     14.29429 0.5765831
##   Rosa     16.13571 0.6169950
```

---


## Discriminative vs Generative

- Discriminative models draw boundaries in the data space
  
  - Focuses on predicting the labels of the data

- Generative models try to model how data is placed throughout the space

  - Focuses on explaining how the data was generated, while a discriminative model 
---

## Summary

- Both logistic regression and Naive Bayes are linear classifiers

- Logistic regression (discriminative)

  - Very commonly used when `\(K=2\)`
  
  - Makes prediction for probability using a direct functional form that relates `\(X\)` to `\(Y\)`

- Naive Bayes (generative)

  - Assumption of conditional independence in features usually not true, but still works very well
  
  - Figures out how data were generated given the results 
  
  - Doesn't tell you how the predictors relate to the response

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
