---
title: "Math 218: Statistical Learning"
author: "Tree-Based Methods"
date: "10/31/2022"
output: 
   xaringan::moon_reader:
    css: "math218-slides.css"
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
      slideNumberFormat: "%current%"  
editor_options: 
  chunk_output_type: console
---


```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = F, warning = F, message = F, fig.align = "center", fig.width = 5, fig.height = 5)

library(tidyverse)
library(broom)
library(ISLR)
library(tree)
library(lars)
library(randomForest)
library(gbm)
set.seed(1)
abalone <- read.table("data/abalone_data.txt", sep = ",",header = T)
abalone <- abalone %>%
  mutate_at(vars(-rings, - sex), function(x){x*200}) %>%
  mutate(rings = rings + runif(nrow(abalone), -0.5,0.5),
         age = rings + 1.5,
         weight = whole_wt)

heart<- read.table("data/heart_cleveland.txt", header = T, sep = ",") %>% 
  mutate(cp = as.factor(cp),
         sex = ifelse(sex == 1, "M", "F"),
         target = ifelse(target == 0,0, 1))
seeds <- read.table("data/seeds_dataset.txt", header = T) %>%
  mutate(variety= case_when(variety == 1~ "Kama",
                          variety == 2 ~ "Rosa",
                          T ~ "Canadian"),
         variety = as.factor(variety))
data(diabetes)
z <- cbind(diabetes$x, y = diabetes$y)
z[,1:10] <- apply(z[,1:10], 2, scale)
diabetes <- as.data.frame(z) %>%
 rename("bp" = "map",
        "total_chol" = "tc") 
```


## Housekeeping


- Lab 06 and HW 05 assigned today, both due Thursday 11/10 at 11:59pm

  - This is your last homework!

- E-mail me about project partners by tomorrow 11:59pm

- Third job candidate is giving talk next Wednesday from 12:30-1:30pm in 75 Shannon Street Room 224
  
  - Candidate: Taylor Okonek
  
  - Talk title: "Child Mortality Estimation in a Low- and Middle-Income Country Context"


---

## Tree-based methods

- These methods use a series of if-then rules to divide/segment the predictor space into a number of simple regions

- The splitting rules can be summarized in a tree, so these approaches are known as **decision-tree** methods

- Can be simple and useful for interpretation

- Decision trees can be applied to both regression and classification problems


--

- Typically not competitive with the best supervised learning approaches in terms of prediction accuracy

--

- We will may later discuss **bagging** and **random forests**

---

## Diabetes data

- Diabetes disease progression colored from low (purple) to high (red)

```{r fig.width=8, fig.height=5}
ggplot(diabetes, aes(x= age, y = total_chol, col =y))+
 geom_point()+
 scale_color_gradientn(colours = rainbow(5)[5:1]) +
 labs(y = "total cholesterol")+
 guides(col = "none")+
  theme(text = element_text(size = 20))
```

---

### Diabetes data: decision tree 

- A regression tree for predicting diabetes progression based on a person's age and total cholesterol

```{r diabetes_simple_tree, fig.width=8, fig.height=6}
set.seed(1)
tree_diabetes <- tree(y ~ age + total_chol, diabetes)
plot(tree_diabetes, col = "blue")
text(tree_diabetes, pretty = 0, cex = 2)
```

---


### Diabetes data: decision tree 


- Top split: observations with $\color{blue}{\text{total cholesterol}} \geq 0.126$ are assigned to right branch

 - Predicted progression is mean response value for observations within this region: $\hat{y} =$ `r round(mean(diabetes[diabetes$total_chol >= 0.126,]$y),1)`
 
- Observations with $\color{blue}{\text{total cholesterol}} < 0.126$ assigned to left branch, and further subdivided by $\color{blue}{\text{age}}$

--

- This tree has two *internal nodes* and three *terminal nodes*

 - The value in each terminal node is the mean of training $Y$ for the observations that fall there
 
---

## Terminology

- **Internal nodes**: points along the tree where the predictor space is split

 - First node is often referred to as **root node**

- **Terminal nodes** or **leaves**: regions where there is no further splitting

 - Decision trees are typically drawn upside down (leaves at bottom)
 
--

- What are the internal nodes and terminal nodes for these data?

--

- Any subnode of a given node is called a *child node*, and the given node, in turn, is the child's *parent node*.

---


### Diabetes data: results

- The tree stratifies the individuals into three regions of predictor space: $R_{1} = \{ X | \color{blue}{\text{total_chol} > 0.126}\}$, $R_{2} = \{ X | \color{blue}{\text{total_chol}} < 0.126, \color{blue}{\text{age}} < 0.075\}$, and $R_{3} = \{ X | \color{blue}{\text{total_chol}} < 0.126, \color{blue}{\text{age}} > 0.075\}$

```{r diabetes_regions}
c1 <-  0.1259
c2 <- 0.0749

xmin <- min(diabetes$age); xmax <- max(diabetes$age)
ymin <- min(diabetes$total_chol); ymax <- max(diabetes$total_chol)
ggplot(diabetes, aes(x= age, y = total_chol))+
 geom_point(col = "orange")+
 labs(y = "total cholesterol")+
 guides(col = "none") +
 geom_segment(x = xmin-1, xend = xmax+1, y = c1, yend = c1)+
 geom_segment(x = c2, xend = c2, y = c1, yend = ymin-1)+
 coord_cartesian(xlim =  c(xmin,xmax), ylim= c(ymin,ymax))+
 annotate("text", x = mean(c(xmin,xmax)), y = mean(c(c1, ymax)), label = expression(R[1]), size = 5) +
 annotate("text", x = mean(c(xmin,c2)), y = mean(c(c1, ymin)), label = expression(R[2]) ,size = 5) +
 annotate("text", x = mean(c(c2,xmax)), y = mean(c(c1, ymin)), label = expression(R[3]), size = 5) +
 theme(text = element_text(size = 20))

```

---

### Diabetes data: interpretation of results

- $\color{blue}{\text{total cholesterol}}$ is the most important factor for determining $\color{blue}{\text{diabetes progression}}$, and younger patients have less progression than older patients

--

- Given that a patient has higher $\color{blue}{\text{total cholesterol}}$, their age seems to play little role in their $\color{blue}{\text{diabetes progression}}$

- But among those who have cholesterol levels less than 0.126, the $\color{blue}{\text{age}}$ of a patient does affect $\color{blue}{\text{diabetes progression}}$


--

- Likely an over-simplification of the true relationships between $\color{blue}{\text{total cholesterol}}$, $\color{blue}{\text{age}}$, and $\color{blue}{\text{diabetes progression}}$

 - But easy to display and interpret
 
---

## Building a regression tree

1. Divide predictor space (the set of possible values for $X_{1}, \ldots, X_{p}$) into $M$ distinct and non-overlapping regions, $R_{1}, \ldots, R_{M}$

--

2. For every observation that lands in $R_{m}$, we output the same prediction: the mean of the training responses in $R_{m}$, $\hat{y}_{R_{m}} = \frac{1}{n_{m}} \sum_{i \in R_{m}} y_{i}$

--

What do these $R_{m}$ look like?

---

## Building a regression tree

- In theory, regions $R_{1},\ldots, R_{M}$ could have any shape. For simplicity, we divde predictor space into high-dimensional rectangle or *boxes*

--

- Goal: find boxes $R_{1},\ldots, R_{M}$ that minimize RSS, given by

$$\sum_{m=1}^{M} \sum_{i\in R_{m}} (y_{i} - \hat{y}_{R_{m}})^2$$

--

- However, it is computationally infeasible to consider every possible partition of the feature space into $M$ boxes!

---


## Building a regression tree

- We take a **top-down, greedy** approach known as *recursive binary splitting*

 - "Top-down": we begin at the top of tree where all observations belong to a single region, and then succesively partition
 
 - "Greedy": at each step, the *best* split is made rather than looking ahead and picking a split that would be better in some future step
 
--

- Making splits on the data as we go

---

## Details

- First, select the predictor $X_{j}$ and the cutpoint $s$ such that splitting the predictor space into the regions $\{X | X_{j} < s\}$ and $\{X | X_{j} \geq s\}$ leads to lowest RSS

- That is, for any predictor $j$ and cutpoint $s$, we define the pair

$$R_l(j,s) = \{X | X_{j} < s\} \text{ and } R_{r}(j,s) = \{X | X_{j} \geq s\}$$
--

- We seek the values of $(j, s)$ that minimize

$$\sum_{i:x_{i}\in R_l(j,s)} (y_{i} - \hat{y}_{R_{l}})^2 + \sum_{i:x_{i}\in R_r(j,s)} (y_{i} - \hat{y}_{R_{r}})^2$$
where $\hat{y}_{R_{l}}$ is the average of the training responses in $R_l(j,s)$

--

- In the diabetes example, the regions after the first split were $R_{l} = \{X | \color{blue}{\text{total cholesterol}} < 0.126\}$ and $R_{r} = \{X | \color{blue}{\text{total cholesterol}} \geq 0.126\}$
 
 
---

## Details (cont.)

- Then, repeat the process of looking for the best predictor and best cutpoint in order to split the data further so as to minimize RSS within each of the resulting regions

 - Instead of splitting entire predictor space, we split one of the two previously identified regions
 
 - Now we have three regions
 
--

- Again, split one of these further so as to minimize RSS. We continue this process until a stopping criterion is reached

---

## Details (cont.)

- We can also split on qualitative predictors. Then the regions would be 

$$
\begin{align*}
R_l(j,s) &= \{X | X_{j} = ``category \ A"\} \text{ and }\\
R_{r}(j,s) &= \{X | X_{j} \neq ``category \ A"\}
\end{align*}
$$

where $``category \ A"$ is one of the possible categories of $X_{j}$

--

- If $X_{j}$ has more than two levels, then we would need to choose the category that yields the best split

- E.g. if $X_{j}$ takes values in  $\{A, B, C\}$, would need to consider:

  - $\{X | X_{j} = A\}$ and $\{X | X_{j} \neq A\}$
  
  - $\{X | X_{j} = B\}$ and $\{X | X_{j} \neq B\}$
  
  - $\{X | X_{j} = C\}$ and $\{X | X_{j} \neq C\}$
---

## Splitting example

`swiss` data

---

## Possible issues

- This process may produce good predictions on the training set, but is likely to overfit the data. Why?

--

- A smaller tree with fewer splits/regions might lead to lower variance and better interpretation, at the cost of a little bias

--

- One possibile fix: build the tree only so long as the decrease in the RSS due to each split exceeds some (high) threshold

 - Will result in smaller trees, but may be short-sighted
 
---

## Tree pruning

- A better strategy is to grow a very large tree $T_{0}$, and then **prune** it back in order to obtain a smaller **subtree**

 - Idea: remove sections that are non-critical
 
--

 - How to best prune the tree? A CV approach might be intuitive, but is expensive
 
--

- **Cost complexity pruning** or weakest link pruning: consider a sequence of trees indexed by a nonnegative tuning parameter $\alpha$. For each value of $\alpha$, there is a subtree $T \subset T_{0}$ such that 

$$\left(\sum_{m=1}^{|T|} \sum_{i: x_{i} \in R_{m}} (y_{i} - \hat{y}_{R_{m}})^2 \right)+ \alpha |T|$$
 is as small as possible. 
 
--

 - $|T|$ = number of terminal nodes of tree $T$
 
 - $R_{m}$ is the rectangle corresponding to the $m$-th terminal node
 
---

## Cost-complexity pruning cont.

- $\alpha$ controls trade-off between subtree's complexity and fit to the training data

--

 - What is the resultant tree $T$ when $\alpha = 0$?

--
 - What happens as $\alpha$ increases? 
 
--

- Feels similar to the Lasso, maybe?

  - Just like in Lasso and ridge, for every value of $\alpha$, we have a different fitted tree $\rightarrow$ need to  choose a best $\alpha$

--

- Select an optimal $\alpha^{*}$ using cross-validation, then return to full data set and obtain the subtree corresponding to  $\alpha^{*}$

---

## Algorithm for building tree

0. Split data into train and validation sets

--

1. Using recursive binary splitting to grow a large tree on the training data, stopping according to a pre-determined stopping rule

--

2. Apply cost complexity pruning to the large tree in order to obtain a sequence of best trees as a function of $\alpha$

--

3. Use $k$-fold CV to choose $\alpha$: divide training data into $K$ folds. For each $k = 1,\ldots, K$:

 a) Repeat Steps 1 and 2 on all but the $k$-th fold
 
 b) Evaluate mean squared prediction error on the data in held-out $k$-th fold, as a function of $\alpha$.  Average the result for each $\alpha$
 
--

4. Choose $\alpha^{*}$ that minimizes the average error. Return the subtree from Step 2 that corresponds to $\alpha^{*}$ and use that for predictions

---

## Diabetes example

- Split data into 50/50 train and validation set, using  all predictors to build the large tree on the train set

```{r}
set.seed(1)
train_ids <- sample(1: nrow(diabetes), nrow(diabetes)/2)
tree_diabetes <- tree(y ~. , diabetes[train_ids,], 
                      control=tree.control(nobs = length(train_ids), mindev = 0))
summary(tree_diabetes)
```

- I perform minimal cost-complexity pruning, selecting $\alpha^*$ using 5-fold CV on the training data

  - For each $\alpha$, there is an associated CV-error estimate when fitting on the training data
  
  - For each $\alpha$, there is an associated test error on the held-out validation data

---

## Diabetes example cont.

```{r diabetes_cv, cache = T}
# CV for different alpha levels k
K<- 5
cv_diabetes <- cv.tree(tree_diabetes, K = K)
ids <- split(train_ids, ceiling(seq_along(1:length(train_ids))/(length(train_ids)/K))) 

y_train <- diabetes$y[train_ids]
y_test <- diabetes$y[-train_ids]
alphas <- (prune.tree(tree_diabetes)$k)[-c(1, length(prune.tree(tree_diabetes)$k))]
M <- (prune.tree(tree_diabetes)$size)[-c(1, length(prune.tree(tree_diabetes)$k))]
test_mse <- train_mse <- rep(NA, length(alphas))
for(i in 1:length(alphas)){
 # for  regression tree, devaince is sum of squared errors

 m <- alphas[i]
 prune_diabetes <- prune.tree(tree_diabetes, k = m)
  # train
 train_mse[i] <- mean((predict(prune_diabetes, diabetes[train_ids,]) - y_train)^2)
 
 ## test
 test_mse[i] <-  mean((predict(prune_diabetes,diabetes[-train_ids,]) - y_test)^2)
 
}


## k-fold CV
cv_preds <- matrix(NA, nrow = K, ncol = length(alphas))
for(k in 1:K){
   temp_tree <- tree(y ~., data = diabetes[unlist(ids[-k]),])
   temp_prune <- prune.tree(temp_tree)
   # M_curr <- temp_prune$size[-length(temp_prune$size)]
   for(i in 1:length(alphas)){
     m <- alphas[i]
     preds <- predict( prune.tree(temp_tree, k = m),  diabetes[ids[[k]],])
     cv_preds[k,i] <- mean((preds - diabetes$y[ids[[k]]])^2)
   }
   
}

cv_mse <- colMeans(cv_preds)
```

```{r fig.width=8}
alpha_hat <- alphas[which(cv_mse == min(cv_mse))]
min_size_cv <- M[which(cv_mse == min(cv_mse))]
min_size_test <- M[which(test_mse == min(test_mse))]
gg_color_hue <- function(n) {
  hues = seq(15, 375, length = n + 1)
  hcl(h = hues, l = 65, c = 100)[1:n]
}
data.frame(size = M, test = test_mse, train = train_mse, cv = cv_mse) %>%
 pivot_longer(cols = -1, names_to = "type") %>%
 ggplot(., aes(x = size, y = value, col = type))+
 geom_point() + 
 geom_line()+
 scale_x_continuous(breaks = seq(1,max(M), 2))+
 labs(x = "Tree size", y = "Mean squared error")+
 theme(text = element_text(size = 20))+
  geom_point(data.frame(x = c(min_size_cv, min_size_test),
                        y = c(min(cv_mse), min(test_mse)),
                        type = c("cv", "test")), 
             mapping = aes(x= x, y =y,col = type),
             shape = 10, size = 4)+
    guides(color = guide_legend(
    override.aes=list(shape = 16)))+
  ggtitle("Errors for tree fit to diabetes data")


```

- $K$-fold CV with $K = 5$. Best CV MSE at `r min_size_cv` leaves

- Note: while the CV error is computed as a function of $\alpha$, we often display as a function of $|T|$, the number of leaves

---

## Diabetes example: final result

```{r fig.height = 8, fig.width=8}
tree_full <- tree(y ~. , diabetes, 
                      control=tree.control(nobs = nrow(diabetes), mindev = 0))
prune_final <- prune.tree(tree_full, k = alpha_hat)
plot(prune_final)
text(prune_final, pretty = 0, cex = 2)
```

---

class: middle, center

## Classification trees

---

## Classification trees

- Very similar to regression tree, but now we are predicting a qualitative response

- Now, we predict that each observation belongs to the *most commonly occurring class* of training observations in the region to which it belongs

 - Often time, also interested in class proportions among training observations that fall in the region
 
---

## Classification trees

- Just like regression trees, use recursive binary splitting to grow the classification tree

- Now, RSS cannot be used as a criterion for making the splits

- Alternative: **classification error rate**, the fraction of training observations in the region that belong to the most common class:

$$E = 1 - \max_{k}(\hat{p}_{mk})$$
 where $\hat{p}_{mk}$ is the proportion of training observations in the region $R_{m}$ that are from class $k$
 
--

- However, classification error is not sufficiently sensitive to tree-growing

---

## Gini index

- The **Gini index** is a measure of the total variance across the $K$ classes

$$G_{m} = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})$$

- $G_{m}$ is small if all the $\hat{p}_{mk}$'s are close zero or one

--

- For this reason, Gini index is referred to as a measure of node *purity*
 
 - A small $G_{l}$ indicates that the node contains predominantly observations from a single class
 

---
## Gini index

$$G_{m} = \sum_{k=1}^{K} \hat{p}_{mk} (1 - \hat{p}_{mk})$$

- Example: 3 classes and 9 observations in three regions

```{r}
data.frame(Region1 = c(rep("A", 9), rep("B", 0), rep("C", 0)),
           Region2 = c(rep("A", 2), rep("B", 2), rep("C", 5)),
           Region3 = c(rep("A", 3), rep("B", 3), rep("C", 3))) %>%
  print.data.frame()
```

- In these three regions, what are the Gini indices $G_{1}, G_{2}, G_{3}$?

--

  - $G_{1} = 1(1-1) + 0(1-0) + 0(1-0) = 0$
  - $G_{2} = \frac{2}{9}(1-\frac{2}{9}) +\frac{2}{9}(1-\frac{2}{9}) + \frac{5}{9}(1-\frac{5}{9}) \approx 0.60$
  - $G_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 0.67$

---

## Entropy

- Alternative to Gini index is **cross-entropy**:

$$D_{m} = -\sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}$$

- Very similar to Gini index, so cross-entropy is also a measure of node purity

---

## Entropy

$$D_{m} = -\sum_{k=1}^{K} \hat{p}_{mk} \log\hat{p}_{mk}$$

- Same example: 3 classes and 9 observations in three regions

```{r}
data.frame(Region1 = c(rep("A", 9), rep("B", 0), rep("C", 0)),
           Region2 = c(rep("A", 2), rep("B", 2), rep("C", 5)),
           Region3 = c(rep("A", 3), rep("B", 3), rep("C", 3))) %>%
  print.data.frame()
```

- In these three regions, what are the cross-entropies $D_{1}, D_{2}, D_{3}$?

--

  - $D_{1} = -(1\log(1) + 0\log(0) + 0\log(0)) = 0$
  - $D_{2} = -(\frac{2}{9}\log(\frac{2}{9}) +\frac{2}{9}\log(\frac{2}{9}) + \frac{5}{9}\log(\frac{5}{9})) \approx 1$
  - $D_{3} = \frac{1}{3}(1-\frac{1}{3}) + \frac{1}{3}(1-\frac{1}{3})+\frac{1}{3}(1-\frac{1}{3}) = \frac{2}{3} \approx 1.1$


---

### Seeds data: full tree

- Recall the seeds data, where we have seven characteristics of three different grains: Kama, Rosa, and Canadian

--

```{r seeds, fig.width=6, fig.height=7}
seeds_full <- tree(variety ~., data = seeds,
                    control=tree.control(nobs = nrow(seeds), mindev = 0))
plot(seeds_full)
text(seeds_full, cex = 1)
title("Full tree")
```

---

## Seeds data: pruned tree

```{r seeds_cv, fig.height=8}
set.seed(2)
train <- sample(1:nrow(seeds), nrow(seeds)/2)
seeds_test <- seeds[-train,]
tree_seeds <- tree(variety ~. , data = seeds, subset = train)
cv_tree <- cv.tree(tree_seeds, FUN = prune.misclass)

best_size <- min(cv_tree$size[which(cv_tree$dev == min(cv_tree$dev))])
prune_seeds <- prune.misclass(tree_seeds, best = best_size)
plot(prune_seeds)
text(prune_seeds, cex = 2)
```


---

## Remarks

- The full classification tree for the seeds data has surprising characteristic: some splits yield two terminal nodes with the same predicted value

  - Why? The split increases node purity even if it may not reduce classification error
  
---

## Remarks

- We can also split on qualitative predictors. Consider again the abalone data, and we want to create a regression tree for $\color{blue}{\text{age}}$ using the predictor $\color{blue}{\text{age}} \in \{I, F, M\}$

```{r abalone_tree}
abalone_rl <- abalone %>%
 mutate(sex = factor(sex, c("I", "F", "M")))
plot(tree(age ~ sex, abalone_rl, control = tree.control(mindev= 0, nobs = nrow(abalone_rl))))
text(tree(age ~ sex, abalone_rl, control = tree.control(mindev= 0, nobs = nrow(abalone_rl))), cex = 2)
```
---

## Remarks cont.

- Trees vs. linear models: which is better? Depends on the true relationships between the response and the predictors


```{r out.width="50%"}
knitr::include_graphics("figs/08-trees/tree_linear.png")
```

.footnote[Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013). An introduction to statistical learning : with applications in R. New York :Springer]
---

## Remarks cont.

- Advantages: 

 - Easy to explain, and may more closely mirror human decision-making than other approaches we've seen
 
 - Can be displayed graphically and interpreted by non-expert
 
 - Can easily handle qualitative predictors without the need to create dummy variables
 
--

- Disadvantages:

 - Lower levels of predictive accuracy compared to some other approaches
 
 - Can be non-robust 
 
--

However, we may see that aggregating many trees can improve predictive performance!

