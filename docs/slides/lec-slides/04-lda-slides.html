<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="LDA/QDA" />
    <script src="04-lda-slides_files/header-attrs-2.16/header-attrs.js"></script>
    <script src="04-lda-slides_files/kePrint-0.0.1/kePrint.js"></script>
    <link href="04-lda-slides_files/lightable-0.0.1/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Math 218: Statistical Learning
]
.author[
### LDA/QDA
]
.date[
### 9/26/2022
]

---





class: center, middle

# Housekeeping

---

## Classification

- Qualitative variables takes values in an unordered set `\(\mathcal{C}\)`, such as:

  - `\(\color{blue}{\text{wine}} \in \{\color{blue}{\text{red}},\color{blue}{\text{white}},\color{blue}{\text{rose}}\}\)`

  - `\(\color{blue}{\text{diagnosis}} \in \{\color{blue}{\text{yes}}, \color{blue}{\text{no}} \}\)`

--

- We have predictors `\(X\)` and a qualitative response `\(Y\)` that takes values in `\(\mathcal{C}\)`

  - Supervised learning task

- Goal of classification: build a function `\(C(X)\)` that predicts a label for `\(Y\)`

--

  - We are often interested in the estimated *probabilities* that `\(X\)` belongs to a given category in `\(\mathcal{C}\)`

---

## Probability warm-up: discuss!

A fair, six-sided die is rolled. Let `\(X\)` denote the result of the die roll. 

1. What is `\(\text{Pr}(X = 1)\)`?

2. What is `\(\text{Pr}(X \text{ is even})\)`?

Now we are rolling two dice.  Let `\(X_{1}\)` denote the result of the first, and `\(X_{2}\)` the second. 

3. What is `\(\text{Pr}(X_{1} + X_{2} = 1)\)`?

4. What is `\(\text{Pr}(X_{1} + X_{2} &lt; 13)\)`?
---

## Probability crash-course!

- The *multiplication rule of independent events* in probability: if two events `\(A\)` and `\(B\)` are independent, then the probability of them both occurring at the same time is equal to the product of the individual probabilities

  - `\(\text{Pr}(A \text{ and } B) = \text{Pr}(A, B) =  \text{Pr}(A) \times \text{Pr}(B)\)`

- The *conditional probability* is the probability of an event occurring, given that another event has already occurred

  - `\(\text{Pr}(A | B)\)`, read as "probability of `\(A\)` given `\(B\)`"
---

## Discuss!

- Assume that we have two independent six-sided dice. What are:
  
  - `\(\text{Pr}(X_{1} = 1, X_{2} = 4)\)`?
  - `\(\text{Pr}(X_{1} \text{ and } X_{2} \text{ are even})\)`?

- Focusing on just the first die, what are:

  - `\(\text{Pr}(X_{1} = 1 | X_{1} \text{ is even})\)`?
  
  - `\(\text{Pr}(X_{1} = 1 | X_{1} \text{ is odd})\)`?
  
  - `\(\text{Pr}(X_{1} = \text{ is odd} | X_{1}  = 1)\)`?

---

## Heart attack data

- Health measurements from 303 patients, along with record of the presence of heart disease in the patient

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-1-1.png" style="display: block; margin: auto;" /&gt;

---

## Seeds data

- Data come from UCI Machine Learning Repository 

- Measurements of geometrical properties of kernels belonging to three different varieties of wheat

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;

* Ask about seed banks

---

## Why not linear regression?

- For seeds data, `\(\mathcal{C} = \{\text{Canadian, Kama, Rosa}\}\)`

- Could we encode the values as a quantitative response, such as:

`$$Y = \begin{cases}
1 &amp; \text{ if } \color{blue}{\text{Canadian} }\\
2 &amp; \text{ if } \color{blue}{\text{Kama}}\\
3 &amp; \text{ if } \color{blue}{\text{Rosa}} \end{cases}$$`

`\(\quad\)` and then fit a linear regression for this `\(Y\)`?

---

## Why not linear regression?

The heart disease data is formatted as:

`$$Y = \begin{cases}
0 &amp; \text{ if } \color{blue}{\text{no heart disease} }\\
1 &amp; \text{ if } \color{blue}{\text{heart disease}}\end{cases}$$`

- Here, `\(Y\)` is binary

--

- For binary response, could we use least squares to fit a linear regression model to `\(Y\)`?

  - Maybe fit a linear regression and predict `\(\color{blue}{\text{heart disease}}\)` if `\(\hat{Y} &gt; 0.5\)`
  
--
  
  - Can show that the estimate `\(\hat{\beta}_{0} + \hat{\beta}_{1} X_{1} + \ldots + \hat{\beta}_{p} X_{p}\)` is an estimate of `\(Pr(Y = 1 | X) = Pr(\color{blue}{\text{heart disease}} | X)\)`
  

---

## Why not linear regression?

- Fit a linear regression line for `\(\color{blue}{\text{disease}} = \beta_{0} + \beta_{1}\color{blue}{\text{age}} + \beta_{2}\color{blue}{\text{sex}}\)`

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

--

- In certain cases, linear regression might produce estimated probabilities less than `\(0\)` or bigger than `\(1\)`

---

## "exp"

- `\(\exp(x)\)` is shorthand for `\(e^{x}\)`

- `\(e \approx\)` 2.7182818 is a special number

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---

## "exp"

- `\(e^{0} = 1\)`

- `\(e^{a} \times e^{b} = e^{a+b}\)`

- `\(\frac{e^{a}}{e^{b}} = e^{a-b}\)`

- `\(e^{a} + e^{b} = \ldots e^{a} + e^{b}\)`

---

class: center, middle

# Discriminant analysis

---

## Motivating example

- Suppose I have two coins in my pocket, `\(C_{1}\)` and `\(C_{2}\)`

- I pull out a coin from random and flip it a bunch of times. Then repeat for a total of 7 iterations.  At each iteration, I record the following information:

  - Which coin?
  
  - Order of heads and tails (Heads = 0, Tails = 1)
  

$$
`\begin{align*}
&amp;C_{1}: \text{0 1 1 1 1} \\
&amp;C_{1}: \text{1 1 0} \\
&amp;C_{2}: \text{1 0 0 0 0 0 0 1} \\
&amp;C_{1}: \text{0 1} \\
&amp;C_{1}: \text{1 1 0 1 1 1} \\
&amp;C_{2}: \text{0 0 1 1 0 1} \\
&amp;C_{2}: \text{1 0 0 0 0} \\
\end{align*}`
$$
---

## Motivating example

- Now you tell me that the next coin flip resulted in `\(\text{0 0 1}\)`. Did it come from `\(C_{1}\)` or `\(C_{2}\)`?

--

- Let each coin flip be a covariate (specifically, an indicator)

  - e.g. `\(X_{1} = 1\)` if the first coin flip is a 1, and `\(X_{1} = 0\)` otherwise

- Writing `\(X = (X_{1}, X_{2}, X_{3})\)`, we want to know `\(\text{Pr}(C_{1} | X = (\text{0 0 1}))\)`

--

- This classification problem is challenging because

  - Different number of covariates for each observation
  
  - Number of covariates can be large
  
--

- Is there any structure at all? 

---

### Motivating example: discuss!

- Based on our data, regardless of the sequence of flips, what did you observe for:

  - `\(\text{Pr}(C_{1})\)` 
  
  - `\(\text{Pr}(C_{2})\)`

--

- Now let's consider the values of the flips. Given that we are using coin `\(C_{1}\)`, what did you observe

  - `\(\text{Pr}(X_{1} = 1 |C_{1})\)`?

  - `\(\text{Pr}(X_{1} = 1 |C_{2})\)`?

---

## Motivating example cont.

- Assume that given a coin, the results from the flips are independent

- `\(\text{Pr}(X = (\text{0 0 1}) | C_{1}) = \text{Pr}(X_{1} = 0 | C_{1}) \text{Pr}(X_{2} = 0 | C_{1})\text{Pr}(X_{3} = 1 | C_{1})\)`

--

- In summary, we have the following info:

  - `\(\text{Pr}(C_{1})\)`, `\(\text{Pr}(C_{2})\)`
  
  - `\(\text{Pr}(\text{flips} | C_{1})\)`, `\(\text{Pr}(\text{flips} | C_{2})\)`

- But we want `\(\text{Pr}(C_{1} | \text{flips})\)`. How can we use these quantities to estimate this probability?

---


## Bayes theorem


- For events `\(A\)` and `\(B\)`: 

`$$\text{Pr}(A|B) = \frac{\text{Pr}(B|A)\text{Pr}(A)}{\text{Pr}(B)}$$`
--

- Letting `\(A\)` be the event of coin `\(C_{1}\)` and `\(B\)` the `\(\text{flips}\)`:


$$
`\begin{align*}
\text{Pr}(C_{1}| \text{flips}) &amp;= \frac{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(\text{flips})} \\
&amp;= \frac{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1})}{\sum_{j=1}^{2} \text{Pr}(\text{flips} | C_{j})\text{Pr}(C_{j})} \\
&amp;= \frac{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1})}{\text{Pr}(\text{flips} | C_{1}) \text{Pr}(C_{1}) + \text{Pr}(\text{flips} | C_{2}) \text{Pr}(C_{2})} 
\end{align*}`
$$

---
## Bayes theorem

- Return to general setting with `\(K\)` possible class labels for `\(Y\)`

- We observe some data `\(X\)`

- Want to obtain `\(\text{Pr}(Y = k | X)\)`. How? Using Bayes theorem

$$
`\begin{align*}
\text{Pr}(Y = k | X) &amp;= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X)}} \\
&amp;= \frac{\text{Pr}(X | Y = k) \text{Pr}(Y = k)}{\text{Pr(X | Y = 1)}\text{Pr}(Y = 1) + \ldots + \text{Pr(X | Y = K)}\text{Pr}(Y = K)} 
\end{align*}`
$$
---

## Notation

- For remaining slides, let

  - `\(\pi_{k}(x) = Pr(Y = k)\)` is marginal or **prior** probability for class `\(k\)`
  - `\(f_{k}(x) = Pr(X = x | Y =k)\)` is the **density** for `\(X\)` in class `\(k\)`
  
  `$$Pr(Y=k| X =x) = \frac{\pi_{k}(x) f_{k}(x)}{\sum_{l=1}^{K} \pi_{l}(x) f_{l}(x)}$$`

--

- Recall Bayes classifier: classifies an observation `\(x\)` to the class for which `\(p_{k}(x) = Pr(Y = k |X=x)\)` is largest
  
  - Will need to estimate the `\(f_{k}(x)\)` to approximate Bayes classifier


---

## Discriminant analysis

- Dashed line is Bayes decision boundary between the two classes

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;


---

## Normal distribution

---

## Linear Discriminant Analysis

- First, consider the case of `\(p =1\)` (one predictor)

- **Assumption 1**: `\(f_{k}(x)\)` is normal distribution. 

  - With mean `\(\mu_{k}\)` and variance `\(\sigma_{k}^{2}\)` for class `\(k\)`,

`$$f_{k}(x) = \frac{1}{\sqrt{2\pi\sigma_{k}^2}} \exp\left\{{-\frac{1}{2\sigma_{k}^2}(x - \mu_{k})^2}\right\}$$`

--

- **Assumption 2**: `\(\sigma_{k}^2= \sigma^{2}\)` for all classes `\(k\)`

--

- Plugging into the formula: `$$p_{k}(x) = \frac{\pi_{k} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac{1}{2\sigma^2}(x - \mu_{k})^2\right\}}{\sum_{l=1}^{K} \pi_{l} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left\{-\frac{1}{2\sigma^2}(x - \mu_{l})^2\right\}}$$`


---

## Linear Discriminant Analysis

- Taking logs and simplifying terms that do no depend on `\(k\)`, classifying based on largest `\(p_{k}(x)\)` is equivalent to classifying based on largest **discriminant score**

`$$\delta_{k}(x) = x \cdot \frac{\mu_{k}}{\sigma^2} - \frac{\mu_{k}^{2}}{2\sigma^2} + \log(\pi_{k})$$`
--

- Bayes decision boundary are values `\(x^*\)` where the discriminant scores are equal across the `\(K\)` labels: 

`$$\delta_{1}(x^*) = \delta_{2}(x^*) = \ldots = \delta_{K}(x^*)$$`


---

## Linear discriminant analysis (LDA)

- We will typically need to estimate the `\(\mu_{k}\)` and `\(\sigma^2\)` from the data


- LDA uses the following estimates:

`\begin{align*}
\hat{\mu}_{k} &amp;= \frac{1}{n_{k}} \sum_{i:y_{i}= k} x_{i} \\
\hat{\sigma}^{2} &amp;= \frac{1}{n-K} \sum_{k=1}^{K} \sum_{i: y_{i} = k} (x_{i} - \hat{\mu}_{k})^2 \\
\hat{\pi}_{k} &amp;= \frac{n_{k}}{n} \qquad (**)
\end{align*}`

such that:

`$$\hat{\delta}_{k}(x)= x\cdot\frac{\hat{\mu}_{k}}{\hat{\sigma}} -\frac{\hat{\mu}_{k}^{2}}{2\hat{\sigma}^{2}} + \log(\hat{\pi}_{k})$$`

---

### LDA: example

- Simulated `\(n=30\)` data points belonging to one of `\(K = 2\)` classes with a single predictor `\(x\)` according to the following:

  - `\(f_{1}(x) = N(1.5, 1)\)`

  - `\(f_{2}(x) = N(-1.5, 1)\)`
  
  - `\(\pi_{1} = 0.2\)`
  
  - `\(\pi_{2} = 0.8\)`

--

- In the following slide, blue is class 1 and red is class 2
---

### LDA: example

- Solid line: true Bayes decision boundary. Dashed line: estimated decision boundary.

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-6-1.png" style="display: block; margin: auto;" /&gt;

---

## Linear discriminant analysis

- What about when `\(p&gt;1\)`? 

- Extend LDA classifier by assuming `\(X = (X_{1}, X_{2}, \ldots, X_{p})\)` is drawn from **multivariate normal** distribution

--

- Multivariate normal is characterized by mean vector `\(\mathbf{\mu}\)` and covariance matrix `\(\Sigma\)`
  
  - `\(\mathbf{\mu} = (\mu_{1}, \mu_{2}, \ldots, \mu_{p})\)`

  - `\(\Sigma\)` is `\(p \times p\)` matrix which describes correlations between each components in `\(X\)`
  
--

`$$f(x) = \frac{1}{(2\pi)^{p/2}|\Sigma|^{1/2}} e^{-\frac{1}{2}(x - \mathbf{\mu})'\Sigma^{-1} (x - \mathbf{\mu})}$$`


---

## Multivariate Normal

- Contour plots of two multivariate densities with `\(p=2\)` and `\(\mathbf{\mu} = (0,0)\)` (i.e. `\(MVN_{2}(\mu = (0,0)', \Sigma)\)`)

- Left: the two components are uncorrelated. Right: the two variables have correlation of 0.75.

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

## Linear discriminant analysis

- For `\(p &gt;1\)`, LDA assumes that observations in class `\(k\)` are drawn from `\(MVN_{p}(\mathbf{\mu}_{k}, \Sigma)\)`

  - Here, `\(\mathbf{\mu}_{k} = (\mu_{k1}, \mu_{k2}, \ldots, \mu_{kp})\)` and `\(\Sigma\)` is `\(p \times p\)`
  
--

- Plugging in `\(f_{k}(x) = MVN_{p}(\mathbf{\mu}_{k}, \Sigma)\)` into `\(p_{k}(x)\)` yields discriminant score

`$$\delta_{k}(x)= x^{T}\Sigma^{-1}\mathbf{\mu}_{k} - \frac{1}{2}\mathbf{\mu}_{k}^{T}\Sigma^{-1} \mathbf{\mu}_{k} + \log(\pi_{k})$$`
--

  - Despite this complicated-looking form, `\(\delta_{k}(x)\)` is still linear in predictors `\(x_{1}, \ldots, x_{p}\)`
  
---

## LDA example

- Simulated data with `\(p = 2\)` predictors and `\(K = 3\)` classes. Each class has 20 observations

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;


---

### LDA example

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

Dashed lines are the Bayes decision boundaries. If known, would yield the fewest misclassification errors.

---

### LDA example

&lt;img src="04-lda-slides_files/figure-html/lda_ests-1.png" style="display: block; margin: auto;" /&gt;

Solid lines are the estimated Bayesian decision boundaries.

---

## LDA: heart data


**Confusion matrix** of predicted (rows) vs true (columns) heart disease status using LDA

  - Predictors: `\(\color{blue}{\text{age}}, \color{blue}{\text{sex}}, \color{blue}{\text{chest pain}}\)`:

&lt;table&gt;
 &lt;thead&gt;
&lt;tr&gt;
&lt;th style="empty-cells: hide;border-bottom:hidden;" colspan="1"&gt;&lt;/th&gt;
&lt;th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="2"&gt;&lt;div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; "&gt;True&lt;/div&gt;&lt;/th&gt;
&lt;/tr&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt;   &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 0 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; 1 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 131 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 36 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 33 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 103 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

- Misclassification rate: (36 + 33)/303 = 0.228

- Of the observations with true `\(\color{blue}{\text{heart disease}}\)`, we make 36/139 = 0.201 errors

- Of the observation with true `\(\color{blue}{\text{no heart disease}}\)`, we make 33/164 = 0.259 errors

---

## Types of errors

- **False positive rate** (FPR): fraction of negative observations incorrectly classified as positive -- 0.201 in our example

- **False negative rate** (FNR): fraction of positive observations incorrectly classified as negative example -- 0.259 in our example

--

- Is a false positive or a false negative worse? In the heart data, maybe a false negative?

--

- This table was produced by classifying an observation at `\(\color{blue}{\text{heart disease}}\)` if `\(\hat{Pr}(\color{blue}{\text{heart disease}} | \color{blue}{\text{age}}, \color{blue}{\text{sex}}, \color{blue}{\text{chest pain}}) \geq 0.5\)`

  - Here, 0.5 is the threshold for assigning an observation to `\(\color{blue}{\text{heart disease}}\)` class
  
  - Can change threshold to any value in `\([0,1]\)`, which will affect error rates
  
---

## Varying threshold

&lt;img src="04-lda-slides_files/figure-html/lda_threshold-1.png" style="display: block; margin: auto;" /&gt;

- Overall error rate minimized at threshold of 0.5
- How to decide a threshold rate?

---

## ROC Curve


- **sensitivity** (true positive rate) = probability of a positive, conditioned on truly being positive = 1 - FNR

- **specificity** (true negative rate) = probability of a negative, conditioned on truly being negative = 1 - FPR

- The **ROC curve** plots both simultaneously

&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-11-1.png" style="display: block; margin: auto;" /&gt;

- **AUC** (area under the curve) summarizes overall performance

---

## Quadratic Discriminant Analysis (QDA)

- Similar to LDA, but lessens one assumption: allow a different covariance matrix (or variance) for each class `\(k\)` 

  - Now, `\(f_{k}(X) =  MVN_{p}(\mathbf{\mu}_{k},\Sigma_{k})\)` 

  - With QDA, discriminant function `\(\delta_{k}(x)\)` has `\(x\)` appearing as a quadratic function:
  
  `$$\delta_{k}(x) = -\frac{1}{2}x^{T}\Sigma_{k}^{-1}x + x^{T}\Sigma_{k}^{-1} \mathbf{\mu}_{k} -\frac{1}{2} \mathbf{\mu}_{k}^{T}\Sigma_{k}^{-1} \mathbf{\mu}_{k} - \frac{1}{2}\log|\Sigma_{k}| + \log(\pi_{k})$$`
  
--

- More flexible than LDA

---

## Example: simulated data

Simulated data under QDA model with `\(K=2\)` and `\(p=2\)` with `\(n_{1} = n_{2} = 50\)`.




&lt;img src="04-lda-slides_files/figure-html/unnamed-chunk-12-1.png" style="display: block; margin: auto;" /&gt;

- Purple curve is Bayes decision boundary, dashed black curve is estimated decision boundary under QDA, and dashed blue line is estimate under LDA

---

## Naive Bayes

- Up until now, we have assumed that for `\(p&gt;1\)`, `\(f_{k}(x)\)` is `\(p\)`-dimensional distribution

- Under **naive Bayes**, we do not assume a specific family of distribution. Instead, assume that within class `\(k\)`, the `\(p\)` predictors are independent:

`$$f_{k}(x) = f_{k1}(x_{1}) \times f_{k2}(x_{2}) \times \cdots \times f_{kp}(x_{p}) = \prod_{j=1}^{p}f_{kj}(x_{j})$$`

--

- Useful when `\(p\)` is large


---

## Naive Bayes

- If we assume each `\(f_{kj}\)` is Normal, then this is QDA with diagonal `\(\Sigma_{k}\)`

  - Under Normal (Gaussian) naive Bayes:

`$$\delta_{k}(x) = -\frac{1}{2}\sum_{j=1}^{p}\left(\frac{(x_{j}-\mu_{kj})^2}{\sigma_{kj}^{2}} + \log\sigma_{kj}^{2}  \right) + \log(\pi_{k})$$`

--

- Allows for *mixed feature types* (qualitative and quantitative)

---

## Logistic Regression vs LDA

- Return back to `\(K=2\)` setting (binary classification) 

- Recall in logistic regression: `$$\log\left(\frac{p_{x}}{1 - p_{x}}\right) = \log\left( \frac{Pr(Y=1 | X=x)}{Pr(Y=2 | X=x)}\right)= \beta_{0} + \beta_{1}x_{1} + \ldots + \beta_{p}x_{p}$$`
--

- In LDA, 
$$
`\begin{align*}
\log\left(\frac{Pr(Y= 1 | X=x)}{Pr(Y= 2 | X=x)}\right) &amp;= \log \left(\frac{\pi_{1} f_{1}(x)}{\pi_{2}f_{2}(x)}\right) \\
&amp;= \log\left(\frac{\pi_{1} \exp\left(-\frac{1}{2}(x - \mathbf{\mu}_{1})^{T} \Sigma^{-1}(x - \mathbf{\mu}_{1})\right)}{\pi_{2} \exp\left(-\frac{1}{2}(x - \mathbf{\mu}_{2})^{T} \Sigma^{-1}(x - \mathbf{\mu}_{2})\right))}\right) \\
&amp;= \ldots \\
&amp;= c_{0} + c_{1}x_{1}+ c_{2}x_{2} + \ldots + c_{p}x_{p}
\end{align*}`
$$
---

## Logistic Regression vs LDA

- LDA has same form of logistic regression for log-odds

- Differ in how parameters  are estimated 

  - Logistic regression uses conditional likelihood `\(Pr(Y|X)\)`
  - LDA uses full likelihood `\(Pr(X,Y)\)`
  
- Often similar results
  
---

## Summary

- Logistic regression is very commonly used when `\(K=2\)`

- LDA useful when `\(n\)` small, or the classes are well
separated, and Gaussian assumptions are reasonable

- Naive Bayes useful when `\(p\)` very large
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
