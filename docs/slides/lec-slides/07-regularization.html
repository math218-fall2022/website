<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Math 218: Statistical Learning</title>
    <meta charset="utf-8" />
    <meta name="author" content="Linear Model Regularization" />
    <meta name="date" content="2022-10-24" />
    <script src="07-regularization_files/header-attrs-2.16/header-attrs.js"></script>
    <link rel="stylesheet" href="math218-slides.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Math 218: Statistical Learning
]
.author[
### Linear Model Regularization
]
.date[
### 10/24/2022
]

---






## Housekeeping

- Midterm grades were released. Really well done!
  
- Stats candidate (Peter Gao) job talk: Thursday 12:30-1:30pm in Warner 101

---

## Revision opportunity 

- You may re-do one CONCEPTUAL question for *at most* half of the points back (e.g. if you originally lost 5 pts, then you would be eligible to gain back 2.5 pts)

  - This applies to questions with multiple parts
  
- E-mail me a PDF file with your revision. Make sure it is clearly noted which question (and which parts) you are revising
  
  
- You should only talk to me concerning your revision. *I will not answer questions via e-mail, so I encourage you to come to my office hours!* 
  
    - Please do not talk to peers or consult the internet. 
  
  - Must e-mail revision by 11:59pm, Wednesday 10/26
  

---


## Three popular classes of methods

1. **Subset selection**: identify a subset of the `\(p\)` possible predictors that we believe to be related to `\(Y\)`, then fit least-squares model (previous lecture)

--

2. **Shrinkage** or **regularization**: fit a model involving all `\(p\)` predictors, but the estimated coefficients are shrunken towards 0 relative to least-squares estimates (this lecture)

  - Can reduce variance and perform variable selection
  
--

3. **Dimension reduction**: project the `\(p\)` predictors into a `\(q\)`-dimensional space where `\(q &lt; p\)` (this lecture)

  - Achieved using `\(q\)` linear combinations or projections of the `\(p\)` predictors, then fit least squares on the `\(q\)` projections
  
---

class: middle, center 

# Shrinkage and Regularization

---

## Shrinkage methods

- Subset selection methods use least squares to fit a linear model that contains a subset of the predictors 

--

- Alternatively, could fit a model containing all `\(p\)` predictors that *constrains* or *regularizes* the coefficient estimates

 - **Shrinks** coefficient estimates towards zero
 
--

- Two common methods: ridge regression and the Lasso

---

## Ridge regression

- Least-squares procedures estimates coefficients by using values that minimize

`$$\text{RSS} = \sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2$$`
--
 
- The **ridge regression** coefficient estimates `\(\hat{\beta}^{R}\)` are obtained by minimizing

`$$\sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 + \lambda \sum_{j=1}^{p} \beta_{j}^2 = \text{RSS} +  \lambda \sum_{j=1}^{p} \beta_{j}^2$$`

`\(\qquad\)` where `\(\lambda \geq 0\)` is a **tuning parameter** that is determined separately
 
---

## Ridge regression

Want to minimize: 
`$$\color{orange}{\text{RSS}} + \color{red}{\lambda \sum_{j=1}^{p} \beta_{j}^2}$$`

--

- Let's consider each component:

  - `\(\color{orange}{\text{RSS}}\)`: minimized when coefficient estimates fit the data well

--

  - `\(\color{red}{\lambda \sum_{j=1}^{p} \beta_{j}^2}\)`: **shrinkage penalty** minimized when the estimates are close to 0

    - Has effect of *shrinking* the estimates of `\(\beta_{j}\)`
  
    - Note: `\(\beta_{0}\)` is *not* subject to penalty
  
---

## Ridge regression

Want to minimize: 
`$$\color{orange}{\text{RSS}} + \color{red}{\lambda \sum_{j=1}^{p} \beta_{j}^2}$$`
  
- `\(\lambda\)` controls relative impact of the two components

 - What happens when `\(\lambda = 0\)`? When `\(\lambda \rightarrow \infty\)`?
 
---

## Ridge example: diabetes data

- Fit ridge regression predicting `LC50` using all predictors, for various `\(\lambda\)`

  - Plot on left: estimated coefficients `\(\hat{\beta}^{R}\)` for each variable, as a function of `\(\lambda\)`

  - Plot on right: same ridge coefficient estimates, plotted as a function of `\(||\hat{\beta}_{\lambda}^{R}||_{2} / ||\hat{\beta} ||_{2}\)`, where `\(\hat{\beta}\)` is vector of least-squares estimates
  
    - `\(||\beta||_{2}\)` is the `\(l_{2}\)`-norm of a vector: `\(||\beta||_{2} = \sqrt{\sum_{j=1}^{p} \beta_{j}^2}\)`
    
    - `\(x\)`-axis can be thought of as amount of shrinkage
 
---

## Diabetes example

&lt;img src="07-regularization_files/figure-html/ridge_ex-1.png" style="display: block; margin: auto;" /&gt;

---


## Ridge regression: scaling

- Standard least-squares coefficient estimates are *equivariant* to scaling of the predictors

  - Example: suppose I fit model `\(Y = \beta_{0} + \beta_{1}X + \epsilon\)`. Results in fitted model with estimates `\(\hat{\beta}_{0}, \hat{\beta}_{1}\)`.
  
  - If I instead fit the model on  `\(Y = \beta_{0} + \beta_{1}(c\times X) + \epsilon\)` `\(X_{j}\)` for some constant `\(c\)`, then estimated coefficient is simply `\(\frac{1}{c}\hat{\beta}_{1}\)`

 - Enters `\(\text{RSS}\)` the same: `\(X\beta_{1} = (c X)(\frac{1}{c}\beta_{1})\)`

 
---

## Ridge regression: scaling

- In contrast, ridge regression coefficients estimates can change *substantially* when multiplying a given predictor by a constant

 - This is due to shrinkage penalty on the `\(\beta_{j}\)`'s
 
--

- Therefore, we usually apply ridge regression after **standardizing** the predictors:

`$$\tilde{x}_{ij} = \frac{x_{ij}}{\sqrt{\frac{1}{n} \sum_{i=1}^{n}(x_{ij} = \bar{x}_{j})^2}}$$`

  - Each standard predictor `\(\tilde{X}_{j}\)` will have standard deviation of 1, so final fit will not depend on the scale on which the predictors are measured
---

### Why does ridge do better?

- Generally, when the relationship between response and predictors is close to linear, least-squares estimates will be low bias but may have high variance

  - i.e. Small change in training data to lead to a dramatic change in estimates of least-squares coefficients
  


---
### Why does ridge do better?




- Simulated data with `\(n=\)` 50 observations and `\(p =\)` 45 predictors

- Displaying variance, squared bias, and test MSE

&lt;img src="07-regularization_files/figure-html/ridge_mse-1.png" style="display: block; margin: auto;" /&gt;

---

## The Lasso

- One disadvantage of ridge regression: model will still include all `\(p\)` predictors (in contrast to subset selection methods)

 - Shrinkage penalty `\(\color{red}{\lambda \sum \beta_{j}^2}\)` will shrink, but not set, coefficients to 0
 
 - Difficult for model interpretation

--

- The **Lasso** is a relatively recent alternative to ridge that overcomes this disadvantage

- Lasso coefficients `\(\hat{\beta}_{\lambda}^{L}\)` minimize the following:

`$$\sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 + \lambda \sum_{j=1}^{p} |\beta_{j}| = \color{orange}{\text{RSS}} +  \color{purple}{\lambda \sum_{j=1}^{p} |\beta_{j}|}$$`


---

## Lasso penalty

- Ridge minimizes `\(\color{orange}{\text{RSS}} + \color{red}{\lambda \sum_{j=1}^{p} \beta_{j}^2}\)`, whereas Lasso minimizes `\(\color{orange}{\text{RSS}} +  \color{purple}{\lambda \sum_{j=1}^{p} |\beta_{j}|}\)`

- Lasso uses an `\(l_{1}\)` penalty (i.e. absolute weights, instead of squared)

 - `\(l_{1}\)` norm of `\(p\)`-vector `\(\beta\)` is `\(||\beta||_{1} = \sum_{j=1}^{p} |\beta_{j}|\)`
 
--

- This `\(l_{1}\)` penalty forces some of the coefficient estimates to be exactly 0 when `\(\lambda\)` is sufficiently large

- Thus, Lasso performs *variable selection*

--

 - We say that the Lasso yelds *sparse* models -- models that involve only a subset of the variables
 
--

- Need a select a good value for `\(\lambda\)`

---

## Lasso: diabetes data

- Now fitting Lasso model to response `LC50`

&lt;img src="07-regularization_files/figure-html/lasso_diabetes-1.png" style="display: block; margin: auto;" /&gt;

--

- Depending on value of `\(\lambda\)`, the Lasso can produce a model involving any number of predictors
 
---

## Lasso and ridge

- Equivalent formulation of the problems:

  - Lasso coefficients estimates solve the problem

`$$\min_{\beta}\left\{ \sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 \right\} \quad \text{ subject to } \sum_{j=1}^{p} |\beta_{j}| \leq s$$`


  - Ridge coefficients solve the problem 

`$$\min_{\beta}\left\{ \sum_{i=1}^{n} \left(y_{i} - \beta_{0} - \sum_{j=1}^{p} \beta_{j} x_{ij}\right)^2 \right\} \quad \text{ subject to } \sum_{j=1}^{p} \beta_{j}^2 \leq s$$`

--

- `\(s\)` can be thought of as a budget



---

## Lasso variable selection

- Why does the lasso result in estimated coefficients that are exactly equal to 0, whereas ridge regression does not?

-  Consider the following plot, where we have `\(p=2\)` predictors

--

- Let `\(\hat{\beta} = (\hat{\beta}_{1}, \hat{\beta}_{2})\)` be the least-squares solution

- The teal diamond and circle are regions that satisfy the Lasso and ridge regression budget constraints for a given `\(s\)`

--

- Each ellipse you will see is a *contour*; all the points on a particular ellipse have the same RSS

  - As ellipse expands away from `\(\hat{\beta}\)`, the RSS increases (confirm that this makes sense!)
  

---

## Lasso variable selection

- The Lasso and ridge coefficient estimates are given by the first point at which the ellipse contacts the contraint region (diamond and circle)

&lt;img src="figs/06-selection/lasso_ridge.png" width="90%" style="display: block; margin: auto;" /&gt;





---


## Lasso variable selection

- Note: if `\(s\)` is large (i.e. `\(\lambda = 0\)`), then constraint regions will contain `\(\hat{\beta}\)`

--

- Lasso constraint has corners at each of the axes, so ellipse will often intersect constraint region at an axis

  - For larger `\(p\)`, many of the coefficient estimates equal zero at the same time
  
  - In previous plot, Lasso results in setting `\(\hat{\beta}_{1}^{L} = 0\)`


---

## Prediction: Lasso

Data simulated with `\(p = 45\)` predictors and `\(n = 50\)` observations, but only 2 predictors actually associated with response




&lt;img src="07-regularization_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

## Prediction: Lasso vs Ridge

- Comparison of squared bias, variance, and test MSE between Lasso (solid) and ridge (dotted) models on same set of data

- Both are plotted against `\(R^2\)` on training data

--

- Data were generated in a manner more closely aligned to Lasso than ridge

---


## Prediction: Lasso vs Ridge


&lt;img src="07-regularization_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---
## Lasso vs Ridge

- Neither approach will universally dominant the other

- In general, we expect the Lasso to perform better when the response is a function of only a relatively small number of prodictors

- Ridge might perform better when the response is a function of many predictors, all with coefficients of roughly equal size

--

- The number of predictors related to `\(Y\)` is never known *a priori* for real data

 - Using techniques such as CV can be used to determine which approach is better for a given dataset
 
---

### How to select tuning parameter?

- We require a method to determine which of the models under consideration is "best"

 - How to select `\(\lambda\)` (or equivalently, the constraint `\(s\)`)?
 
--

- Cross-validation gives us a simple solution: choose a range/grid of `\(\lambda\)` values, and compute the CV error rate for each value

 - Select the value of tuning parameter `\(\lambda^{*}\)` for which CV error is smallest
 
- The re-fit the model using all of the available observations and `\(\lambda^{*}\)`

---

## Diabetes data

- I perform 5-fold CV to determine the optimal `\(\lambda^*\)` value for applying ridge regression to the `diabetes` 

  - That is, the 5 folds are determined first and do not change
  
  - Then we implement the cross-validation method 51 times, once for every value in `\(\lambda = (0.001, 0.0126, ..., 3.98, 5.01, ..., 79.43, 100)\)`
  
- The plots display the estimated 5-fold CV test error for each `\(\lambda\)`, as well as the estimated `\(\hat{\beta}_{j}^{R}\)`

---

## k-fold CV: diabetes data

- Dashed line denotes `\(\lambda^*\)`. What do these plots tell us?


&lt;img src="07-regularization_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;
 
---

## Simulated data

- Return to the case of `\(n = 50\)` and `\(p=45\)`, but only 2 predictors are actually related to the response

- Perform LOOCV to obtain optimal `\(\lambda^*\)` for Lasso model

--

- The following plots shows the estimated LOOCV error and estimated `\(\hat{\beta}_{j}^{L}\)` plotted against various `\(\lambda\)` values

---

## LOOCV: simulated data

- Dashed line is once again optimal `\(\lambda^{*}\)`. What do these plots tell us?

&lt;img src="07-regularization_files/figure-html/lasso_loocv-1.png" style="display: block; margin: auto;" /&gt;

---

## Summary

- High-dimensional data (small `\(n\)`, large `\(p\)`) are becoming more available

- Research into methods that give *sparsity* (such as the Lasso) are thriving research areas

--

- Selection and regularization methods can help avoid overfitting!

- We usually do not perform these methods to interpret the values of the `\(\hat{\beta}_{j}^{R}\)` or `\(\hat{\beta}_{j}^{L}\)` themselves

  - Rather, we often just want to learn which predictors (if any) are actually related to the response, or to obtain good predictions
  
--

- Uncertainty quantification is difficult

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightLines": true,
"highlightStyle": "github",
"countIncrementalSlides": false,
"slideNumberFormat": "%current%"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
