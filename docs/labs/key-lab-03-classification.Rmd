---
title: "Lab 03 - Classification"
subtitle: "KEY"
output: html_document
date: "2022-09-03"
---

```{r message = F, warning = F}
library(tidyverse)
library(MASS)
library(e1071)
library(class)
bmd_dat <- read.csv("data/bmd.csv", header = T) %>%
  mutate(status = factor(fracture, levels = c("no fracture", "fracture")))

bdiag_dat <- read.csv("data/bdiag.csv", stringsAsFactors = T, header = T)
```

## Exercise 1

a) Obtain estimates for the mean parameters $\mu_{k}$. Store these as values `mu_frac` and `mu2_nofrac`.

```{r}
# means and std. deviation
K <- 2
mu_frac <- mean(bmd_dat$bmd[bmd_dat$status == "fracture"])
mu_nofrac <- mean(bmd_dat$bmd[bmd_dat$status ==  "no fracture"])
```

b) Obtain an estimate for the standard deviation of `bmd`. Store this as a value called `s_bmd`. Note, this is *not* as simple as using the function `sd()`. Refer back to the slides for the form!

```{r}
s_bmd <- sqrt(with(bmd_dat,
                       (sum((bmd[status=="no fracture"] - mu_nofrac)^2) +
                       sum((bmd[status=="fracture"]- mu_frac)^2))/
                    (length(bmd)-K)
                    )
                  )
```

c) Obtain prior probabilities for each class. Store these as values `p_frac` and `p_nofrac`.

```{r}
p_frac<- mean(bmd_dat$status == "fracture")
p_nofrac<- 1 - p_frac
```

By this point, you have done all the hard work!

d) Now, obtain the discriminant scores for each group when `bmd` = 0.50. That is, calculate $\delta_{k}(\color{blue}{\text{bmd}} =0.5)$ for each group $k$.


```{r}
bmd <- 0.5
# no fracture
bmd * mu_nofrac/s_bmd^2 - mu_nofrac^2/(2*s_bmd^2) + log(p_nofrac)

## fracture
bmd * mu_frac/s_bmd^2 - mu_frac^2/(2*s_bmd^2) + log(p_frac)
```

e) Based on your answer in (d), would you predict that someone with a `bmd` of 0.50 has a `fracture` or `no fracture`, and why?

For `bmd` = 0.50, the classification would be `fracture` given that this category has the highest value for the discriminant function.

2.

```{r}
myLDA <- function(x, mu, sd, pi){
  K <- length(mu)
  scores <- rep(NA, K)
  for(k in 1:K){
    scores[k] <- x * mu[k]/sd^2 - mu[k]^2/(2*sd^2) + log(pi[k])
  }
  return(scores)
}

myLDA(0.50, c(mu_frac, mu_nofrac), s_bmd, c(p_frac, p_nofrac))
```


2. Here, we will work with the dataset `bdiag.csv`, which includes several imaging details from patients that had a biopsy to test for breast cancer. The variable `diagnosis` classifies the biopsied tissue as `M` = malignant or `B` = benign. In addition, ten real-valued features are computed for each cell nucleus:

```{r}
bdiag_dat <- read.csv("data/bdiag.csv", stringsAsFactors = T, header = T)
```

a) Plot the distribution of the predictors. Describe what you see.

```{r}
ggplot(bdiag_dat %>% pivot_longer(cols = -c(1:2), names_to = "variable"),
       aes(x = value))+
  geom_histogram(bins = 15)+
  facet_wrap(~variable, scales = "free")
```

b) What proportion of patients in the dataset had a malignant biopsy?

```{r}
mean(bdiag_dat$diagnosis == "M")
```

c) Perform a logistic regression on the full data using `texture`, `smoothness`,  and `symmetry`. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

```{r}
glm_fit <- glm(diagnosis ~ texture + symmetry + smoothness, data = bdiag_dat, family= "binomial")
summary(glm_fit)
```

d) From your fitted model in (c), compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.


```{r}
glm_probs <- predict(glm_fit, type = "response")
glm_preds <- rep("B", nrow(bdiag_dat))
glm_preds[which(glm_probs >= 0.5)] <- "M"
glm_preds <- factor(glm_preds)
table(glm_preds, truth = bdiag_dat[, "diagnosis"])
```

Now split the data into a train set and a test set using the following code.

```{r eval = T}
set.seed(5)
train_ids <- sample(1:nrow(bdiag_dat), 0.5 * nrow(bdiag_dat))
test_ids <- (1:nrow(bdiag_dat))[-train_ids]
```

e) Fit a logistic regression model *on the training data* using the same predictors. Then compute the confusion matrix and overall fraction of correct predictions *for the test data*. 

```{r}
glm_train <- glm(diagnosis ~ texture + symmetry + smoothness, data = bdiag_dat[train_ids,], family= "binomial")

glm_probs_test <- predict(glm_train, newdata = bdiag_dat[test_ids,], type = "response")
glm_preds_test <- rep("B", length(test_ids))
glm_preds_test[which(glm_probs_test >= 0.5)] <- "M"
glm_preds_test <- factor(glm_preds_test)
table(glm_preds_test, truth = bdiag_dat[test_ids, "diagnosis"])
```

f) Repeat (e) using LDA.

```{r}
lda_train <- lda(diagnosis ~ texture + symmetry + smoothness, bdiag_dat[train_ids,])
lda_preds <- predict(lda_train, newdata = bdiag_dat[test_ids,])$class
table(lda_preds, bdiag_dat[test_ids, "diagnosis"])
```

g) Repeat (e) using QDA

```{r}
qda_train <- qda(diagnosis ~ texture + symmetry + smoothness, bdiag_dat[train_ids,])
qda_preds <- predict(qda_train, newdata = bdiag_dat[test_ids,])$class
table(qda_preds, bdiag_dat[test_ids, "diagnosis"])
```

h) Repeat (e) using naive Bayes.

```{r}
nb_train <- naiveBayes(diagnosis ~ texture + symmetry + smoothness, bdiag_dat[train_ids,])
nb_preds <- predict(qda_train, newdata = bdiag_dat[test_ids,])$class
table(nb_preds, bdiag_dat[test_ids, "diagnosis"])
```

i) Repeat (e) using KNN with $K = 1$. First, make another dataset called `bdiag_dat_scale` which centers and scales the predictors.

```{r}
set.seed(1)
bdiag_dat_scale <- bdiag_dat %>%
  mutate_if(is.numeric, scale) 
knn_preds <- knn(bdiag_dat_scale[train_ids,c("texture", "symmetry", "smoothness")],
                bdiag_dat_scale[test_ids,c("texture", "symmetry", "smoothness")],
                bdiag_dat_scale$diagnosis[train_ids], k = 1)
table(knn_preds, bdiag_dat_scale$diagnosis[test_ids])
```

j) Repeat (e) using KNN with $K = 15$

```{r}
set.seed(1)
knn_preds <- knn(bdiag_dat_scale[train_ids,c("texture", "symmetry", "smoothness")],
                bdiag_dat_scale[test_ids,c("texture", "symmetry", "smoothness")],
                bdiag_dat_scale$diagnosis[train_ids], k = 15)
table(knn_preds, bdiag_dat_scale$diagnosis[test_ids])
```

k) Which of these methods appears to provide the best results on this data?

