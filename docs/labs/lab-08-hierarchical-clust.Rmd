---
title: "Lab 08 - Hierarchical clustering"
subtitle: "Due Sunday, 10/9 at 11:59pm"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

```{r message = F, warning = F}
library(tidyverse)
```

# Introduction

This lab explores the hierarchical clustering technique for unsupervised learning. 

# Hierarchical clustering

We use the function `hclust()` to implement hierarchical clustering. We will simulate data as we did in the $K$-means lab:
 
```{r}
set.seed(2)
x <- matrix(rnorm(50 * 2), ncol = 2)
x[1:25, 1] <- x[1:25, 1] + 3
x[1:25, 2] <- x[1:25, 2] - 4
```

We need to pass in a distance matrix of the dissimilarities between observations. The distance matrix must always be $n \times n$, where $n$ is the number of observations. The Euclidean distance is easily calculated used `dist()`. Then we also need to specify the type of linkage.

```{marginfigure}
The other methods can be found in the help file for `hclust()`!
```

```{r}
hc_complete <- hclust(dist(x), method = "complete")
```

We could use the base `R` `plot()` function to plot the dendrogram. But I personally prefer the `ggplot()` look. The library `ggdendro` takes as input an output from `hclust()` and plots it for us using the `ggdendrogram()` function! Notice that you can add layers to it like you would any `ggplot()`.

```{r}
library(ggdendro)
ggdendrogram(hc_complete) +
  ggtitle("Complete linkage")
```

The numbers at the bottom of the plot identify each observation. 

To determine the cluster labels associated with a given cut of the dendrogram, we use the `cutree()` function. It takes in the output from `hclust()` the number of clusters we want to obtain: 

```{r}
cutree(hc_complete, 2)
```

It appears that for this data, complete linkage generally separates the observations into the correct groups!

# Correlation-based distance

We can compute a correlation-based distance using the `as.dist()` function, which  converts an arbitrary square symmetric matrix into a form that the `hclust()` function recognizes as a distance matrix.

We simulate data as follows:


```{r}
set.seed(3)
x <- matrix(rnorm(30 * 3), ncol = 3)
```

Then we use `cor()` and `as.dist()` to get the correlation-based distance!

```{marginfigure}
Note that the function `t()` transposes a matrix. 
```
```{r}
cor_x <- cor(t(x))
dd <- as.dist(1-cor_x)
hc_single <- hclust(dd, method = "single")
ggdendrogram(hc_single) +
  ggtitle("Single linkage with Correlation-Based Distance")
```

Typically, single linkage will yield *trailing* clusters: very large clusters onto which individual observations attach one-by-one.


# YOUR TURN!

## Exercise 1

We will visit the `USArrests` data we saw in class. 

```{r}
data("USArrests")
```

a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states together.

```{r}
hc_out <- hclust(dist(USArrests), method = "complete")
```

b) Cut the dendrogram at a height that results into four distinct clusters. What do you notice about the resulting clusters?

```{r}
clusts <- cutree(hc_out, 4)
table(clusts)

# USArrests %>%
#   mutate(cluster = clusts) %>%
#   arrange(cluster)
```

c) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

```{r}
arrests_std <- scale(USArrests)
hc_std <- hclust(dist(arrests_std), method = 'complete')
```

d) Cut the dendrogram from (c) to yield four distinct clusters. What effect does scaling the variables have on the hierarchical clustering obtained? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed? Provide a justification for your answer.

```{r}
clusts_std <- cutree(hc_std, 4)
table(clusts_std)

# data.frame(arrests_std) %>%
#   mutate(cluster = clusts_std) %>%
#   arrange(cluster)
```

e) Now continuing to use the standardized data, hierarchically cluster the states use a correlation-based distance with complete linkage. Cut the dendrogram to yield four distinct clusters. 

```{r}
cor_x <- cor(t(arrests_std))
dd <- as.dist(1-cor_x)
hc_cor <- hclust(dd, method = "complete")
clusts_cor <- cutree(hc_cor, 4)

```

f) Using the `table()` function, compare the clusters obtained when using the two different distance metrics on the standardized data. Based on the table, do the distance metrics result in similar clusters? Why or why not? *Note: Be careful here! The cluster numbers themselves are arbitrary. That is, cluster 1 could also be, say, cluster 4*.

```{r}
table(cor_dist = clusts_cor, euc_dist =clusts_std)
```

## Exercise 2: Building visualizations

*In the pre-midterm feedback, someone asked for some "cool code". I'm not sure if this is what you meant, but hopefully some of you find our resulting graphic "cool"!*

Thus far, we have been using the `ggdendrogram()` function to visualize our dendrograms. This is a basic wrapper function that makes our life easy, but doesn't allow for much customization. Here, we will use more functions from the `ggdendro` library to make cooler dendrograms!

a) Using the `hclust()` output obtained using the standardized data and Euclidean distance in Exercise 1c, convert the output into "dendrogram data" format using the following code. Replace the underscores with your `hclust` object. Make sure to remove `eval = F` before knitting!

```{r eval  = T}
dd <- dendro_data(hc_std, type = "rectangle")
```

b) Next, you will create a data frame called `cluster_df` for plotting purposes, which contains the following two columns:

  - `label`: the state 
  - `cluster`: the assigned cluster
  
Make sure to set `cluster` to be a factor. 

```{r eval = T}
cluster_df <- data.frame(label = rownames(USArrests),
                         cluster = clusts_std) %>%
  mutate(cluster = factor(cluster))
```

Once you have created `cluster_df`, run the following code *once* (don't forget to remove `eval = F` before knitting). *Note*: if you try running the code more than once, you will unfortunately make unncessary columns. If that happens, go back and re-run code chunks starting from where we created the `dd` object! 

```{r eval = T}
dd[["labels"]] <- merge(dd[["labels"]],cluster_df, by="label")
```

c) Time to plot the dendrogram! Replace the underscores with the appropriate values. Note that we want to *color* the labels by the cluster assignment, and that we want to *label* each observation by the state name. Run `label(dd)` to see what you should pass in for these values! *Note*: making pretty visualizations usually requires lots of ugly/customized code...

```{r eval = T}
ggplot() +
  geom_segment(data = segment(dd), 
               aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(data = label(dd), 
            aes(x = x, y = y, label = label, color = cluster),
            hjust = 0, size = 2) + # hjust = 0 will left-align text
  coord_flip() + # flip the coordinate axes to have horizontal tree
  scale_y_reverse(limits=c(6.1, -1)) + # make axis go from 6.1 to -1; larger range to accommodate text 
  theme(axis.line.y=element_blank(),
    axis.ticks.y=element_blank(),
    axis.text.y=element_blank(),
    axis.title.y=element_blank(),
    panel.background=element_rect(fill="white"),
    panel.grid=element_blank()) +
  guides(color = "none") + # remove legend for color
  labs(y = "dissimilarity") + 
  geom_hline(yintercept = hc_std$height[46] + 0.1, col = "orange", lty = "dashed")
```

d) To your plot above, add a dashed line in orange indicating where we might cut the tree to obtain the four clusters. 

Rather than hard-coding a value, try using the outputted heights from the algorithm. The heights of each fusion can be found by accessing the `height` of your `hclust()` output. Example, if `hc` is the name of my object, then I would access the heights using `hc$height`. *Note: you might want a bit of a "fudge factor" to make the graphic more aesthetically, i.e. cut the tree at the fusion height plus/minus a small amount.*

e) From your finished plot, try to describe the sorts of states that are clustered together. You might discuss geography, political leanings, size, etc.

<!-- ## Exercise 3 -->

<!-- We will implement one method to evaluate the cluster assignments when we know the true classes (which is rarely the case in real applications).  The metrics we use will feel very similar to FPR and FNR in binary classification. -->

<!-- Suppose my data are: -->

<!-- a) Returning to the iris data, use hierarchical clustering with complete linkage and Euclidean distance. Cut the tree to obtain three clusters. Then create a $3 \times 3$ table of the assigned clusters (rows) and the true species classes (columns). -->

<!-- ```{r eval  = F} -->
<!-- data("iris") -->
<!-- set.seed(15) -->
<!-- B <- 5000 -->
<!-- n<- nrow(iris) -->
<!-- match_mat2 <- match_mat3 <- match_mat4 <- counts2 <- counts3 <- counts4 <-  matrix(0, nrow= n, ncol = n) -->
<!-- # .632 bootstrap -->
<!-- for(b in 1:B){ -->
<!--   ids <- sample(1:n, round((1-1/exp(1))*n), replace = F) -->
<!--   dat<- iris[ids,1:4]  -->
<!--   hc_out <- hclust(dist(dat), method = "complete") -->
<!--   clusts2 <- cutree(hc_out, 2) -->
<!--   clusts3 <- cutree(hc_out, 3) -->
<!--   clusts4 <- cutree(hc_out, 4) -->
<!--   for(i in 1:(n-1)){ -->
<!--     if(i %in% ids){ -->
<!--       i_id <- which(ids == i);  -->
<!--       for(j in (i+1):n){ -->
<!--         if(j %in% ids){ -->
<!--           j_id <- which(ids == j) -->
<!--           match_mat2[i,j] <- match_mat2[i,j] + 1*(clusts2[j_id] == clusts2[i_id]) -->
<!--           counts2[i,j] <- counts2[i,j] + 1 -->
<!--           match_mat3[i,j] <- match_mat3[i,j] + 1*(clusts3[j_id] == clusts3[i_id]) -->
<!--           counts3[i,j] <- counts3[i,j] + 1 -->
<!--           match_mat4[i,j] <- match_mat4[i,j] + 1*(clusts4[j_id] == clusts4[i_id]) -->
<!--           counts4[i,j] <- counts4[i,j] + 1           -->
<!--         } -->

<!--       } -->
<!--     } -->

<!--   } -->
<!-- } -->

<!-- par(mfrow= c(1,1)) -->
<!-- M2 <-  match_mat2/counts2 -->
<!-- M3 <- match_mat3/counts3 -->
<!-- M4 <- match_mat4/counts4 -->
<!-- e2 <- ecdf(M2[upper.tri(M2)]) -->
<!-- e3 <- ecdf(M3[upper.tri(M3)]) -->
<!-- e4 <- ecdf(M4[upper.tri(M4)]) -->
<!-- plot(e3, col = "red", verticals = T) -->
<!-- lines(e2, verticals = T) -->
<!-- lines(e4, col = "blue", verticals = T) -->

<!-- M2_sort <- sort(M2[upper.tri(M2)]) -->
<!-- M3_sort <- sort(M3[upper.tri(M3)]) -->
<!-- M4_sort <- sort(M4[upper.tri(M4)]) -->

<!-- auc_mat <- matrix(0, nrow = length(M2_sort), ncol = 3) -->
<!-- for(i in 2:length(M2_sort)){ -->
<!--   auc_mat[i,] <- c((M2_sort[i] - M2_sort[i-1])*e2(M2_sort[i]), -->
<!--                    (M3_sort[i] - M3_sort[i-1])*e3(M3_sort[i]), -->
<!--                    (M4_sort[i] - M4_sort[i-1])*e4(M4_sort[i])) -->
<!-- } -->
<!-- colSums(auc_mat) -->

<!-- ## proportion increase -->
<!-- colSums(auc_mat)[1] -->

<!-- (colSums(auc_mat)[2] - colSums(auc_mat)[1])/(colSums(auc_mat)[1]) -->
<!-- (colSums(auc_mat)[3] - colSums(auc_mat)[2])/(colSums(auc_mat)[2]) -->


<!-- ``` -->


<!-- ```{r eval = F} -->
<!-- data("iris") -->

<!-- B <- 5000 -->
<!-- n<- nrow(iris) -->
<!-- match_mat2 <- match_mat3 <- match_mat4 <- matrix(0, nrow= n, ncol = n) -->

<!-- # semiparametric bootstrap -->
<!-- for(b in 1:B){ -->
<!--   # dat_jitter <- scale(iris[,1:4]) + rnorm(50*4, 0, 0.05) -->
<!--     dat_jitter <- iris[,1:4] + rnorm(50*4, 0, 0.05) -->
<!--   hc_out <- hclust(dist(dat_jitter), method = "complete") -->
<!--   clusts2 <- cutree(hc_out, 2) -->
<!--   clusts3 <- cutree(hc_out, 3) -->
<!--   clusts4 <- cutree(hc_out, 4) -->
<!--   for(i in 1:(n-1)){ -->
<!--       for(j in (i+1):n){ -->
<!--         match_mat2[i,j] <- match_mat2[i,j] + 1*(clusts2[i] == clusts2[j]) -->
<!--         match_mat3[i,j] <- match_mat3[i,j] + 1*(clusts3[i] == clusts3[j]) -->
<!--         match_mat4[i,j] <- match_mat4[i,j] + 1*(clusts4[i] == clusts4[j]) -->

<!--       } -->
<!--   } -->
<!-- } -->

<!-- M2<- match_mat2/B -->
<!-- M3 <- match_mat3/B -->
<!-- M4 <- match_mat4/B -->
<!-- e2 <- ecdf(M2[upper.tri(M2)]) -->
<!-- e3 <- ecdf(M3[upper.tri(M3)]) -->
<!-- e4 <- ecdf(M4[upper.tri(M4)]) -->
<!-- plot(e3, col = "red", verticals = T) -->
<!-- lines(e2, verticals = T) -->
<!-- lines(e4, col = "blue", verticals = T) -->

<!-- M2_sort <- sort(M2[upper.tri(M2)]) -->
<!-- M3_sort <- sort(M3[upper.tri(M3)]) -->
<!-- M4_sort <- sort(M4[upper.tri(M4)]) -->

<!-- auc_mat <- matrix(0, nrow = length(M2_sort), ncol = 3) -->
<!-- for(i in 2:length(M2_sort)){ -->
<!--   auc_mat[i,] <- c((M2_sort[i] - M2_sort[i-1])*e2(M2_sort[i]), -->
<!--                    (M3_sort[i] - M3_sort[i-1])*e3(M3_sort[i]), -->
<!--                    (M4_sort[i] - M4_sort[i-1])*e4(M4_sort[i])) -->
<!-- } -->
<!-- colSums(auc_mat) -->

<!-- ## proportion increase -->
<!-- colSums(auc_mat)[1] -->

<!-- (colSums(auc_mat)[2] - colSums(auc_mat)[1])/(colSums(auc_mat)[1]) -->
<!-- (colSums(auc_mat)[3] - colSums(auc_mat)[2])/(colSums(auc_mat)[2]) -->

<!-- ``` -->


