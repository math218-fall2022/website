---
title: "Lab 03 - Logistic Regression and KNN"
subtitle: "due ? at 11:59pm"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

# Data and Packages

```{marginfigure}
You probably need to install the `e1071` and `class` libraries.
```

```{r message = F, warning = F}
library(tidyverse)
library(MASS)
library(class)
```

The file bmd.csv contains 169 records of bone densitometries (measurement of bone mineral density). The following variables were collected:

- `id` – patient’s number
- `age` – patient’s age
- `fracture` – hip fracture (fracture / no fracture)
- `weight_kg` – weight measured in Kg
- `height_cm` – height measure in cm
- `waiting_time` – time the patient had to wait for the densitometry (in minutes)
- `bmd` – bone mineral density measure in the hip

Our goal is predict whether someone had a hip `fracture` or `no fracture`.

```{r}
bmd_dat <- read.csv("data/bmd.csv", header = T)
names(bmd_dat)
str(bmd_dat)
```

# Logistic Regression

We being by fitting a logistic regression model to predict `fracture` using the  bone mineral density `bmd`. The `glm()` function can be used to fit a range of generalized linear models, including logistic regression. The syntax of `glm()` is almost identical to that of `lm()`, but we pass in the argument `family = "binomial"` in order to tell `R` to run a logistic regression rather than some other type of generalized linear model


```{r eval = F}
glm_mod <- glm(fracture ~ bmd, data = bmd, family = "binomial")
```

However, if we were to run the above code, we would get the following error message:

```{r fig.align="center", echo = F}
knitr::include_graphics("img/03-classification/glm_error.png")
```


This is because the type of variable that `fracture` is in the dataset is a character, when `R` is expecting it to be a factor or 0/1 variable. Let's remedy that here.

```{marginfigure}
Here, I use the `factor()` function to create a factor variable, and the `levels` argument specifies the order. In particular, "no fracture" is the 0 case, and "fracture" is the 1 case. I am saving it as a new variable called `status`.
```

```{r}
bmd_dat <- bmd_dat %>%
  mutate(status = factor(fracture, levels = c("no fracture", "fracture")))
glm_mod <- glm(status ~ bmd, data = bmd_dat, family = "binomial")
summary(glm_mod)
```

Just like with `lm()`, we can use the `coef()` function to obtain the estimated coefficients:

```{r}
coef(glm_mod)
```

The `predict()` function can be used to predict the probability that the market will go up, given values of the predictors. The `type = "response"` option tells `R` to output probabilities of the form `P(Y = 1|X)`, as opposed to other information such as the logit. 
If no new data are passed into the function, then the probabilities for the training data are computed.

```{r}
glm_probs <- predict(glm_mod, type = "response")
glm_probs[1:8]
```

If we want to obtain the predicted probability of `fracture` for someone with `bmd` = 0.75:

```{r}
predict(glm_mod, type = "response", newdata = data.frame(bmd = 0.75))
```

In any event, we need to convert these probabilities of `fracture` given `bmd` into actual classifications of `fracture` or `no fracture`. The following two commands create a vector of class predictions based on whether the predicted probability of a fracture is greater than or less than 0.5. 

```{marginfigure}
Can you explain what each command is doing?
```

```{r}
glm_pred <- rep("no fracture", nrow(bmd_dat))
glm_pred[glm_probs >= 0.5] <- "fracture"
```

Given these predictions, the `table()` function can be used to produce a confusion matrix in order to determine how many observations were correctly or incorrectly classified.

```{r}
glm_pred <- factor(glm_pred, levels = c("no fracture", "fracture") )
table(glm_pred, truth = bmd_dat$status)
```

The diagonal elements of the confusion matrix indicate correct predictions, while the off-diagonals represent incorrect predictions. Our model correctly predicted 110 patients with no fractures and 34 with a fracture. We can compute the fraction of days for which the prediction was correct:


```{r}
mean(glm_pred == bmd_dat$status)
```

We can add other predictors into the logistic regression model:

```{r}
glm_mod2 <- glm(status ~ age + sex + bmd, data = bmd_dat, family = "binomial")
summary(glm_mod2)
```

It seems like after adjusting for `bmd`, the other predictors are not very helpful in explaining whether or not someone has a `fracture`. However, it seems like adding these predictors variables helps us better classify patients with fractures:

```{r}
glm_probs2 <- predict(glm_mod2, type = "response")
glm_pred2 <- rep("no fracture", nrow(bmd_dat))
glm_pred2[glm_probs2 >= 0.5] <- "fracture"

glm_pred2 <- factor(glm_pred2, levels = c("no fracture", "fracture") )
table(glm_pred2, truth = bmd_dat$status)
```

# K-Nearest Neighbors Classification

We will now perform KNN using the `knn()` function, which is part of the `class` library. This function works rather differently from the other model-fitting functions that we have encountered thus far. Rather than a two-step approach in which we first fit the model and then we use the model to make predictions, `knn()` forms predictions using a single command. The function requires four inputs:

- A matrix or data frame containing the predictors associated with the training data, labeled `X_train` below.
- A matrix or data frame containing the predictors associated with the data for which we wish to make predictions, labeled `X_test` below.
- A vector containing the class labels for the training observations, labeled `class_train` below.
- A value for $k$, the number of nearest neighbors to be used by the classifier.

Also, remember that it is usually good practice to standardize the data so that all variables are given a mean of zero and a standard deviation of one. Then all variables will be on a comparable scale. The `scale()` function does just this. 

```{r}
bmd_dat_scale <- bmd_dat %>%
  mutate_if(is.numeric,scale)
```


Here, I will illustrate the `knn()` function to obtain predictions for the full set of data (i.e. we do not have a test set). I will predict `status` using the predictors `bmd` and `age` with $k = 3$.

We set a random seed before we apply `knn()` because if several observations are tied as nearest neighbors, then `R` will randomly break the tie. Therefore, a seed must be set in order to ensure reproducibility of results.

```{marginfigure}
What happens if you choose $k = 1$?
```

```{r}
set.seed(1)
knn_preds <- knn(train = bmd_dat_scale %>% dplyr::select(bmd,age),
                 test = bmd_dat_scale %>% dplyr::select(bmd,age),
                 cl = bmd_dat_scale$status, 
                 k = 3)
table(knn_preds, truth = bmd_dat_scale$status)
```

# YOUR TURN!

## Exercise 1
Here, we will work with the dataset `bdiag.csv`, which includes several imaging details from patients that had a biopsy to test for breast cancer. The variable `diagnosis` classifies the biopsied tissue as `M` = malignant or `B` = benign. In addition, ten real-valued features are computed for each cell nucleus:

- `radius` (mean of distances from center to points on the perimeter)
- `texture` (standard deviation of gray-scale values)
- `perimeter`
- `area`
- `smoothness` (local variation in radius lengths)
- `compactness` (perimeter^2 / area - 1.0)
- `concavity` (severity of concave portions of the contour)
- `concave points` (number of concave portions of the contour)
- `symmetry`
- `fractal dimension` (“coastline approximation” - 1)

```{r}
bdiag_dat <- read.csv("data/bdiag.csv", stringsAsFactors = T, header = T)
```

a) Plot the distribution of the predictors. Describe what you see.

b) What proportion of patients in the dataset had a malignant biopsy?

c) Perform a logistic regression on the full data using `texture`, `smoothness`,  and `symmetry`. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?

d) From your fitted model in (c), compute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.


Now split the data into a train set and a test set using the following code. Remember to set `eval = T` before you knit.

```{r eval = F}
set.seed(5)
train_ids <- sample(1:nrow(bdiag_dat), 0.5 * nrow(bdiag_dat))
test_ids <- (1:nrow(bdiag_dat))[-train_ids]
```

e) Fit a logistic regression model *on the training data* using the same predictors. Then obtain the confusion matrix and compute the fraction of correct predictions *for the test data*. 


f) Repeat (e) using KNN with $K = 1$. First, make another dataset called `bdiag_dat_scale` which centers and scales the predictors.


g) Repeat (e) using KNN with $K = 15$


h) Which of these methods appears to provide the best results on this data?

# Submission

Once you are finished with this lab, knit one last time, commit your changes, and push to GitHub. Then submit the PDF to Canvas.

Data courtesy of Biostatistics Collaboration of Australia.
