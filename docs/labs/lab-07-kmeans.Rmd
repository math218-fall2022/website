---
title: "Lab 07 - K-means clustering"
subtitle: "Due Thursday, 12/1 at 11:59pm"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

```{r message = F, warning = F}
library(tidyverse)
```

# Introduction

This lab explores the $K$-means clustering technique for unsupervised learning. 

# $K$-means Clustering

The function `kmeans()` performs $K$-means clustering in `R`. I will generate some simple  data in which there truly are two clusters. The first 25 observations have a mean shift relative to the next 25 observations

```{r}
set.seed(2)
x <- cbind(rnorm(50, 0, 2), rnorm(50, 0, 0.5))
x[1:25, 1] <- x[1:25, 1] + 3
x[25+1:25, 2] <- x[25+1:25, 2] - 4
```



The `kmeans()` function requires a minimum of two inputs: the matrix of features and the number of clusters $K$. You can (and will) specify other optional arguments including:

  - `nstart`: the number of random initializations for repeated $K$-means. Default is 1.
  - `iter.max`: the maximum number of iterations allowed to run the algorithm. Default is 10.
  - `algorithm`: the algorithm to run $K$-means. Defaults to "Hartigan-Wong"
  
```{marginfigure}
The default for `nstart` is 1, but that is not good practice!
```
  
If you specify `nstart` $>1$, the function will output the results from best initailization. Here, "best" is in terms of minimizing the objective.

```{marginfigure}
Note: you make get a warning from the `kmeans()` function stating that the alogirhm has failed to converge. In that case, you will need to increase `iter.max`!
```

Because $K$-means is an iterative algorithm, it should run until convergence. However, due to computation resources, the writers of the `kmeans()` function do not allow the algorithm to be run indefinitely until convergence. Instead, they make it so that the algorithm as up to `iter.max` iterations to run. If convergence is achieved prior to the `iter.max`-th iteration, great! If not, the function will return the clusters associated with that last iteration.

Due to the random initial cluster assignments, we should always set a seed for reproducibility! Also, while unlikely, it is possible that observations might tie when we calculate the distances between an observations and each centroid. In these instances, `kmeans()` will randomly choose one of the centroid clusters. Another good reason to set a seed!

Lastly, because we only learned the Lloyd algorithm in class, we will use that algorithm in our lab.

```{r}
set.seed(4)
km_out <- kmeans(x, centers = 2, nstart = 20, algorithm = "Lloyd")
km_out
```

The final cluster centroids (`centers`) and assignments (`cluster`) can be accessed as follows:

```{r}
km_out$centers
km_out$cluster
```

We can plot the data because our features are in 2D space. We will color each point by the cluster it was assigned to:

```{r}
data.frame(x) %>%
  mutate(cluster = km_out$cluster,
         cluster = factor(cluster),
         obs = 1:nrow(x)) %>%
  ggplot(., aes(x = X1, y = X2, col = cluster, label = obs)) +
  geom_text() 
```

Why should we use `nstart` $> 1$? The code below runs the $K$-means clustering algorithm twice: once with just 1 random assignment, and a second time with 20 different assignments. We then obtain the value of the best (smallest) objective value from each of runs by using the code `km_out$tot.withinss`. 

```{r}
set.seed(6)
km_out <- kmeans(x, 3, nstart = 1, algorithm = "Lloyd")
km_out$tot.withinss
km_out <- kmeans(x, 3, nstart = 20,  algorithm = "Lloyd")
km_out$tot.withinss
```


`tot.withinss` is the total within-cluster sum of squares, which we seek to minimize by performing K-means clustering. Thus, we can clearly see that by running the algorithm with many different initial assignments and reporting the best one, we can do a better job at making sure the observations within a cluster are similar. 

# Standardizing features

As mentioned in lecture, one decision that you need to make is to standardize the features such that each feature has standard deviation 1. We can use the `scale()` function, which both centers (yields a mean of 0) and standardizes (yields a standard deviation of 1). If you didn't want to center, you could specify `center = F` in the function call.

```{r}
std_x <- scale(x)
```

We can confirm that the resulting `std_x` does in fact have columns with mean 0 and standard deviation 1:

```{marginfigure}
Notice that we don't get the mean to be exactly 0, but it's close enough!
```
```{r}
apply(std_x, 2, mean)
apply(std_x, 2, sd)
```


# Your turn!

<!-- ## Exercise 1 -->

<!-- You will generate simulated data and then perform $K$-means clustering on the data. -->

<!-- a) Generate a simulated data set with 20 observations in each of three classes (i.e. 60 observations total), and 50 variables. Set a seed for reproducibility. -->

<!-- *Hint: you may use the `rnorm()` or `runif()` functions to generate data. To generate distinct classes, add a mean shift to the observations in each class like we did above.* -->

<!-- ```{r} -->
<!-- n <- 60 -->
<!-- p <- 50 -->
<!-- set.seed(2) -->
<!-- x <- matrix(rnorm(p * n), ncol = p) -->
<!-- x[1:20, 1] <- x[1:20, 1] + 3 -->
<!-- x[21:40, 2] <- x[21:40, 2] - 4 -->
<!-- x[41:60, 3] <- x[41:50, 2] + 1.5 -->
<!-- ``` -->

<!-- **Answer**: *Note: answers will vary based on simulated data!* -->

<!-- b) Perform $K$-means clustering with $K= 3$. Don't forget to set a seed! How well do the clusters that you obtained in K-means clustering compare to the true class labels? -->

<!-- *Hint: use the `table()` function to compare the true class labels to the class labels obtained by clustering. Be careful how you interpret the results: K-means clustering will arbitrarily number the clusters, so you cannot simply check whether the true class labels and clustering labels are the same.* -->


<!-- ```{r} -->
<!-- km_out <- kmeans(x, centers = 3, nstart = 20) -->
<!-- table(assign = km_out$cluster, true = rep(c(1:3), each = 20)) -->
<!-- ``` -->

<!-- c) Perform $K$-means clustering with $K = 2$. Describe your results. -->

<!-- ```{r} -->
<!-- km_out <- kmeans(x, centers = 2, nstart = 20) -->
<!-- table(assign = km_out$cluster, true = rep(c(1:3), each = 20) ) -->
<!-- ``` -->


<!-- d) Perform $K$-means clustering with $K = 4$. Describe your results. -->

<!-- ```{r} -->
<!-- km_out <- kmeans(x, centers = 4, nstart = 20) -->
<!-- table(assign = km_out$cluster, true = rep(c(1:3), each = 20) ) -->
<!-- ``` -->

## Exercise 1

We will perform a customer segmentation analysis on some simple, fake data. We have information about the `Gender`, `Age`, `AnnualIncome` ($1000s), and `SpendingScore` for customers at a mall. `SpendingScore` is a value between 1 and 100 that is generated based on each customer's spending history, preferences, etc.  The additional column `CustomerID` is just an index for each customer.

We will perform $K$-means clustering using all the pertinent quantitative features. 

```{r load-data}
dat <- read.csv("data/Mall_Customers.csv")
```

a) We will wrangle the data here. As we know, $K$-means cannot handle categorical features. Create a new data frame called `mall_std` which only contains male customers AND standardizes each of the continuous features.

b) Using a `for()` loop, fit $K$-means using Lloyd's algorithm to this data for $K = 1, 2, \ldots, 10$. For each $K$, obtain and store the value of the objective. Use a seed of 2, and multiple starts!


c) Now we will use the elbow method to choose a $K$. Plot the objectives values against the number of clusters $K$, using both points and lines. Make it so that the x-axis displays whole numbers. Based on your plot, what is the optimal $K$?


d) Using your optimal $K$, fit the $K$-means algorithm to the data, obtain the cluster assignments, and store them.


e) Create a visualization of the cluster assignments plotted in predictor space. Choose two features of your choice for the axes, and color the points by cluster assignment. *Stretch goal*: show on your plot where the cluster centroids are!


f) Based on your visualization in (e), try describing the types of customers within each cluster. Even though the features are standardized, we interpret positive values as being greater than average and negative values as lower. E.g. a standardized `Age` of 2 would correspond to a customer of older age.

## Exercise 2

Here, we will look at the same mall customer data, but this time perform $K$-means on the original, *non-standardized* data.

a) Create a new data frame called `mall_dat` that only contains male customers, but does NOT standardize the features. 


b) Repeat steps (b)-(e) from Exercise 1 using this `mall_dat` data frame. Use a seed of 2.


c) Discuss the differences (if any) that occurred when you ran $K$-means with standardized versus non-standardized data. You may want to discuss choosing an optimal $K$ and separation/interpretation of the resulting cluster assignments.
