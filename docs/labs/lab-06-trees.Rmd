---
title: "Lab: Tree-based methods"
subtitle: "Due Sunday, 10/23 at 11:59pm"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

```{r setup, message=FALSE, warnings = F}
library(tidyverse)
```

## Classification Trees

```{marginfigure}
The `tree()` function requires all your categorical predictors and response to be coded as factor variables!
```

```{r}
library(tree)
bmd_dat <- read.csv("data/bmd.csv") %>%
  mutate_if(is.character, factor)
```


We fit a classification decision tree using the `tree()` function from the `tree` package. The syntax is just as `glm()` where we have `response ~ predictor(s)`, and we specify the data.



```{r}
tree_bmd <- tree(fracture ~ medication + sex + bmd, data = bmd_dat)
```

Passing the fitted tree into the `summary()` function, we see the predictors that were ultimately select, as well as the number of terminal nodes $|T_{0}|$, and the training error rate. Note: if you don't see any predictors listed, that means the function used all of the predictors at least once in the decision tree!

```{r}
summary(tree_bmd)
```

What is that  "residual mean deviance" from the `summary()` output? "Deviance" is often used to describe the fit of a model; a smaller deviance corresponded to a better fit to the observed data. Here, deviance is defined as

$$-2 \sum_{m} \sum_{k} n_{mk} \log \hat{p}_{mk}$$

where $n_{mk}$ is the number of observations in the $m$th terminal node that belong to the $k$th class. Then the residual mean deviance is this quantity divided by $n - |T_{0}| = 169-11$.

Time to visualize the decision tree! The `plot()` function will easily display the fitted tree for us, and then `text()` displays the node labels. The argument `pretty = 0` instructs R to include the category names for any qualitative predictors, rather than simply displaying a generic a, b, c... letter for each category.

```{r}
plot(tree_bmd)
text(tree_bmd, pretty = 0)
```

Based on this tree, it seems like the predictor `bmd` really helps discriminate between those with and without a fracture.

While not always the most useful, you can type the variable name of your fitted tree to see the decision/split criteria at each branch of the tree:

```{r}
tree_bmd
```

Of course, we want to evaluate our model on some test data. We will split the data into test/train sets, then obtain predictions using `predict()` for the test data. For classification trees, we will usually want the actual class prediction, so we need to specify that in `predict()`:

```{marginfigure}
Note: we set a seed for two reasons! First, to generate the random test/train splits. Second, in the event of ties in classification, `predict()` will randomly choose one of the classes to output.
```

```{r}
set.seed(12)
train_ids <- sample(1:nrow(bmd_dat), 0.5*nrow(bmd_dat))
test_ids <- (1:nrow(bmd_dat))[-train_ids]
bmd_dat_test <- bmd_dat[test_ids,]
```

```{r}
tree_bmd <- tree(fracture ~ medication+ sex+ bmd , data = bmd_dat[train_ids,])
tree_preds <- predict(tree_bmd, bmd_dat_test, type = "class")
table(preds = tree_preds, true = bmd_dat_test$fracture)
```

In the contingency table above, we see that we do a great job of predicting the classes! We make correct predictions for `r sum(diag(table(preds = tree_preds, true = bmd_dat_test$fracture)))/nrow(bmd_dat_test)` of the observations in the test data set.

Maybe we think this tree is too complex and might want to prune it to improve results. We can use `cv.tree()` to perform cross-validation in order to determine the optimal level of tree complexity. This function uses cost-complexity pruning. 
We use the argument `FUN = prune.misclass` in order to indicate that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the `cv.tree()` function, which is deviance. The `cv.tree()` function will report the number of terminal nodes of each tree considered (`size`) as well as the corresponding error rate and the value of the cost-complexity parameter used (`k`, which corresponds to $\alpha$).

```{r}
set.seed(21)
cv_bmd <- cv.tree(tree_bmd, FUN = prune.misclass)
cv_bmd
```

`dev` corresponds to the number of cross-validation errors. So the tree with 4 terminal nodes has 1 cross-validation errors. In the following, I plot the error rate as a function of both `size` and `k`:

```{r}
data.frame(error_rate = cv_bmd$dev, size = cv_bmd$size, k = cv_bmd$k) %>%
  pivot_longer(cols = 2:3, names_to = "var", values_to = "value") %>%
  ggplot(., aes(x = value, y = error_rate)) +
  geom_point()+
  geom_line()+
  facet_wrap(~var, scales = "free")
```

Based on the output, the tree with only 2 terminal nodes seems to be the best. This results in a pretty boring tree, but oh well. I will prune the original tree to obtain the two-node tree instead using `prune.misclass()`:

```{r}
prune_bmd <- prune.misclass(tree_bmd, best = 2)
plot(prune_bmd)
text(prune_bmd, pretty = 0)
```

How well does this pruned tree perform on the test data set? Once again, we apply the `predict()` function:

```{marginfigure}
Notice that we are using the *pruned* tree now to make predictions!
```

```{r}
tree_pred <- predict(prune_bmd, bmd_dat_test,
                     type = "class")
table(preds = tree_pred, bmd_dat_test$fracture)
```

Pruning really helped! Not only does it make our tree more interpretable, but also increased our correct classification rate to `r sum(diag(table(preds = tree_pred, true = bmd_dat_test$fracture)))/nrow(bmd_dat_test)`!

## Regression Trees

Regression trees are coded very similarly to classification trees using the `tree()` function.

```{r}
fish <- read.csv("data/fish.csv") %>%
  mutate(Species = factor(Species))
set.seed(2)
n <- nrow(fish)
train_ids <- sample(1:n, n/2)

tree_fish <- tree(Weight ~ ., data = fish[train_ids,])
summary(tree_fish)
```


You'll notice that `tree()` only used three of the possible variables in the construction of the tree: `TotalLength`, `BodyLength`, and `Width`. For regression trees, deviance is the the sum of squared errors for the tree. Plotting the tree:

```{r}
plot(tree_fish)
text(tree_fish, pretty = 0)
```

Not surprisingly, the tree shows us that bigger fish tend to correspond to heavier fish.

Does pruning improve performance?

```{r}
cv_tree <- cv.tree(tree_fish)
cv_tree
```

Based on the function, pruning actually increases the error! That is, cross-validation selects the most complex tree. So we will not prune. To make predictions:

```{r}
ypred <- predict(tree_fish, newdata = fish[-train_ids,])
mse <- mean( (ypred - fish$Weight[-train_ids])^2)
```

Our test set MSE associated with the regression tree is `r round(mse)`.

## YOUR TURN!

```{marginfigure}
I'm pretty positive you will need to install this package! Once installed, type ?palmerpenguins to get a description of this very fun data!! I highly recommend you click on the first link under "Useful links".
```

```{r}
library(tree)
library(palmerpenguins)
data("penguins")
penguins <- penguins %>%
  na.omit()
```

```{marginfigure}
Cape Town, South Africa has penguins that are nicknamed "jackass" penguins because they bray like a donkey!
```

We will build some classification trees for the three penguin `species`: Adelie, Chinstrap, and Gentoo. 

## Exercise 1: Decision Trees

a) Create a training set containing 200 random samples of observations, and a test set containing the remaining observations. I suggest setting your own seed!


b) Fit a decision classification tree to the training data, using `species` as the response variable. For predictors, use everything except `year` and `island`. Then use the `summary()` function to produce summary statistics about the tree. Describe the results obtained: What is the training error rate? How many terminal nodes does the tree have? Which predictors did the tree end up using?


c) Type in the name of the tree object and run that line in order to get a detailed text output. Pick one of the terminal nodes and interpret the information displayed.


d) Create a plot of the tree and (generally) interpret the results. Don't forget to show the decision splits.


e) Predict the response on the test data and produce a contingency table comparing the true test labels to the predictions. What is the test misclassification error rate?



f) Time to determine the optimal tree size. Use the `cv.tree()` function on the training penguin data. Specify that the classification error rate should guide the cross-validation and pruning process. Don't forget to set a seed!



g) Create a plot with the tree size on the $x$-axis and the cross-validated classification error rate on the $y$-axis.



h) Which tree size corresponds to the lowest cross-validated classification error rate? Use code to answer this question (i.e. do not just look at the plot in g).
 

i) Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes. Plot it and show the splitting conditions!


j) Create a contingency table comparing the predictions on the test data using your pruned tree versus the true classes. 


k) Compare the misclassification rates on the test data between the pruned and unpruned trees. Which tree appears to be better?




