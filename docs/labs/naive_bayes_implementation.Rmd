---
title: "Naive Bayes implementation"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

We will manually implement Naive Bayes with some data I simulated for you! The data and associated `naive_bayes_implemendation.Rmd` file can be found in your Lab 03 Repository. 
Work through this exercise with your group.


```{r setup, message = F}
library(tidyverse)
dat <- readRDS("data/naive_bayes_sim.Rda")
```

## Data

We have two quantitative predictors `X1` and `X2`, and a categorical response with three levels.

1. Fill out the following code to plot histograms of each predictor, filled by the associated class label. Don't forget to set `eval = T` before you knit. Which variable(s) seems to be more helpful in understanding how the class labels were generated?

```{r eval  = F}
# histogram of first variable
ggplot(data = _____, aes(x = _____ , fill = _____))+
  geom_histogram(alpha = 0.5)

# also create a plot for the second variable
```

## Gaussian Naive Bayes

We will use normal distributions to approximate the densities $f_{kj}(x)$. In particular, for every class $k$ and predictor $j$, we assume that $X_{j} \sim N(\mu_{kj}, \sigma^2_{kj})$ with probability $\pi_{k}$. Time to estimate the $\pi_{k}, \mu_{kj}, \sigma^2_{kj}$.

2. Estimate the  $\pi_{k}$ marginal probabilities for each class. I've pseudo-coded for you below

```{r}
# create a variable n that stores the number of total observations

# create three variables (nA, nB, nC) that store the number of observations for each label

# create estimates piA_hat, piB_hat, piC_hat using (n, nA, nB, nC)

```

3. Estimate the mean $\mu_{kj}$ for $k = 1, 2, 3$ and $j = 1, 2$.

```{r eval = F}
# create muA1_hat as the average of x1 values among the observations in class A
muA1_hat <- dat %>% filter(Y == ____) %>% pull(X1) %>% mean()

# create muB1_hat

# create muC1_hat

# create muA2_hat

# create muB2_hat

# create muC2_hat

```

4. Estimate the standard deviations $\sigma_{kj}$ for $k = 1, 2, 3$ and $j = 1, 2$.

```{r}
# HINT: use code from part 3

# create sdA1_hat

# create sdB1_hat

# create sdC1_hat

# create sdA2_hat

# create sdB2_hat

# create sdC2_hat
```

We're basically done! We have everything we need to calculate the posterior probability of each class, given new predictors.

5. Suppose I have the following predictors: `X1` = 0, `X2` = -1. What are the posterior probabilities of class A, B, and C given these values? What would you classify this new point as?

```{r}
# Hint: the dnorm() function evaluates the normal distribution at a specified x for given mu and sd

# classA <- piA_hat x f_{A1}(x1) x f_{A2}(x2)

# classB <- piB_hat x f_{B1}(x1) x f_{B2}(x2)

# classC <- piB_hat x f_{C1}(x1) x f_{C2}(x2)

# Pr(A | X = (0, -1)) = ?

```

## Confirm

You can confirm you answer by running the following code:

```{r eval = F}
nb_mod <- naiveBayes(Y ~ X1 + X2, dat)
predict(nb_mod, newdata = data.frame(X1 = 0, X2 = -1), type = "raw")
```