---
title: "Lab 04 - LDA, QDA, and Naive Bayes"
subtitle: "Classification"
subtitle: "due ? at 11:59pm"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

# Data and Packages

```{marginfigure}
You probably need to install the `e1071` and `class` libraries.
```

```{r message = F, warning = F}
library(tidyverse)
library(MASS)
library(e1071)
```

We will continuous with the bone density data we saw last week. As a reminder, the file bmd.csv contains 169 records of bone densitometries (measurement of bone mineral density). The following variables were collected:

- `id` – patient’s number
- `age` – patient’s age
- `fracture` – hip fracture (fracture / no fracture)
- `weight_kg` – weight measured in Kg
- `height_cm` – height measure in cm
- `waiting_time` – time the patient had to wait for the densitometry (in minutes)
- `bmd` – bone mineral density measure in the hip

Our goal is predict whether someone had a hip `fracture` or `no fracture`.

```{r}
bmd_dat <- read.csv("data/bmd.csv", header = T) %>%
  mutate(status = factor(fracture, levels = c("no fracture", "fracture")))
```


# Linear Discriminant Analysis

We will now perform LDA on the `bmd` data. Let's again try and predict `status` using `bmd`. 
In `R`, we fit an LDA model using the `lda()` function, which is part of the MASS library. Notice that the syntax for the `lda()` function is identical to that of `lm()`, and to that of `glm()` except for the absence of the family option.

```{r}
lda_mod <- lda(fracture ~ bmd, data = bmd_dat)
lda_mod
```

Notice that the LDA output indicates that $\hat{\pi}_{\color{blue}{\text{no fracture}}} =$ `r lda_mod$prior[1]` and  $\hat{\pi}_{\color{blue}{\text{fracture}}} =$ `r lda_mod$prior[2]`.  It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of the $\mu_{k}$.

We can once again use the `predict()` function to obtain predictions for a given value of `bmd`. This function returns a list with three elements, the first of which is `class`.  This element contains LDA’s predictions about the `status`. This case, LDA preditions `fracture`.

```{r}
predict(lda_mod, newdata=data.frame(bmd=0.50))$class
```

As discussed in lecture, LDA and logistic regressions yield very similar predictions. In this case, they are identical:

```{marginfigure}
Note: the `predict()` function also returns an element called `posterior`. This is a matrix whose $k$-th column holds the posterior probability that the corresponding observation belongs to the $k$-th class (see slides). This is used to classify.
```

```{r}
lda_preds <- predict(lda_mod, bmd_dat)
mean(lda_preds$class == glm_pred)
mean(lda_preds$class == bmd_dat$status)
```

# Quadratic Discriminant Analysis

We will now fit a QDA model to the same data. QDA is implemented in `R` using the `qda()` function, which is also part of the `MASS` library. The syntax is identical to that of `lda()`.

```{r}
qda_mod <- qda(fracture ~ bmd, bmd_dat)
qda_mod
```

The output once again contains the group means and prior probabilities for each group. The `predict()` function works in exactly the same fashion as for LDA.

```{r}
qda_preds <- predict(qda_mod, bmd_dat)
mean(qda_preds$class == bmd_dat$status)
```

Interestingly, QDA does slightly worse than than LDA (and logistic regression). This suggests that QDA does not  capture the true relationship more accurately than the linear forms assumed by LDA and logistic regression. It could also be the case that we do not have enough training data $n$.

# Naive Bayes

Naive Bayes is implemented in `R` using the `naiveBayes()` function, which is part of the `e1071` library. The syntax is identical to that of the previous two methods:


```{r}
nb_mod <- naiveBayes(fracture ~ bmd, data = bmd_dat)
nb_mod
```

The output contains the estimated mean and standard deviation for `bmd` in each class. For example, the mean `bmd` for those with `fracture` is `r round(nb_mod$tables$bmd[1,1], 3)`, and the standard deviation of `bmd` for this group is `r round(nb_mod$tables$bmd[1,2], 3)`.

```{marginfigure}
Note that `predict()` for a Naive Bayes model defaults to outputting the predicted class. If we wanted estimates of the probabilities, we would need to pass in the argument `type = "raw"` into the function as well.
```

The `predict()` function is straightforward. We see that Naive Bayes yields the same predictions as QDA for this data:

```{r}
nb_preds <- predict(nb_mod, bmd_dat)
mean(nb_preds == qda_preds$class)
```

# YOUR TURN!

## Exercise 1
Here, we will hand-code the LDA method for the `bmd_dat` data we worked with above.  In particular, we will use LDA to predict `status` given `bmd`.

Recall that the discriminant function is given by:

$$\delta_{k}(\color{blue}{\text{bmd}}) = \color{blue}{\text{bmd}} \times \frac{\mu_{k}}{\sigma^2} - \frac{\mu_{k}^2}{2\sigma^2} + \log(\pi_{k})$$
where $\mu_{k}$ is the mean `bmd` for the group $k=$ `fracture` or $k=$ `no fracture`, $\sigma$ is the standard deviation for `bmd`, and $\pi_{k} = P(\color{blue}{\text{status}} = k)$ is the marginal probability of each category.


a) Obtain estimates for the mean parameters $\mu_{k}$. Store these as values `mu_frac` and `mu2_nofrac`.

b) Obtain an estimate for the standard deviation of `bmd`. Store this as a value called `s_bmd`. Note, this is *not* as simple as using the function `sd()`. Refer back to the slides for the form!

c) Obtain prior probabilities for each class. Store these as values `p_frac` and `p_nofrac`.

By this point, you have done all the hard work!

d) Now, obtain the discriminant scores for each group when `bmd` = 0.50. That is, calculate $\delta_{k}(\color{blue}{\text{bmd}} =0.5)$ for each group $k$.

e) Based on your answer in (d), would you predict that someone with a `bmd` of 0.50 has a `fracture` or `no fracture`, and why?

## Exercise 2
Write a function called `myLDA` that takes as input the following: a real-valued `x`, $K$-length vectors `mu` and `pi`, and a real_valued `sd`. Here, `x` represents that value of the predictor. `mu` and `pi` hold the mean of the predictor and marginal probabilities for each class $k$. `sd` is the standard deviation for LDA. The function should return a $K$-length vector of discriminant scores.

You can (and should) check to see if your function is working probably by passing in the the value 0.50 for `x` and confirm that your output matches your answer in 1(d).

The following code should help you get started by replacing the underlines with your own code. Remember to set `eval = T` when you knit.

```{r eval = F}
myLDA <- function(___, ____, ____, ____){
  K <- length(___)
  scores <- rep(NA, K)
  for(k in 1:K){
    scores[k] <- ________
  }
  return(scores)
}
```


## Exercise 1 
Here, we will once again work with the dataset `bdiag.csv`, which includes several imaging details from patients that had a biopsy to test for breast cancer. The variable `diagnosis` classifies the biopsied tissue as `M` = malignant or `B` = benign. In addition, ten real-valued features are computed for each cell nucleus:

- `radius` (mean of distances from center to points on the perimeter)
- `texture` (standard deviation of gray-scale values)
- `perimeter`
- `area`
- `smoothness` (local variation in radius lengths)
- `compactness` (perimeter^2 / area - 1.0)
- `concavity` (severity of concave portions of the contour)
- `concave points` (number of concave portions of the contour)
- `symmetry`
- `fractal dimension` (“coastline approximation” - 1)

```{r}
bdiag_dat <- read.csv("data/bdiag.csv", stringsAsFactors = T, header = T)
```

In last week's lab, you fit a logistic regression model and KNN model to this a train subset of this data, and evaluated the models on a set of test data. We will use the same test/train split here. Remember to set `eval = T` before you knit.

```{r eval = F}
set.seed(5)
train_ids <- sample(1:nrow(bdiag_dat), 0.5 * nrow(bdiag_dat))
test_ids <- (1:nrow(bdiag_dat))[-train_ids]
```


a) Fit an LDA model *on the training data* using the same predictors as last week (`texture`, `smoothness`,  and `symmetry`.) to predict `diagnosis`. Then compute the confusion matrix and overall fraction of correct predictions *for the test data*. 

b) Repeat (a) using QDA


c) Repeat (a) using naive Bayes.


d) Among the models in this lab (LDA, QDA, Naive Bayes) and last week (logistic regression, KNN), which method appears to provide the best results on this dataset?

# Submission

Once you are finished with this lab, knit one last time, commit your changes, and push to GitHub. Then submit the PDF to Canvas.

Data courtesy of Biostatistics Collaboration of Australia.
