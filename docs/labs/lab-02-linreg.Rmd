---
title: "Lab 02 - Linear Regression"
subtitle: "due ? at 11:59pm"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

# Getting started

```{marginfigure}
**IMPORTANT:** If there is no GitHub repo created for you for this assignment, it means I didn't have your GitHub username as of when I assigned the homework. Please let me know your GitHub username asap, and I can create your repo.
```

Go to the course GitHub organization and locate your HW 02 repo, which should be named `lab-02-linear-regression-[GITHUB USERNAME]`. Grab the URL of the repo, and clone it in RStudio. Refer to Lab 01 for step-by-step for cloning a repo and creating a new RStudio project.


## Configure git

```{marginfigure}
The name should be your github username and your email address is the one tied to your GitHub account.
```

Before we can get started we need to do one more thing.  Specifically,  we need to configure your git so that RStudio can communicate with GitHub. This requires two pieces of information: your email address and your GitHub username.

To do so, run the following:

```{r eval=FALSE}
library(usethis)
use_git_config(user.name = "github username", user.email = "your email")
```

# Packages

Here, we will make use of the package `ggfortify`. Check if it is installed by running 

```{r eval = F}
library(ggfortify)
```

If you receive an error message, then run the following command in your Console once:

```{r eval = F}
install.packages("ggfortify")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = F, 
                      warning = FALSE, 
                      message  = FALSE,
                      fig.align="center", fig.width=4, fig.height=4)
library(tidyverse)
library(MASS)
library(ISLR2)
library(ggfortify)
```


# Simple linear regression

We will begin by working with some data about fish. The data consist of record of seven common different fish species in fish market sales. Our goal will be to estimate the `Weight` (g) of a fish given its other characteristics. We have access to the following predictors: `Species`, `BodyLength` (nose to beginning of tail), `TotalLength` (nose to end of tail), `Height`, and `Width`. All physical measurements are in cm.

```{r}
fish <- read.csv("data/fish.csv", header = T)
head(fish)
```

We will begin by using the `lm()` function to fit a simple linear model, with `Weight` and the response and `BodyLength` as the predictor. The `summary()` function gives us estimates, standard errors, and $p$-values for the coefficients, as well as the $R^2$ for the model. 

```{r}
mod0 <- lm(Weight ~ BodyLength, data = fish)
summary(mod0)
```

We can access just the estimates $\hat{\beta}$ using the `coef()` function, and obtain confidence intervals (default are 95%).

```{r}
coef(mod0)
confint(mod0)
```

The `predict()` function can be used to obtain confidence and prediction intervals for the response, for a given value of the predictors:

```{marginfigure}
Compare the widths of the intervals. Why is there a difference?
```

`fit` is the estimated value, and `lwr` and `upr` give the lower and upper bounds of the 95%  intervals, respectively.

```{r}
predict(mod0, data.frame(BodyLength = c(15, 30, 45)), interval = "confidence")
predict(mod0, data.frame(BodyLength = c(15, 30, 45)), interval = "prediction")
```

# Diagnostics

Here, I plot the response and predictors, along with the least squares regression line. There is some evidence for non-linearity between `Weight` and `BodyLength`...

```{r}
ggplot(fish, aes(x = BodyLength,y = Weight))+
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

We can compute the residuals from a linear regression fit using the `residuals()` function. Here, it is clear that we have non-linearity.

```{r}
resids <- residuals(mod0)
data.frame(fitted = predict(mod0), residuals = resids) %>%
  ggplot(., aes(x = fitted, y = residuals))+
  geom_point()
```

Leverage statistics can be computed using the `hatvalues()` function. Here, I plot the leverage in the order of data collection:
 
```{r}
lev <- hatvalues(mod0)
data.frame(leverage = lev) %>%
  mutate(order = row_number()) %>%
  ggplot(., aes(x = order, y = leverage)) +
  geom_point()
```

Alternatively, we can take advantage of the `autoplot()` function from the `ggfortify` library we loaded. We pass in a fitted `lm()` object, and `autoplot()` will plot up to six diagnostic plots. Here, we will focus on four: residuals vs fitted, Normal q-q plot, standardized residuals vs fitted, and residuals vs leverage. Notice that `autoplot` will provide the indices of points of interest (possible outliers or high leverage) in the respective plots. 

```{r}
autoplot(mod0, which = c(1,2,3,5))
```

# Multiple linear regression

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The `summary()` function now outputs the regression coefficients for all the predictors.

```{marginfigure}
This is a bad example of naming models...oops! Usually we would want to have more informative variable names.
```

```{r}
mod1 <- lm(Weight ~ BodyLength + Height, data = fish)
summary(mod1)
```

Suppose I want to fit a multiple linear regression model using all the quantitative predictors. Rather than type them all out, I can use shorthand. On the right side of the `~`, typing a period `.` will tell `R` to use all the variables in the specified dataset (minus the specified response). Because I am currently only interested in the quantitative predictors, I will add `- Species` to tell `R` to use all the predicotrs *except* for `Species`.

```{r}
mod2 <- lm(Weight ~ . - Species, data = fish)
summary(mod2)
```

## Interactions

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `BodyLength:Height` tells `R` to include an interaction term between the two predictors. Here, we will use shorthand:

```{marginfigure}
Note, this is equivalent to `BodyLength + Height + BodyLength:Height`
```

```{r}
summary(lm(Weight ~ BodyLength * Height, data = fish))
```

## Non-linear transformations of predictors

Given a predictor $X$, we can create a predictor $X^{k}$ using `I(X^k)`. The function `I()` is necessary because the carat `^` has a special meaning. The following regresses `Weight` on `BodyLength` and `BodyLength^2`.

```{r}
mod_poly <- lm(Weight ~ BodyLength + I(BodyLength^2), data = fish)
summary(mod_poly)
```

The $p$-value for the quadratic term is significant, and suggests that the quadratic model might be an improvement over the simple linear regression model. We can use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{marginfigure}
Note, the `anova()` function can only compare nested models. That is, `mod0` is a "subset" of `mod_poly`.
```

```{r}
anova(mod0, mod_poly)
```

The `anova()` function performs a hypothesis test comparing the two models, where the null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the larger model is superior.The resulting $p$-value is small, and suggests that the model containing the quadratic term is superior to the simpler model that is only linear in `BodyLength`. This is not surprising, as we earlier saw evidence for non-linearity in the relationship between `Weight` and `BodyLength`.


## Qualitative predictors

Now, I will fit a  linear regression model using all the predictors, including the qualitative `Species` variable.

```{marginfigure}
What is our baseline level for `Species`?
```

```{r}
mod3 <- lm(Weight ~ ., data = fish)
summary(mod3)
```


Looking at a plot of residuals vs fitted values, it is clear that a linear model is still insufficient. However, it appears to be an improvement from the simpler model.

```{marginfigure}
What are some other predictors that you think would be helpful in predicting the `Weight` of these species? 
```

```{r}
resids <- residuals(mod3)
data.frame(fitted = predict(mod3), residuals = resids) %>%
  ggplot(., aes(x = fitted, y = residuals))+
  geom_point()
```

# Writing functions

We will often be interested in performing an operation for which no function is available. In this case, we may want to write our own functions. As an example, below I am creating a function called `greeting()`, which takes in one parameter as an input. This simple function will simply greet you! 

Notice that the function name goes on the left, and the generic placeholder for the input is `name`. You could really change this to anything you want, so long as it is consistent throughout the function definition. We can type as many commands between the curly braces as we wish.

```{r}
greeting <- function(name){
  print(paste0("Hello, ", name, "!"))
}
```

If we type `greeting`, `R` will tell us what is in the function:

```{r}
greeting
```

To call the function, simply pass in a name of your choice:

```{marginfigure}
What happens if you don't pass in a parameter?
```

```{r}
greeting("student")
```

# Your turn!


## Exercise 1 

You will create some simulated data and will fit simple linear regression models to it. Make sure to use `set.seed(2)` prior to starting Exercise 1 to ensure consistent results.

  a. Using the `rnorm()` function, create a vector `x` that contains 100 observations drawn from a $N(0,1)$ distirbution. This represents a feature or predictor, $X$. 
  
  b. Using the `rnorm()` function, create a vector `eps` that contains 100 observations drawn from a $N(0,0.25)$ distribution -- a Normal distribution with mean 0 and variance 0.25. 

  c. | Using `x` and `eps`, generate a vector `y` according to the model
$$Y = -2 + 0.5X + \epsilon$$

     | What is the length of `y`? What are the values of $\beta_{0}$ and $\beta_{1}$?

  d) Fit a least squares linear model to predict `y` using `x`. How do $\hat{\beta}_{0}$ and  $\hat{\beta}_{1}$ compare to $\beta_{0}$ and $\beta_{1}$?

  e) Now fit a polynomial regression model that predicts `y` using `x` and `x^2`. Is there evidence that the quadratic term improves model fit? Explain. 

  f) Suppose we wanted to generate data with *more* noise. How might we do that? Would you expect the confidence intervals for $\beta_{0}$ and $\beta_{1}$ to be wider, narrower, or about the same as compared to the original data?


<div class = "commit"> This is a good place to pause, knit and <b>commit changes</b> with the commit message "Added answer for Ex 1." Push these changes when you're done.
</div>


## Exercise 2

Here, we explore the *collinearity* problem. Run the following by changing to `eval = T`:

```{r eval = F}
set.seed(2)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

  a) Write out the form of the linear model. What are the regression coefficients?
  
```{marginfigure}
Hint: the `cor()` function may be useful
```

  b) What is the correlation between `x1` and `x2`? Create a scatterplot displaying a relationship between the two variables, and comment on what you see.

  c) Using this data, fit a least squares regression line to predict `y` using `x1` and `x2`. Describe the results obtained. How do the estimates $\hat{\beta}_{j}$ relate to the true $\beta_{j}$? Can you reject the null hypothesis $H_{0}: \beta_{1} = 0$ or the null hypothesis $H_{0}: \beta_{2} = 0$?

  d) Now fit a least squares regression line to predict `y` using only `x1`. Comment on your results. Can you reject the null hypothesis $H_{0}: \beta_{1} = 0$?

  e) Now fit a least squares regression line to predict `y` using only `x2`. Comment on your results. Can you reject the null hypothesis $H_{0}: \beta_{2} = 0$?

  f) Do the results obtained in (c) - (e) contradict each other? Explain.

  g) Now, suppose we obtain one additional observation which happened to be mismeasured. Be sure to run this code by changing `eval = F` to `eval = T` in the `R` chunk header.
     

```{r eval = F}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
x1 <- c(x1, 0.1 )
x2 <- c(x2, 0.8)
y <- c(y, 6)
```

Refit the linear models from (c) - (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.
    
<div class = "commit"> This is a good place to pause, knit and <b>commit changes</b> with the commit message "Added answer for Ex 2." Push these changes when you're done.
</div>


## Exercise 3

Create a function called `dist` that takes two vectors of the same length as input and returns their Euclidean distance. 

# Submission

Once you are finished, knit one last time, commit your changes, and push to GitHub. Then submit the PDF file to Canvas!
