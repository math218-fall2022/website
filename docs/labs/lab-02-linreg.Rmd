---
title: "Lab 02 - Linear Regression"
subtitle: "due Sunday, 9/25 at 11:59pm"
output: 
  tufte::tufte_html:
    css: "./math218-labs.css"
    tufte_variant: "envisioned"
    highlight: tango
    toc: true
    toc_depth: 1
editor_options: 
  chunk_output_type: console
---

# Getting started

## Clone the repo & start new RStudio project

- Go to the Math 218 organization on GitHub (http://www.github.com/math218-fall2022). Click on the repo with the prefix **lab-02-linear-regression-**. It contains the starter documents you need to complete the lab.

- Click on the green **Code** button, select the second option "Open with GitHub Desktop"

```{r echo = F, fig.align="center", out.width="70%"}
knitr::include_graphics("img/00-git/clone.png")
```

- The GitHub Desktop application will open up, with a white window that says "Clone a Repository". **Important**: in the second line that says "Local Path", there is a button that says `Choose...`. Click on it, and find and select the folder we created for this course. Then hit the blue `Clone` button.

```{r echo = F, fig.align="center", out.width="70%"}
knitr::include_graphics("img/00-git/find_folder.png")
```

- After successfully cloning, the window will disappear and you will see the that Current Repository is the one you just cloned. Success!

- **Note**: if you want to commit and push changes  for a different project (e.g. an application exercise), you can switch to the other project by hitting the first top=left button `Current Repository`. You should see all of your cloned projects here. Simply pick the one you would like to commit changes for!



# Packages

Here, we will make use of the package `ggfortify`. Check if it is installed by running 

```{r eval = F}
library(ggfortify)
```

If you receive an error message, then run the following command in your Console once:

```{r eval = F}
install.packages("ggfortify")
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = T, 
                      warning = FALSE, 
                      message  = FALSE,
                      fig.align="center", fig.width=6, fig.height=4)
library(tidyverse)
library(MASS)
library(ISLR2)
library(ggfortify)
```


# Simple linear regression

We will begin by working with some data about fish. The data consist of record of seven common different fish species in fish market sales. Our goal will be to estimate the `Weight` (g) of a fish given its other characteristics. We have access to the following predictors: `Species`, `BodyLength` (nose to beginning of tail), `TotalLength` (nose to end of tail), `Height`, and `Width`. All physical measurements are in cm.

```{r}
fish <- read.csv("data/fish.csv", header = T)
head(fish)
```

We will begin by using the `lm()` function to fit a simple linear model, with `Weight` and the response and `BodyLength` as the predictor. The `summary()` function gives us estimates, standard errors, and $p$-values for the coefficients, as well as the $R^2$ for the model. 

```{r}
mod0 <- lm(Weight ~ BodyLength, data = fish)
summary(mod0)
```

We can access just the estimates $\hat{\beta}$ using the `coef()` function, and obtain confidence intervals (default are 95%).

```{r}
coef(mod0)
confint(mod0)
```

The `predict()` function can be used to obtain confidence and prediction intervals for the response, for a given value of the predictors:

```{marginfigure}
Compare the widths of the intervals. Why is there a difference?
```

`fit` is the estimated value, and `lwr` and `upr` give the lower and upper bounds of the 95%  intervals, respectively.

```{r}
predict(mod0, data.frame(BodyLength = c(15, 30, 45)), interval = "confidence")
predict(mod0, data.frame(BodyLength = c(15, 30, 45)), interval = "prediction")
```

# Diagnostics

Here, I plot the response and predictors, along with the least squares regression line. There is some evidence for non-linearity between `Weight` and `BodyLength`...

```{r}
ggplot(fish, aes(x = BodyLength,y = Weight))+
  geom_point() +
  geom_smooth(method = "lm", se = F)
```

We can compute the residuals from a linear regression fit using the `residuals()` function. Once we compute them, we can plot them! The following code also provides a horizontal line at 0 as a point of reference. It is clear that we have non-linearity.

```{r}
resids <- residuals(mod0)
data.frame(fitted = predict(mod0), residuals = resids) %>%
  ggplot(., aes(x = fitted, y = residuals))+
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed", col = "red")
```

Leverage statistics can be computed using the `hatvalues()` function. Also, recall that the average leverage is calculated as $(p+1)/n$. I obtain average leverage as well:

```{r}
lev <- hatvalues(mod0)
p <- 1
n <- nrow(fish)
av_lev <- (p+1)/n
```


Here, I plot the leverage in the order of data collection, with a reference for the average leverage:
 
```{r}
data.frame(leverage = lev) %>%
  mutate(order = row_number()) %>%
  ggplot(., aes(x = order, y = leverage)) +
  geom_point() +
  geom_hline(yintercept = av_lev, col = "red", linetype = "dashed")
```

Alternatively, we can take advantage of the `autoplot()` function from the `ggfortify` library we loaded. We pass in a fitted `lm()` object, and `autoplot()` will plot up to six diagnostic plots. Here, we will focus on four: residuals vs fitted, Normal q-q plot, standardized residuals vs fitted, and residuals vs leverage. Notice that `autoplot` will provide the indices of points of interest (possible outliers or high leverage) in the respective plots. 

```{r}
autoplot(mod0, which = c(1,2,3,5))
```


# Multiple linear regression

In order to fit a multiple linear regression model using least squares, we again use the `lm()` function. The `summary()` function now outputs the regression coefficients for all the predictors.

```{marginfigure}
This is a bad example of naming models...oops! Usually we would want to have more informative variable names.
```

```{r}
mod1 <- lm(Weight ~ BodyLength + Height, data = fish)
summary(mod1)
```

Suppose I want to fit a multiple linear regression model using all the quantitative predictors. Rather than type them all out, I can use shorthand. On the right side of the `~`, typing a period `.` will tell `R` to use all the variables in the specified dataset (minus the specified response). Because I am currently only interested in the quantitative predictors, I will add `- Species` to tell `R` to use all the predictors *except* for `Species`.

```{r}
mod2 <- lm(Weight ~ . - Species, data = fish)
summary(mod2)
```

## Interactions

It is easy to include interaction terms in a linear model using the `lm()` function. The syntax `BodyLength:Height` tells `R` to include an interaction term between the two predictors. We can also use `*` as shorthand:

```{r}
# these two models are equivalent!
int_mod <- lm(Weight ~ BodyLength + Height + BodyLength:Height, data = fish)
int_mod <- lm(Weight ~ BodyLength * Height, data = fish)
summary(int_mod)
```

Let's visualize what this interaction implies about the model fit. In the following code, I am generating estimates of $\color{blue}{\text{weight}}$ from the model for a range of  $\color{blue}{\text{height}}$ values, at three different, fixed values of  $\color{blue}{\text{BodyLength}}$. Then we plot them. Notice how the relationship between  $\color{blue}{\text{height}}$ and  $\color{blue}{\text{weight}}$ really changes at different values of  $\color{blue}{\text{BodyLength}}$!

```{r}
beta_hat <- coef(int_mod)
height_seq <- seq(min(fish$Height), max(fish$Height), 0.1)
body_length1 <- 30

y_pred1 <- beta_hat[1] + beta_hat[2]*body_length1 + beta_hat[3]*height_seq +
  beta_hat[4]*body_length1*height_seq

df1 <- data.frame(height = height_seq, body_length = body_length1, weight = y_pred1)

body_length2 <- 25
y_pred2 <- beta_hat[1] + beta_hat[2]*body_length2 + beta_hat[3]*height_seq +
  beta_hat[4]*body_length2*height_seq

df2 <- data.frame(height = height_seq, body_length = body_length2, weight = y_pred2)

body_length3 <- 20
y_pred3 <- beta_hat[1] + beta_hat[2]*body_length3 + beta_hat[3]*height_seq +
  beta_hat[4]*body_length3*height_seq

df3 <- data.frame(height = height_seq, body_length = body_length3, weight = y_pred3)


rbind(df1, df2, df3) %>%
  mutate(body_length = factor(body_length)) %>%
  ggplot(., aes(x = height, y = weight, col = body_length)) + 
  geom_line()
```

## Non-linear transformations of predictors

Given a predictor $X$, we can create a predictor $X^{k}$ using `I(X^k)`. The function `I()` is necessary because the carat `^` has a special meaning in R. The following regresses `Weight` on `BodyLength` and `BodyLength^2`.

```{r}
mod_poly <- lm(Weight ~ BodyLength + I(BodyLength^2), data = fish)
summary(mod_poly)
```

The $p$-value for the quadratic term is significant, and suggests that the quadratic model might be an improvement over the simple linear regression model. We can use the `anova()` function to further quantify the extent to which the quadratic fit is superior to the linear fit.

```{marginfigure}
Note, the `anova()` function can only compare nested models. That is, `mod0` is a "subset" of `mod_poly`.
```

```{r}
anova(mod0, mod_poly)
```

The `anova()` function performs a hypothesis test comparing the two models, where the null hypothesis is that the two models fit the data equally well, and the alternative hypothesis is that the larger model is superior.The resulting $p$-value is small, and suggests that the model containing the quadratic term is superior to the simpler model that is only linear in `BodyLength`. This is not surprising, as we earlier saw evidence for non-linearity in the relationship between `Weight` and `BodyLength`.


## Qualitative predictors

Now, I will fit a  linear regression model using all the predictors, including the qualitative `Species` variable.

```{marginfigure}
What is our baseline level for `Species`?
```

```{r}
mod3 <- lm(Weight ~ ., data = fish)
summary(mod3)
```


Looking at a plot of residuals vs fitted values, it is clear that a linear model is still insufficient. However, it appears to be an improvement from the simpler model.

```{marginfigure}
What are some other predictors that you think would be helpful in predicting the `Weight` of these species? 
```

```{r}
resids <- residuals(mod3)
data.frame(fitted = predict(mod3), residuals = resids) %>%
  ggplot(., aes(x = fitted, y = residuals))+
  geom_point()
```

# Writing functions

We will often be interested in performing an operation for which no function is available. In this case, we may want to write our own functions. As an example, below I am creating a function called `greeting()`, which takes in one parameter as an input. This simple function will simply greet you! 

Notice that the function name goes on the left, and the generic placeholder for the input is `name`. You could really change this to anything you want, so long as it is consistent throughout the function definition. We can type as many commands between the curly braces as we wish.

```{r}
greeting <- function(name){
  print(paste0("Hello, ", name, "!"))
}
```

If we type `greeting`, `R` will tell us what is in the function:

```{r}
greeting
```

To call the function, simply pass in a name of your choice:

```{marginfigure}
What happens if you don't pass in a parameter?
```

```{r}
greeting("student")
```

# Your turn!


## Exercise 1 

You will create some simulated data and will fit simple linear regression models to it. Make sure to use `set.seed(2)` prior to starting Exercise 1 to ensure consistent results.

  a. Using the `rnorm()` function, create a vector `x` that contains 100 observations drawn from a $N(0,1)$ distribution. This represents a feature or predictor, $X$. 
  
  b. Using the `rnorm()` function, create a vector `eps` that contains 100 observations drawn from a $N(0,0.25)$ distribution -- a Normal distribution with mean 0 and variance 0.25. 

  c. | Using `x` and `eps`, generate a vector `y` according to the model
$$Y = -2 + 0.5X + \epsilon$$

     | What is the length of `y`? What are the values of $\beta_{0}$ and $\beta_{1}$?

  d) Fit a least squares linear model to predict `y` using `x`. How do $\hat{\beta}_{0}$ and  $\hat{\beta}_{1}$ compare to $\beta_{0}$ and $\beta_{1}$?

  e) Now fit a polynomial regression model that predicts `y` using `x` and `x^2`. Is there evidence that the quadratic term improves model fit? Explain. 

  f) Suppose we wanted to generate data with *more* noise. How might we do that? Would you expect the confidence intervals for $\beta_{0}$ and $\beta_{1}$ to be wider, narrower, or about the same as compared to the original data?


<div class = "commit"> This is a good place to pause, knit and <b>commit changes</b> with the commit message "Added answer for Ex 1." Push these changes when you're done.
</div>


## Exercise 2

Here, we explore the *collinearity* problem. Run the following by changing to `eval = T`:

```{r eval = F}
set.seed(2)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

  a) Write out the form of the linear model. What are the regression coefficients?
  
```{marginfigure}
Hint: the `cor()` function may be useful
```

  b) What is the correlation between `x1` and `x2`? Create a scatterplot displaying a relationship between the two variables, and comment on what you see.

  c) Using this data, fit a least squares regression line to predict `y` using `x1` and `x2`. Describe the results obtained. How do the estimates $\hat{\beta}_{j}$ relate to the true $\beta_{j}$? Can you reject the null hypothesis $H_{0}: \beta_{1} = 0$ or the null hypothesis $H_{0}: \beta_{2} = 0$?

  d) Now fit a least squares regression line to predict `y` using only `x1`. Comment on your results. Can you reject the null hypothesis $H_{0}: \beta_{1} = 0$?

  e) Now fit a least squares regression line to predict `y` using only `x2`. Comment on your results. Can you reject the null hypothesis $H_{0}: \beta_{2} = 0$?

  f) Do the results obtained in (c) - (e) contradict each other? Explain.

  g) Now, suppose we obtain one additional observation which happened to be mismeasured. Be sure to run this code by changing `eval = F` to `eval = T` in the `R` chunk header.
     

```{r eval = F}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
x1 <- c(x1, 0.1 )
x2 <- c(x2, 0.8)
y <- c(y, 6)
```

Refit the linear models from (c) - (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.
    
<div class = "commit"> This is a good place to pause, knit and <b>commit changes</b> with the commit message "Added answer for Ex 2." Push these changes when you're done.
</div>


## Exercise 3

Create a function called `mse` that takes two vectors, one of true values `true` and one of predicted values `preds`, and returns the mean squared error.

## Exercise 4

Create a function called `dist` that takes in three arguments: the first two arguments are vectors `x1`, `x2` of the same length. The third argument is a boolean value called `euclidean`. When `euclidean = T`, your `dist()` function should return the Euclidean distance between `x1` and `x2`. When `euclidean = F`, your `dist()` function should return the Manhattan distance. The following code should be used somewhere in your function:

```{r eval = F}
if(euclidean == T){
  ## code 
} else{
  ## code 
}
```

# Submission

Once you are finished, knit one last time, commit your changes, and push to GitHub. Then submit the PDF file to Canvas!
