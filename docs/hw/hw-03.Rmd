---
title: 'HW 03: Classification'
subtitle: due Thursday, Oct. 6 at 11:59pm
output:
  pdf_document:
    latex_engine: xelatex
editor_options:
  chunk_output_type: console
---

```{r include = F}
library(tidyverse)
library(MASS)
```


### Exercise 1

Suppose I have data about patients who have Melanoma disease. My response $Y$ is binary `status`, where $Y = 0$ implies a patient died, and $Y = 1$ means the patient is alive. I have the following predictors: a binary predictor `ulcer` (1 = has an ulcer, 0 = no ulcer), and continuous predictor `thickness` of the tumour (mm). I fit the model in `R` and obtain the following results.


```{r echo = F}
data("Melanoma")
melanoma <- Melanoma %>%
  filter(status != 3) %>%
  mutate(status = status - 1,
         ulcer = factor(ulcer))
mod <- glm(status ~ ulcer + thickness, data = melanoma, family = "binomial" )
summary(mod)
```

a) Write down the fitted logistic regression model based on this summary output from R.

b) Interpret all the coefficients.

c) Given that someone has an ulcer and the melanoma tumour has a thickness of 10mm, what is their estimated probability of dying from melanoma?

d) The following displays the contingency table for predictions on the training data using the model. Obtain the following three rates: overall error rate, false positive rate, and false negative rate.

```{r echo = F}
pred_probs <- predict(mod, type = "response")
preds <- rep(0, nrow(melanoma))
preds[which(pred_probs >= 0.5)] <- 1
table(pred = preds, truth = melanoma$status) 
```

e) From your answer in (d), interpret the kinds of errors our model makes, in the context of this problem. If you were a patient, would this worry you? Why or why not?

### Exercise 2

The sinking of the Titanic is one of the most infamous shipwrecks in history. We have data about the survival status of the passengers (0 = died, 1 = survived), as well as some other demographic and socioeconomic information. I fit a logistic regression model to the survival status of the passengers, regressed on age (years) and ticket class (categorical with three levels: 1st, 2nd, 3rd). 

After fitting the model, I have the following estimates coefficients:

```{r echo = F}
titanic_dat <- read.csv("data/titanic.csv") %>%
  mutate(class = factor(Pclass))
titanic_dat <- titanic_dat %>%
  mutate(class = relevel(class, 2))
titanic_mod <- glm(Survived ~ Age + class, titanic_dat, family = "binomial")
coef(titanic_mod)
```

The following plot displays the estimated probability of survival across age, at the three different ticket classes.

```{r echo = F, fig.align="center", fig.width=5, fig.height=3}
age_seq <- 1:71
probs <- predict(titanic_mod, 
                    newdata = data.frame(Age = rep(age_seq, 3),
                                         class = rep(1:3, each = 71)) %>%
                      mutate(class = factor(class, c(2,1,3))), type = "response") 

data.frame(probs = probs, age = rep(age_seq, 3),  class = rep(1:3, each = 71)) %>%
  mutate(class = factor(class)) %>%
  # mutate(probs = odds/(1+odds)) %>%
  ggplot(., aes(x = age , y = probs, col = class))+
  geom_line(size = 1.2)+
  guides(col = "none")+
  labs(x = "Age (years)", y = "Probability of surviving") +
  theme(text =element_text(size = 14)) +
  scale_y_continuous(breaks  = seq(0, 1, 0.25), limits = c(0,1))+
  scale_x_continuous(breaks = seq(0,70,10))
```

a) Based on this information, match each class to one of the lines in the plot. Briefly explain your reasoning. 

b) Based on your decision in (a), approximately how old would someone in third class have to be in order to have the same probability of survival as a 70-year old person in first class?

### Exercise 3

Recall that the Naive Bayes classifier assumes that the $p$ features are independent, i.e.
$f_{k}(x) = f_{k1}(x_{1}) \times f_{k2}(x_{2}) \times \cdots \times f_{kp}(x_{p})$. For categorical predictors, an estimate for each of the $f_{kj}$ is simply the proportion of training observations for class $k$ that fall into each level of the $j$-th predictor. 

Assume in this problem that we have $n = 200$ training observations with $p=2$ features ($X_{1}$ and $X_{2}$), and the response has $K=3$ possible classes. Further, assume that $X_{1}$ and $X_{2}$ are both qualitative, where $X_{1}$ has three levels and $X_{2}$ has two levels. 

The following displays the "aggregated" or summary data. That is, you are provided with the total counts within each class and predictor combination, as opposed to the individual, raw observations. For example, 22 observations have $X_{1}$ = `brown` and $Y$ = class 1. Similarly, 8 observation have $X_{2}$ = `tree` and $Y$ = class 2.  

```{r echo = F, message= F, warning = F}
library(tidyverse)
set.seed(2)
n <- 200

Y <- sample(1:3, n, replace = T, prob = c(0.3, 0.3, 0.4))
X1 <- sample(c("red", "white", "brown"), size = n, replace = T, prob = c(0.2, 0.4, 0.4))
X2 <- sample(c("tree", "ground"), size = n, replace = T, prob = c(0.2, 0.8))

table(X1, Y)

table(X2,Y)
```

a) What are the number of observations within each class of $Y$? Call them $n_{1}, n_{2}, n_{3}$. 

b) Using your answer from (a), what are your estimates for the marginal class probabilities? Call them $\hat{\pi}_{1}, \hat{\pi}_{2}, \hat{\pi}_{3}$. 

c) What are $f_{kj}(x_{j})$ for $k = 1, 2, 3$ and $j = 1,2$? For clarity, write these as $f_{Y =1, X1=brown}$, $f_{Y =2,X1 = brown}$, $f_{Y =3,X1 = brown}$, etc. You should have 15 quantities in total.


d) Suppose that we wish to classify a new observation $x^{∗} = (x_{1}^{*},x_{2}^{*}) = (\text{red},\text{ground})$. Under the Naive Bayes classifier with your estimates from (b) and (c), calculate $\text{Pr}(Y = k | X = x^{*})$ for each $k$. Based on your answer, what would you classify this new observation as and why?

e) Now let's say that our new observation is  $x^{∗} =  (x_{1}^{*},x_{2}^{*})= (\text{green},\text{ground})$. For this new observation, what would $\text{Pr}(Y = k | X = x^{*})$ be for each $k = 1,2,3$, and why? This is known as the *zero-frequency* problem. 


### Exercise 4

When the number of features $p$ is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. Organizing/clustering/searching data often relies on detecting spaces where objects form groups with similar properties. When $p$ is large, we will see that the objects (features) will seem very dissimilar.
This phenomenon is known as the *curse of dimensionality*, and it ties into the fact that non-parametric approaches often perform poorly when $p$ is large. We
will now investigate this curse (which is common in many disciplines, e.g. genetics). 


a) Suppose that we have a set of observations, each with measurements on $p = 1$ feature, $X$. We assume that $X$ is uniformly (evenly) distributed on $[0, 1]$. Associated with each observation is a response value $Y$. Suppose that we wish to predict a test observation’s response using only observations that are within 10% of the range of $X$ closest to that test observation. For instance, in order to predict the response for a test observation with $X = 0.6$, we will use observations with associated $X$ in the range $[0.55,0.65]$. On average, what fraction of the available observations will we use to make the prediction? 


b) Now suppose that we have a set of observations, each with measurements on $p = 2$ features, $X_1$ and $X_2$. We assume that $(X_1, X_2)$ are uniformly distributed on $[0, 1] \times [0, 1]$. We wish to predict a test observation’s response using only observations that are within 10% of the range of $X_{1}$ and within 10% of the range of $X_{2}$ closest to that test observation. For instance, in order to predict the response for a test observation with $X_1 = 0.6$ and $X_2 = 0.35$, we will use observations in the range $[0.55, 0.65]$ for $X_1$ and in the range $[0.3,0.4]$ for $X_2$. On average, what fraction of the available observations will we use to make the prediction?


c) Now suppose that we have a set of observations on $p = 100$ features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from 0 to 1. We wish to predict a test observation’s response using observations within the 10% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?


d) Using your answers to parts (a)–(c), argue that a drawback of KNN when $p$ is large is that there are very few training observations “near” any given test observation.


e)  Now suppose that we wish to make a prediction for a test observation by creating a $p$-dimensional hypercube centered around the test observation that contains, on average, 10% of the training observations. For $p$ = 1,2, and 100, what is the length of each side of the hypercube? Comment on your answer.

*Note: A hypercube is a generalization of a cube to an arbitrary number of dimensions. When $p = 1$, a hypercube is simply a line segment, when $p = 2$ it is a square, and when $p = 100$ it is a 100-dimensional cube.*

<!-- ### Exercise 5 -->

<!-- In the slides, we compared logistic regression and LDA for the 2-class (binary) response case. Let's now consider the more general $K$ class case.  We assign an observation to the class that maximizes $Pr(Y = k|X = x)$. Equivalently, we can set the last group $K$ as the baseline class and assign an observation to the class that maximizes $$\log\left(\frac{\text{Pr}(Y=k | X=x)}{\text{Pr}(Y = K|X=x)}\right)$$ -->


<!-- For LDA with $p$ features, the mean for class $k$ has mean vector $\mathbf{\mu}_{k}$ of length $p$, with a $p \times p$ covariance matrix $\mathbf{\Sigma}$. However, the case of $p=1$ is much simpler because we have scalars for means $\mu_{1}, \ldots, \mu_{K}$ and variance $\sigma^2$. In this $p=1$ case, $$\log\left(\frac{\text{Pr}(Y=k | X=x)}{\text{Pr}(Y = K|X=x)}\right) = a_{k}  + b_{k}x$$ -->

<!-- Provide expressions for $a_{k}$ and $b_k$ in terms of $\pi_{k}, \pi_{K}, \mu_{k}, \mu_{K}$, and $\sigma^2$. To get you started: -->

<!-- \begin{align*} -->
<!-- \log\left(\frac{\text{Pr}(Y=k | X=x)}{\text{Pr}(Y = K|X=x)}\right) &= -->
<!-- \log\left(\frac{\pi_{k}f_{k}(x)}{\pi_{K}f_{K}(x)}\right) \\ -->
<!-- &= \log\left(\frac{\pi_{k}\exp\{-\frac{1}{2\sigma^2}(x - \mu_{k})^2\}}{\pi_{K}\exp\{-\frac{1}{2\sigma^2}(x - \mu_{K})^2\}}\right)  -->
<!-- \end{align*} -->

### Exercise 5

a) Based on your group discussion, write your version of pseudo-code for implementing KNN *regression*. If you'd like, you may assume there are no ties in distances and that we will only consider quantitative predictors. 

b) Based on your group discussion, write your version of pseudo-code for implementing KNN *classification*. If you'd like, you may assume there are no ties in distances nor in classes, and that we will only consider quantitative predictors.


### BONUS (each part is worth extra points)

Suppose that you wish to classify an observation into a `toxic` or `nontoxic` class, based on a predictor $X$. You fit a logistic regression model and find that

$$\widehat{\text{Pr}}(Y = \text{toxic}| X=x) = \frac{e^{\hat{\beta}_{0} + \hat{\beta}_{1}x}}{1+e^{\hat{\beta}_{0} + \hat{\beta}_{1}x}}$$

I also fit a logistic regression model to the same data, but I use the *softmax* formulation and find that 

$$\widehat{\text{Pr}}(Y = \text{toxic}| X=x) = \frac{e^{\hat{\alpha}_{0,\text{toxic}} + \hat{\alpha}_{1, \text{toxic}}x}}{e^{\hat{\alpha}_{0,\text{toxic}} + \hat{\alpha}_{1,\text{toxic}}x} + e^{\hat{\alpha}_{0,\text{nontoxic}} + \hat{\alpha}_{1,\text{nontoxic}}x}}$$
(I am writing my model with $\alpha$'s to not confuse with your model's $\beta$'s.)

a) In my model, what are the log-odds of `toxic` vs `nontoxic`?

b) Suppose that in your model, $\hat{\beta}_{0} = -2$  and $\hat{\beta}_{1} = 3$. What are the coefficient estimates in my model? Be as specific as possible.

c) Now suppose that we fit the same two models on a different data set. This time, I estimate the coefficients $\hat{\alpha}_{0,\text{toxic}} = -1.5$, $\hat{\alpha}_{1,\text{toxic}} = 2$, $\hat{\alpha}_{0,\text{nontoxic}} = 1$, and $\hat{\alpha}_{1,\text{nontoxic}} = -1.25$.  What are the coefficient estimates in your model?

d)  Finally, suppose you apply both models from (c) to a data set with 2000 test observations. What fraction of the time do you expect the predicted class labels from your model to agree with those from my model? Explain your answer.

e) In the binary response case, would you prefer the usual logistic regression with the logit coding, or the softmax coding? Why?

## Submission 

Upload your assignment as a PDF file to Canvas. Please show all work!

