---
title: "HW 02: Regression"
date: "Due: Thursday, Sept. 29 at 11:59pm"
author: "Total: ? points"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.height = 4, fig.width = 4, fig.align = "center")
library(tidyverse)
library(ggfortify)
data("airquality")
air <- airquality %>% na.omit()
```



## Introduction

In class, we learned about the coefficient of determination $R^2$. In linear regression models, $R^2$ quantifies the proportion of variation in the response that is explained by the predictors. 

Recall that $R^2$ is defined as $$R^2 = 1 - \frac{\text{RSS}}{\text{TSS}}$$ where $\text{RSS} = \sum_{i=1}^{n}(y_{i} - \hat{y}_{i})^2$ and $\text{TSS} = \sum_{i=1}^{n}(y_{i} - \bar{y})^2$

```{marginfigure}
$r$ is the quantity calculated when you use the `cor()` function, as in Lab 02.
```

As it turns out, in simple linear regression, the $R^2$ value is exactly the square of the correlation coefficient $r$ (defined below) between the predictor $x$ and the response $y$! However, $R^2$ and $r$ have two very different meanings. We will explore both quantities in this homework.



## Exercise 1

It is not uncommon to want to chase a "good" $R^2$. However, a low $R^2$  isn’t necessarily a problem, and a high value doesn’t automatically indicate that you have a good model. (You don’t get paid in proportion to R-squared!) Consider the following data about air quality measurements in New York during May to September 1973. Here, we are regressing `Ozone` levels (ppb) on the temperature `Temp` (F). Note that ozone levels are always non-negative.

```{r}
mod <- lm(Ozone ~ Temp, air)
summary(mod)

```

a) Interpret the coefficient for `Temp`. Does the model's $R^2$ inform/affect your interpretation? If so, how?

b) The following plot displays 95% confidence intervals for predicted ozone for various `Temp` values.

```{r}
preds <- predict(mod, newdata = data.frame(Temp = seq(55,90,2)), interval = "confidence")
data.frame(preds) %>%
  mutate(Temp =seq(55,90,2)) %>%
  ggplot(., aes(x = Temp, y = fit))+
  geom_point()+
  geom_errorbar(aes(ymin = lwr, ymax = upr)) +
  labs(x = "Temperature (F)", y = "Predicted ozone + 95% CIs (ppb)")
```

What do you think about the quality of our predictions for Ozone? Are you comfortable with all of these intervals? 

c) We should look at some diagnostic plots. Below is a plot of the residuals vs fitted values. What does this plot reveal about the fit of the linear model? How does the $R^2$ inform about whether the linear model is a good choice?

```{r}
data.frame(residuals = mod$residuals, fitted = mod$fitted.values) %>%
  ggplot(.,aes(x = fitted, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed")
```


## Exercise 2

In statistics, correlation coefficients measure the strength of the relationship between two variables. They are a quantitative assessment that measures both the direction and the strength of the tendency to vary together. We will focus on a common correlation coefficient known as the Pearson correlation, $r$ (when estimated from a sample) or $\rho$ (population parameter). This correlation coefficient is a single number that measures both the strength and direction of the *linear* relationship between two continuous variables. It is always the case that $-1 \leq r \leq 1$. Here are some examples:

```{r fig.width=10}
# examples
set.seed(100)
n <- 100
x <- sort(rnorm(n))
y1 <-  1 + 0.5*x + rnorm(n, 0, 1)
y2 <- 1 -1*x + rnorm(n,0,0.2)
y3 <- rnorm(n, 0, 1)
p1 <- data.frame(x = x, y = y1) %>%
  ggplot(.,aes(x = x, y = y))+
  geom_point()+
  ggtitle("Moderately positive", subtitle = paste0("r = ", round(cor(x,y1), 2)))+
  geom_smooth(method = "lm", se = F)
p2 <- data.frame(x = x, y = y2) %>%
  ggplot(.,aes(x = x, y = y))+
  geom_point()+
  ggtitle("Strongly negative", subtitle = paste0("r = ", round(cor(x,y2), 2))) +
  geom_smooth(method = "lm", se = F)
p3 <- data.frame(x = x, y = y3) %>%
  ggplot(.,aes(x = x, y = y))+
  geom_point()+
  ggtitle("Uncorrelated", subtitle = paste0("r = ", round(cor(x,y3), 2))) +
  geom_smooth(method = "lm", se = F)
gridExtra::grid.arrange(p1, p2, p3, nrow = 1)
```

While we can always calculate the correlation $r$ between two quantitative variables, we should exercise some caution. The next two plots display the relationship between two variables `x` and `y`, as well as the calculate correlated coefficient.

Note, the two plots below are generated with different sets of data. 

```{r echo = F}
set.seed(1)
n <- 100
x <- sort(rnorm(n))
y <- 1 + 0.5*x + rnorm(n,0, 0.25)
id <- 40
y[id] <- y[id]*5
r_all <- cor(x,y)
r_no_out <- cor(x[-id], y[-id])

p1 <- data.frame(x = x, y =y ) %>%
  mutate(outlier = ifelse(row_number() == id, T, F)) %>%
  ggplot(., aes(x =x , y = y, col = outlier))+
  geom_point() +
  scale_color_manual(values = c("black", "orange"))+
  guides(col = "none") +
  ggtitle(paste0("All points: r = ", round(r_all, 2), "\nOmitting orange point: r = ", round(r_no_out,2))) 
```


```{r}
set.seed(1)
n <- 100
x <- rnorm(n)
y <- 1 + x + x^3 + rnorm(n, 0, 0.25)
p2 <- data.frame(x = x, y =y ) %>%
  ggplot(., aes(x =x , y= y))+
  geom_point() +
  ggtitle(paste0("r = ", round(cor(x,y),2)))
```

```{r fig.width=8}
gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Based on these two plots, what are some cautions when it comes to interpreting Pearson's correlation coefficient (with respect to linearity and outliers)?

## Exercise 3

The formula for the correlation coefficient $r$ is $$r = \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})( y_{i} - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_{i} - \bar{x})^2 \sum_{i=1}^{n}(y_{i} - \bar{y})^2 }}$$

Suppose we have the following data:

```{r}
df <- data.frame(x = c(-1, 0, 1, 0, 2, -2),
           y = c(0, 2, 1, 2, 3, -2))
print.data.frame(df)

```

a) Calculate $\bar{x}$ and $\bar{y}$.

b) Calculate (manually) the correlation $r$ between $x$ and $y$ using this data. 


c) Now, suppose we actually measured `x2 = 5x`. That is, `x2` is simply a *scaled* version of the original `x`: 

```{r}
print.data.frame(df %>% mutate(x = 5*x) %>% rename("x2" = "x"))
```

Calculate the correlation between  $x2$ and $y$.

d) Now, supposed we actually measured `x3 = x + 1`. That is, `x3` is just a shifted version of the original `x`. 

```{r}
print.data.frame(df %>% mutate(x = x+1) %>%  rename("x3" = "x"))
```

Calculate the correlation between  $x3$ and $y$.

e) Based on your answers in (b)-(d), how does shifting or scaling variables affect the correlation?

f) One advantage of the correlation coefficient $r$ is that it is "unitless". Can you demonstrate why that is? How do (b)-(e) support this statement?


## Exercise 4

I have claimed that in the case of simple linear regression of Y onto X, the $R^2$ statistic is equal to the square of the correlation $r$ between X and Y. Prove that this is the case. For simplicity, you may assume that $\bar{x} = \bar{y} = 0$.

## Exercise 5

Recall from the slides that our least squares estimate for $\beta_{1}$ is

$$\hat{\beta}_{1} = \frac{\sum_{i=1}^{n}(x_{i} - \bar{x})( y_{i} - \bar{y})}{ \sum_{i=1}^{n}(x_{i} - \bar{x})^2 }$$

Assume that we are in the simple linear regression setting, i.e. $$y = \beta_{0} + \beta_{1}x + \epsilon$$


a) Re-write $\hat{\beta}_{1}$ as a function of $r$.



b) True or False: $\hat{\beta}_{1}$ has the same sign as $r$. Explain.

c) If $r=0$, what is the implication for $\beta_1$? Why does this make sense?


## Exercise 6

Show that the least squares regression line always passes through the sample mean of the data. For simplicity, you may assume we are in the case of simple linear regression.

## Exercise 7

In this course, we will focused mainly on how close the predicted value $\hat{y}$ is to the true $y$. We will rarely discuss the amount of *uncertainty* there is about the prediction. (even though the discipline of Statistics studies uncertainty!) However, we will spend a little time on the idea of uncertainty here.

In the lab, we saw how given a fitted linear regression model, the `predict()` function provides confidence and prediction intervals for the response for fixed values of the predictors. We also saw how that for given $x$,  the prediction interval was wider than the confidence interval. Why is this the case?

The **confidence interval** of a prediction yields a range that likely contains the *mean *value of $y$ given specific values of the predictors $x$. These yield a *population* average, where the particular population is defined by the values $x$. The confidence interval does not tell you anything about the spread of the individual data points around the population mean. 

A **prediction interval** yields a range that likely contains the value of $y$ given specific values of the predictors $x$. With this type of interval, we predict ranges for *individual* observations rather than the mean value. The prediction intervals are wider than confidence intervals because they account for the inherent variability of the individual data points (i.e. the irreducible error $\epsilon$).


Consider the following simulated data. In the plot on the left, we have generated $n = 20$ data points. The plot on the right adds an additional 80 data points for a total of $n = 100$ observations. The orange dashed lines are the 95% confidence intervals at various `x` values,  and blue dashed lines are the 95% prediction intervals. The vertical gray line marks the sample mean $\bar{x}$ of the predictor. Note, both plots have the same $x$ and $y$ axes for ease of comparison,

```{r, fig.width=8}
set.seed(1)
n <- 20
x <- rnorm(n)
y <- 1 + 0.3*x + rnorm(n, 0, 0.5)
df <- data.frame(x = x, y = y)
xx <- seq(-4,4,0.1)
mod0 <- lm(y ~ x, data = df)
pred <- predict(mod0, newdata = data.frame(x = xx), interval = "prediction")
conf <- predict(mod0, newdata = data.frame(x = xx), interval = "confidence")

p1 <- ggplot()+
  geom_point(data = df, aes(x = x, y = y))+
  geom_abline(intercept = coef(mod0)[1], slope = coef(mod0)[2]) +
  geom_smooth(data = data.frame(x = xx, y = pred[,2]),
              aes(x = x, y=y), linetype = "dashed")+
  geom_smooth(data = data.frame(x = xx, y = pred[,3]),
              aes(x = x, y=y), linetype = "dashed")+
    geom_smooth(data = data.frame(x = xx, y = conf[,2]),
              aes(x = x, y=y), linetype = "dashed", col = "orange")+
  geom_smooth(data = data.frame(x = xx, y = conf[,3]),
              aes(x = x, y=y), linetype = "dashed", col = "orange") +
  ggtitle(paste0("n = ", n)) +
  ylim(c(-1, 3))+
  xlim(range(xx))+
  geom_vline(xintercept = mean(x),size = 0.1)


n2 <- 100
x2 <- c(x, rnorm(n2 - n))
y2 <- c(y,1 + 0.3*x2[-(1:n)] + rnorm(n2 -n, 0, 0.5))
df <- data.frame(x = x2, y = y2)
mod2 <- lm(y ~ x, data = df)
pred <- predict(mod2, newdata = data.frame(x = xx), interval = "prediction")
conf <- predict(mod2, newdata = data.frame(x = xx), interval = "confidence")

p2 <- ggplot()+
  geom_point(data = df, aes(x = x2, y = y2))+
  geom_abline(intercept = coef(mod0)[1], slope = coef(mod0)[2]) +
  geom_smooth(data = data.frame(x =xx, y = pred[,2]),
              aes(x = x, y=y), linetype = "dashed")+
  geom_smooth(data = data.frame(x = xx, y = pred[,3]),
              aes(x = x, y=y), linetype = "dashed")+
    geom_smooth(data = data.frame(x = xx, y = conf[,2]),
              aes(x = x, y=y), linetype = "dashed", col = "orange")+
  geom_smooth(data = data.frame(x = xx, y = conf[,3]),
              aes(x = x, y=y), linetype = "dashed", col = "orange") +
  ggtitle(paste0("n = ", n2))+
  ylim(c(-1, 3))+
  xlim(range(xx)) +
  labs(x = "x", y = "y")+
  geom_vline(xintercept = mean(x2), size = 0.1)

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

The formula for the confidence interval at predictors $x_{0}$ is:

$$\hat{y_{0}} \pm t^{*}\sqrt{\text{MSE} \times \left( \frac{1}{n}+ \frac{(x_{0} - \bar{x})^2}{\sum(x_{i} - \bar{x})^2}\right )} $$
The formula for the prediction interval at predictors $x_{0}$ is:

$$\hat{y_{0}} \pm t^{*}\sqrt{\text{MSE} \times \left(1+ \frac{1}{n}+ \frac{(x_{0} - \bar{x})^2}{\sum(x_{i} - \bar{x})^2}\right )} $$


a) Explain why, intuitively, the confidence and prediction intervals are centered around the same predicted value.

b) Looking at the plots above: for which value of the predictor $x$ are both intervals narrowest? Explain why this is the case intuitively.

c)  Looking at the plots above: for a given value of $x_{0}$ and interval type (prediction or confidence), are the intervals narrower when $n = 20$ or $n = 100$? Explain why this is the case.

d) Assume $t^{*} > 0$ and $MSE >> 0$ (here $>>$ means "much greater than"). Can we make it so the confidence interval width approaches 0 (i.e. we are extremely confident in the predicted population response)? If so, how? If not, why not?

e) Assume $t^{*} > 0$ and $MSE >> 0$. Can we make it so the prediction interval width approaches 0 (i.e. we are extremely confident in the predicted individual response)? If so, how? If not, why not?

## Exercise 8

I collect a set of data (n = 100 observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. $Y = \beta_{0} + \beta_{1} X + \beta_{2} X^{2} + \beta_{3} X^{3} + \epsilon$.

a) Suppose the true relationship between $X$ and $Y$ is linear, i.e.  $Y = \beta_{0} + \beta_{1} X  + \epsilon$. Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

b) Answer (a) using test rather than training RSS.

c) Suppose that the true relationship between $X$ and $Y$ is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.

d) Answer (c) using test rather than training RSS.




## Submission 

Upload your assignment as a PDF file to Canvas. Please show all work!
