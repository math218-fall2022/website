---
title: "HW 01: Introduction"
date: "Due:  11:59pm"
author: "Total: ? points"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE, echo = FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
```


## Introduction

For the $K$-Nearest Neighbors classifier, after choosing $K$ we need to somehow find the nearest neighbors. That is, for a new point $x_{0}$, we need to determine the neighboring set $\mathcal{N}_{0}$. 

Here, "nearest" gives off a notion of closeness or distance. Therefore, we want to somehow measure distance between features/predictors. How might we do that?

### Euclidean distance

In Mathematics, the Euclidean distance is defined as the shortest possible path through space between two points.

On a number line (one-dimension), the distance between two points $a$ and $b$ is simply the absolute value of their difference: letting $d(a,b)$ denote the Euclidean distance between points $a$ and $b$, $d(a,b) = |a-b|$. This is equivalent to $d(a,b) = \sqrt{(a-b)^2}$


In two-dimensions (think $x$ - $y$ coordinate system, latitude-longitude), the two points are $\mathbf{a} = (a_{1}, a_{2})$ and $\mathbf{b} = (b_{1}, b_{2})$. The euclidean distance between points  $\mathbf{a}$ and  $\mathbf{b}$ is given by $$d(\mathbf{a}, \mathbf{b}) = \sqrt{(a_{1} - b_{1})^2 + (a_{2} - b_{2})^2}$$ Importantly, "two-dimensions" refers to the number of coordinates in each point; not the fact that we are calculating a distance between two points.

This easily generalizes to $p$-dimensions! If our two points are $p$-dimensional (i.e. $\mathbf{a} = (a_{1}, a_{2}, \ldots, a_{p})$ and $\mathbf{b} = (b_{1}, b_{2}, \ldots, b_{p})$), then 
$$d(\mathbf{a}, \mathbf{b}) = \sqrt{(a_{1} - b_{1})^2 + (a_{2} - b_{2})^2 + \ldots + (a_{p} - b_{p})^2 } = \sqrt{\sum_{j=1}^{p}(a_{j} - b_{j})^2}$$


For example: let $\mathbf{a}= (1,2)$ and $\mathbf{b} = (3,5)$. Then the Euclidean distance between them is $d(\mathbf{a}, \mathbf{b}) = \sqrt{(3-1)^2 + (5-2)^2} = \sqrt{13}$.


### Manhattan distance

Another possible way to define distance is using the Manhattan distance. This is named after the grid-system of Manhattan's roads. An analogy is the number of blocks (in the north, south, east or west directions) a taxicab must travel on, in order to reach its destination on the grid of streets in parts of New York City.

The Manhattan distance $d_{m}$ between two $p$-dimensional points $\mathbf{a}$ and  $\mathbf{b}$ is defined as 

$$d_{m}(\mathbf{a}, \mathbf{b}) = \sum_{j=1}^{p}|a_{j} - b_{j}|$$


### Distances for categorical features

Thus far, we have assumed that all our predictors are quantitative, but oftentimes we have qualitative features. How does one measure the distance between categorical levels?

One approach is to assign numeric values for each possible category. For example, if the feature is "Experience level" with levels (Beginner, Average, Professional), maybe we assign the values (0, 5, 10). However, this assumes an implicit ordering in the levels.

Alternatively, consider that all our $p$ features are binary in that they each take one of two values. Example: $X_{1}$ = smoker status (non-smoker/smoker) and $X_{2}$ = drinks alcohol (no/yes). The Hamming distance is the number (or proportion, depending on the context) of features for which the two values do not match.


## Exercise 1

```{r echo = F}
n <- 6
```

Suppose we have `r n` training data points as follows:

```{r echo = F}
obs <- 1:n
x1 <- c(0, 1, 1, 2, -1, 0)
x2 <- c(3, -1, 1, 0, 1, 1)
x3 <- c(1, 2, -1, 0, 0, 0)
y <- c("red", "red", "blue", "red", "blue", "red")
df1 <- data.frame(X1 = x1, X2 = x2, X3 = x3, Y = y)
print.data.frame(df1)
```

Suppose we want to use this data set to make a prediction for Y when (X1, X2, X3) = (0,0,0) using K-nearest neighbors.



a) Compute the Euclidean distance between each observation and the test point.

```{r eval = F}
rowSums(df1[,1:3]^2)
```

b) Using the Euclidean distance metric, what is our prediction with $K = 1$? Why?

c) Using the Euclidean distance metric, what is our prediction with $K = 3$? Why?


d) Compute the Manhattan distance between each observation and the test point.

```{r eval = F}
rowSums(abs(df1[,1:3]))
```

e) Using the Manhattan distance metric, what is our prediction with $K = 1$? Why?

f) Using the Manhattan distance metric, what is our prediction with $K = 3$? Why?

g) Are there certain scenarios where we might prefer one distance metric over another? 


## Exercise 2

a) Suppose we have `r n` training data points as follows
 
```{r}
x1 <- c("smoker", "non-smoker", "smoker", "smoker", "smoker","non-smoker")
x2 <- c("yes", "no", "no", "yes", "yes", "no")
x3 <- c("no", "yes", "no", "yes", "no", "yes")
df2 <- data.frame("Smoke status" = x1, "Alcohol" = x2, "Athlete" = x3)
print.data.frame(df2)
```

Calculate the Hamming distance between a new test point where the individual does not smoke, does not consume alcohol, and does not play a sport.

```{r eval = F}
rowSums(apply(df2, 2, function(x){x != c("non-smoker", "no", "no")}))
```

b) Hamming distance is commonly used to measure distance between words or phrases. What is the Hamming distance between the words "MIDDLEBURY" and "SWARTHMORE"?

c) For points with $p$ features, are the minimum and maximum possible Hamming distances?


## Exericise 3

In class, we introduced the notion of mean squared error (MSE):

$$\text{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^2$$ 
where $y_{i}$ is the true value, and $\hat{y}_{i}$ is the predicted value. We can use MSE to compare different models' quality of predictions. 

Another metric that is commonly used is the mean absolute error (MAE), which simply uses the absolute value of the difference, rather than the squared difference:

$$\text{MAE}=\frac{1}{n}\sum_{i=1}^{n}|y_{i}-\hat{y}_{i}|$$ 

In the following table, the first column `y_true` denotes the true values $y_{i}$. The second column `yhat1` holds the predicted values obtained from a model $\mathcal{M}_{1}$, and the third column `yhat2` holds the predicted values obtained from a different model $\mathcal{M}_{2}$.

```{r}
y_true <- c(1, 3, 1, -1, 0, 5)
yhat1 <- c(0, 1, 0, 0, -1, 3)
yhat2 <- c(2, 3, 2, 2, 1, 4)
df3 <- data.frame(y_true = y_true, yhat1 = yhat1, yhat2 = yhat2)
print.data.frame(df3)
```

a) Calculate the MSE for both models.

```{r eval = F}
mean((yhat1 - y_true)^2)
mean((yhat2 - y_true)^2)
```

b) Calculate the MAE for both models.

```{r eval = F}
mean(abs(yhat1 - y_true))
mean(abs(yhat2 - y_true))
```

c) Based on your results in (a) and (b), which model performs better predictions?

## Exericise 4

Now, let's consider some new data with just one model for predictions: 

```{r}
y_true <- c(1, 3, 1, -1, 0, 2)
ypred <- c(0, 2, 0, -3, -1, 1)
df4 <- data.frame(y_true = y_true, yhat = ypred)
print.data.frame(df4)
```

a) Calculate the MSE and MAE.

```{r eval = F}
mean( (y_true - ypred)^2)
mean(abs(y_true - ypred))
```

b) Now, suppose we measure a 7th point, where $y_{7} = 5$ and $\hat{y}_{7} = 15$. Clearly our prediction is way off! Calculate the MSE and MAE including this new observation, and comment on how they compare to the errors obtained on the original six data points in part (a).

```{r eval = F}
mean( (y_true - ypred)^2 + (5 - 15)^2)
mean(abs(y_true - ypred) + abs(5-15))
```

c) Are there scenarios where we might prefer one error metric over the other?

## Exercise 5

Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide $n$ and $p$.

a) We are foraging for mushrooms and happen to find a mushroom we have never seen before. How exciting! We would like to determine if it is poisonous based on thirty mushroom species we have previously collected. For each of these thirty mushrooms, we know its color, shape, where it was found, spore dispersal method, and poison status. **While statistical learning methods are powerful, please do not trust them to help identify poisonous mushrooms!**

b) We collect data on 300 national universities, recording the average school expenditures per student, average SAT scores, tuition cost, and average family income. We would like to understand what factors might influence the average salary of alumni from these universitities.


## Submission 

Please upload your finished assignment to Canvas as a PDF (either scanned or converted document). 